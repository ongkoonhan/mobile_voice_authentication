{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import copy\n",
    "import random\n",
    "import pickle\n",
    "import numpy as np\n",
    "from skimage.color import gray2rgb\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torchvision import models\n",
    "from torchsummary import summary\n",
    "\n",
    "from config import models_folder, output_data_folder\n",
    "from config import n_mels\n",
    "\n",
    "from model_definitions import SpectrogramEncoderNet, MultiSiameseContrastiveClassifierNet\n",
    "from data_generators import ContrastiveDataGenerator, BaseDataGenerator\n",
    "from project_utils import ModelSaveAndLogHandler, load_module_from_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_HEIGHT = n_mels\n",
    "CANDIDATE_SIZE = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Mobile net\n",
    "# model = models.mobilenet_v2(pretrained=False)\n",
    "# # Dense net\n",
    "# model = models.densenet121(pretrained=False)\n",
    "# summary(model, input_size=(3, IMG_HEIGHT, IMG_HEIGHT), device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # mobile net classifier\n",
    "# model.classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 64, 64]           9,408\n",
      "       BatchNorm2d-2           [-1, 64, 64, 64]             128\n",
      "              ReLU-3           [-1, 64, 64, 64]               0\n",
      "         MaxPool2d-4           [-1, 64, 32, 32]               0\n",
      "       BatchNorm2d-5           [-1, 64, 32, 32]             128\n",
      "              ReLU-6           [-1, 64, 32, 32]               0\n",
      "            Conv2d-7          [-1, 128, 32, 32]           8,192\n",
      "       BatchNorm2d-8          [-1, 128, 32, 32]             256\n",
      "              ReLU-9          [-1, 128, 32, 32]               0\n",
      "           Conv2d-10           [-1, 32, 32, 32]          36,864\n",
      "      BatchNorm2d-11           [-1, 96, 32, 32]             192\n",
      "             ReLU-12           [-1, 96, 32, 32]               0\n",
      "           Conv2d-13          [-1, 128, 32, 32]          12,288\n",
      "      BatchNorm2d-14          [-1, 128, 32, 32]             256\n",
      "             ReLU-15          [-1, 128, 32, 32]               0\n",
      "           Conv2d-16           [-1, 32, 32, 32]          36,864\n",
      "      BatchNorm2d-17          [-1, 128, 32, 32]             256\n",
      "             ReLU-18          [-1, 128, 32, 32]               0\n",
      "           Conv2d-19          [-1, 128, 32, 32]          16,384\n",
      "      BatchNorm2d-20          [-1, 128, 32, 32]             256\n",
      "             ReLU-21          [-1, 128, 32, 32]               0\n",
      "           Conv2d-22           [-1, 32, 32, 32]          36,864\n",
      "      BatchNorm2d-23          [-1, 160, 32, 32]             320\n",
      "             ReLU-24          [-1, 160, 32, 32]               0\n",
      "           Conv2d-25          [-1, 128, 32, 32]          20,480\n",
      "      BatchNorm2d-26          [-1, 128, 32, 32]             256\n",
      "             ReLU-27          [-1, 128, 32, 32]               0\n",
      "           Conv2d-28           [-1, 32, 32, 32]          36,864\n",
      "      BatchNorm2d-29          [-1, 192, 32, 32]             384\n",
      "             ReLU-30          [-1, 192, 32, 32]               0\n",
      "           Conv2d-31          [-1, 128, 32, 32]          24,576\n",
      "      BatchNorm2d-32          [-1, 128, 32, 32]             256\n",
      "             ReLU-33          [-1, 128, 32, 32]               0\n",
      "           Conv2d-34           [-1, 32, 32, 32]          36,864\n",
      "      BatchNorm2d-35          [-1, 224, 32, 32]             448\n",
      "             ReLU-36          [-1, 224, 32, 32]               0\n",
      "           Conv2d-37          [-1, 128, 32, 32]          28,672\n",
      "      BatchNorm2d-38          [-1, 128, 32, 32]             256\n",
      "             ReLU-39          [-1, 128, 32, 32]               0\n",
      "           Conv2d-40           [-1, 32, 32, 32]          36,864\n",
      "      _DenseBlock-41          [-1, 256, 32, 32]               0\n",
      "      BatchNorm2d-42          [-1, 256, 32, 32]             512\n",
      "             ReLU-43          [-1, 256, 32, 32]               0\n",
      "           Conv2d-44          [-1, 128, 32, 32]          32,768\n",
      "        AvgPool2d-45          [-1, 128, 16, 16]               0\n",
      "      BatchNorm2d-46          [-1, 128, 16, 16]             256\n",
      "             ReLU-47          [-1, 128, 16, 16]               0\n",
      "           Conv2d-48          [-1, 128, 16, 16]          16,384\n",
      "      BatchNorm2d-49          [-1, 128, 16, 16]             256\n",
      "             ReLU-50          [-1, 128, 16, 16]               0\n",
      "           Conv2d-51           [-1, 32, 16, 16]          36,864\n",
      "      BatchNorm2d-52          [-1, 160, 16, 16]             320\n",
      "             ReLU-53          [-1, 160, 16, 16]               0\n",
      "           Conv2d-54          [-1, 128, 16, 16]          20,480\n",
      "      BatchNorm2d-55          [-1, 128, 16, 16]             256\n",
      "             ReLU-56          [-1, 128, 16, 16]               0\n",
      "           Conv2d-57           [-1, 32, 16, 16]          36,864\n",
      "      BatchNorm2d-58          [-1, 192, 16, 16]             384\n",
      "             ReLU-59          [-1, 192, 16, 16]               0\n",
      "           Conv2d-60          [-1, 128, 16, 16]          24,576\n",
      "      BatchNorm2d-61          [-1, 128, 16, 16]             256\n",
      "             ReLU-62          [-1, 128, 16, 16]               0\n",
      "           Conv2d-63           [-1, 32, 16, 16]          36,864\n",
      "      BatchNorm2d-64          [-1, 224, 16, 16]             448\n",
      "             ReLU-65          [-1, 224, 16, 16]               0\n",
      "           Conv2d-66          [-1, 128, 16, 16]          28,672\n",
      "      BatchNorm2d-67          [-1, 128, 16, 16]             256\n",
      "             ReLU-68          [-1, 128, 16, 16]               0\n",
      "           Conv2d-69           [-1, 32, 16, 16]          36,864\n",
      "      BatchNorm2d-70          [-1, 256, 16, 16]             512\n",
      "             ReLU-71          [-1, 256, 16, 16]               0\n",
      "           Conv2d-72          [-1, 128, 16, 16]          32,768\n",
      "      BatchNorm2d-73          [-1, 128, 16, 16]             256\n",
      "             ReLU-74          [-1, 128, 16, 16]               0\n",
      "           Conv2d-75           [-1, 32, 16, 16]          36,864\n",
      "      BatchNorm2d-76          [-1, 288, 16, 16]             576\n",
      "             ReLU-77          [-1, 288, 16, 16]               0\n",
      "           Conv2d-78          [-1, 128, 16, 16]          36,864\n",
      "      BatchNorm2d-79          [-1, 128, 16, 16]             256\n",
      "             ReLU-80          [-1, 128, 16, 16]               0\n",
      "           Conv2d-81           [-1, 32, 16, 16]          36,864\n",
      "      BatchNorm2d-82          [-1, 320, 16, 16]             640\n",
      "             ReLU-83          [-1, 320, 16, 16]               0\n",
      "           Conv2d-84          [-1, 128, 16, 16]          40,960\n",
      "      BatchNorm2d-85          [-1, 128, 16, 16]             256\n",
      "             ReLU-86          [-1, 128, 16, 16]               0\n",
      "           Conv2d-87           [-1, 32, 16, 16]          36,864\n",
      "      BatchNorm2d-88          [-1, 352, 16, 16]             704\n",
      "             ReLU-89          [-1, 352, 16, 16]               0\n",
      "           Conv2d-90          [-1, 128, 16, 16]          45,056\n",
      "      BatchNorm2d-91          [-1, 128, 16, 16]             256\n",
      "             ReLU-92          [-1, 128, 16, 16]               0\n",
      "           Conv2d-93           [-1, 32, 16, 16]          36,864\n",
      "      BatchNorm2d-94          [-1, 384, 16, 16]             768\n",
      "             ReLU-95          [-1, 384, 16, 16]               0\n",
      "           Conv2d-96          [-1, 128, 16, 16]          49,152\n",
      "      BatchNorm2d-97          [-1, 128, 16, 16]             256\n",
      "             ReLU-98          [-1, 128, 16, 16]               0\n",
      "           Conv2d-99           [-1, 32, 16, 16]          36,864\n",
      "     BatchNorm2d-100          [-1, 416, 16, 16]             832\n",
      "            ReLU-101          [-1, 416, 16, 16]               0\n",
      "          Conv2d-102          [-1, 128, 16, 16]          53,248\n",
      "     BatchNorm2d-103          [-1, 128, 16, 16]             256\n",
      "            ReLU-104          [-1, 128, 16, 16]               0\n",
      "          Conv2d-105           [-1, 32, 16, 16]          36,864\n",
      "     BatchNorm2d-106          [-1, 448, 16, 16]             896\n",
      "            ReLU-107          [-1, 448, 16, 16]               0\n",
      "          Conv2d-108          [-1, 128, 16, 16]          57,344\n",
      "     BatchNorm2d-109          [-1, 128, 16, 16]             256\n",
      "            ReLU-110          [-1, 128, 16, 16]               0\n",
      "          Conv2d-111           [-1, 32, 16, 16]          36,864\n",
      "     BatchNorm2d-112          [-1, 480, 16, 16]             960\n",
      "            ReLU-113          [-1, 480, 16, 16]               0\n",
      "          Conv2d-114          [-1, 128, 16, 16]          61,440\n",
      "     BatchNorm2d-115          [-1, 128, 16, 16]             256\n",
      "            ReLU-116          [-1, 128, 16, 16]               0\n",
      "          Conv2d-117           [-1, 32, 16, 16]          36,864\n",
      "     _DenseBlock-118          [-1, 512, 16, 16]               0\n",
      "     BatchNorm2d-119          [-1, 512, 16, 16]           1,024\n",
      "            ReLU-120          [-1, 512, 16, 16]               0\n",
      "          Conv2d-121          [-1, 256, 16, 16]         131,072\n",
      "       AvgPool2d-122            [-1, 256, 8, 8]               0\n",
      "     BatchNorm2d-123            [-1, 256, 8, 8]             512\n",
      "            ReLU-124            [-1, 256, 8, 8]               0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Conv2d-125            [-1, 128, 8, 8]          32,768\n",
      "     BatchNorm2d-126            [-1, 128, 8, 8]             256\n",
      "            ReLU-127            [-1, 128, 8, 8]               0\n",
      "          Conv2d-128             [-1, 32, 8, 8]          36,864\n",
      "     BatchNorm2d-129            [-1, 288, 8, 8]             576\n",
      "            ReLU-130            [-1, 288, 8, 8]               0\n",
      "          Conv2d-131            [-1, 128, 8, 8]          36,864\n",
      "     BatchNorm2d-132            [-1, 128, 8, 8]             256\n",
      "            ReLU-133            [-1, 128, 8, 8]               0\n",
      "          Conv2d-134             [-1, 32, 8, 8]          36,864\n",
      "     BatchNorm2d-135            [-1, 320, 8, 8]             640\n",
      "            ReLU-136            [-1, 320, 8, 8]               0\n",
      "          Conv2d-137            [-1, 128, 8, 8]          40,960\n",
      "     BatchNorm2d-138            [-1, 128, 8, 8]             256\n",
      "            ReLU-139            [-1, 128, 8, 8]               0\n",
      "          Conv2d-140             [-1, 32, 8, 8]          36,864\n",
      "     BatchNorm2d-141            [-1, 352, 8, 8]             704\n",
      "            ReLU-142            [-1, 352, 8, 8]               0\n",
      "          Conv2d-143            [-1, 128, 8, 8]          45,056\n",
      "     BatchNorm2d-144            [-1, 128, 8, 8]             256\n",
      "            ReLU-145            [-1, 128, 8, 8]               0\n",
      "          Conv2d-146             [-1, 32, 8, 8]          36,864\n",
      "     BatchNorm2d-147            [-1, 384, 8, 8]             768\n",
      "            ReLU-148            [-1, 384, 8, 8]               0\n",
      "          Conv2d-149            [-1, 128, 8, 8]          49,152\n",
      "     BatchNorm2d-150            [-1, 128, 8, 8]             256\n",
      "            ReLU-151            [-1, 128, 8, 8]               0\n",
      "          Conv2d-152             [-1, 32, 8, 8]          36,864\n",
      "     BatchNorm2d-153            [-1, 416, 8, 8]             832\n",
      "            ReLU-154            [-1, 416, 8, 8]               0\n",
      "          Conv2d-155            [-1, 128, 8, 8]          53,248\n",
      "     BatchNorm2d-156            [-1, 128, 8, 8]             256\n",
      "            ReLU-157            [-1, 128, 8, 8]               0\n",
      "          Conv2d-158             [-1, 32, 8, 8]          36,864\n",
      "     BatchNorm2d-159            [-1, 448, 8, 8]             896\n",
      "            ReLU-160            [-1, 448, 8, 8]               0\n",
      "          Conv2d-161            [-1, 128, 8, 8]          57,344\n",
      "     BatchNorm2d-162            [-1, 128, 8, 8]             256\n",
      "            ReLU-163            [-1, 128, 8, 8]               0\n",
      "          Conv2d-164             [-1, 32, 8, 8]          36,864\n",
      "     BatchNorm2d-165            [-1, 480, 8, 8]             960\n",
      "            ReLU-166            [-1, 480, 8, 8]               0\n",
      "          Conv2d-167            [-1, 128, 8, 8]          61,440\n",
      "     BatchNorm2d-168            [-1, 128, 8, 8]             256\n",
      "            ReLU-169            [-1, 128, 8, 8]               0\n",
      "          Conv2d-170             [-1, 32, 8, 8]          36,864\n",
      "     BatchNorm2d-171            [-1, 512, 8, 8]           1,024\n",
      "            ReLU-172            [-1, 512, 8, 8]               0\n",
      "          Conv2d-173            [-1, 128, 8, 8]          65,536\n",
      "     BatchNorm2d-174            [-1, 128, 8, 8]             256\n",
      "            ReLU-175            [-1, 128, 8, 8]               0\n",
      "          Conv2d-176             [-1, 32, 8, 8]          36,864\n",
      "     BatchNorm2d-177            [-1, 544, 8, 8]           1,088\n",
      "            ReLU-178            [-1, 544, 8, 8]               0\n",
      "          Conv2d-179            [-1, 128, 8, 8]          69,632\n",
      "     BatchNorm2d-180            [-1, 128, 8, 8]             256\n",
      "            ReLU-181            [-1, 128, 8, 8]               0\n",
      "          Conv2d-182             [-1, 32, 8, 8]          36,864\n",
      "     BatchNorm2d-183            [-1, 576, 8, 8]           1,152\n",
      "            ReLU-184            [-1, 576, 8, 8]               0\n",
      "          Conv2d-185            [-1, 128, 8, 8]          73,728\n",
      "     BatchNorm2d-186            [-1, 128, 8, 8]             256\n",
      "            ReLU-187            [-1, 128, 8, 8]               0\n",
      "          Conv2d-188             [-1, 32, 8, 8]          36,864\n",
      "     BatchNorm2d-189            [-1, 608, 8, 8]           1,216\n",
      "            ReLU-190            [-1, 608, 8, 8]               0\n",
      "          Conv2d-191            [-1, 128, 8, 8]          77,824\n",
      "     BatchNorm2d-192            [-1, 128, 8, 8]             256\n",
      "            ReLU-193            [-1, 128, 8, 8]               0\n",
      "          Conv2d-194             [-1, 32, 8, 8]          36,864\n",
      "     BatchNorm2d-195            [-1, 640, 8, 8]           1,280\n",
      "            ReLU-196            [-1, 640, 8, 8]               0\n",
      "          Conv2d-197            [-1, 128, 8, 8]          81,920\n",
      "     BatchNorm2d-198            [-1, 128, 8, 8]             256\n",
      "            ReLU-199            [-1, 128, 8, 8]               0\n",
      "          Conv2d-200             [-1, 32, 8, 8]          36,864\n",
      "     BatchNorm2d-201            [-1, 672, 8, 8]           1,344\n",
      "            ReLU-202            [-1, 672, 8, 8]               0\n",
      "          Conv2d-203            [-1, 128, 8, 8]          86,016\n",
      "     BatchNorm2d-204            [-1, 128, 8, 8]             256\n",
      "            ReLU-205            [-1, 128, 8, 8]               0\n",
      "          Conv2d-206             [-1, 32, 8, 8]          36,864\n",
      "     BatchNorm2d-207            [-1, 704, 8, 8]           1,408\n",
      "            ReLU-208            [-1, 704, 8, 8]               0\n",
      "          Conv2d-209            [-1, 128, 8, 8]          90,112\n",
      "     BatchNorm2d-210            [-1, 128, 8, 8]             256\n",
      "            ReLU-211            [-1, 128, 8, 8]               0\n",
      "          Conv2d-212             [-1, 32, 8, 8]          36,864\n",
      "     BatchNorm2d-213            [-1, 736, 8, 8]           1,472\n",
      "            ReLU-214            [-1, 736, 8, 8]               0\n",
      "          Conv2d-215            [-1, 128, 8, 8]          94,208\n",
      "     BatchNorm2d-216            [-1, 128, 8, 8]             256\n",
      "            ReLU-217            [-1, 128, 8, 8]               0\n",
      "          Conv2d-218             [-1, 32, 8, 8]          36,864\n",
      "     BatchNorm2d-219            [-1, 768, 8, 8]           1,536\n",
      "            ReLU-220            [-1, 768, 8, 8]               0\n",
      "          Conv2d-221            [-1, 128, 8, 8]          98,304\n",
      "     BatchNorm2d-222            [-1, 128, 8, 8]             256\n",
      "            ReLU-223            [-1, 128, 8, 8]               0\n",
      "          Conv2d-224             [-1, 32, 8, 8]          36,864\n",
      "     BatchNorm2d-225            [-1, 800, 8, 8]           1,600\n",
      "            ReLU-226            [-1, 800, 8, 8]               0\n",
      "          Conv2d-227            [-1, 128, 8, 8]         102,400\n",
      "     BatchNorm2d-228            [-1, 128, 8, 8]             256\n",
      "            ReLU-229            [-1, 128, 8, 8]               0\n",
      "          Conv2d-230             [-1, 32, 8, 8]          36,864\n",
      "     BatchNorm2d-231            [-1, 832, 8, 8]           1,664\n",
      "            ReLU-232            [-1, 832, 8, 8]               0\n",
      "          Conv2d-233            [-1, 128, 8, 8]         106,496\n",
      "     BatchNorm2d-234            [-1, 128, 8, 8]             256\n",
      "            ReLU-235            [-1, 128, 8, 8]               0\n",
      "          Conv2d-236             [-1, 32, 8, 8]          36,864\n",
      "     BatchNorm2d-237            [-1, 864, 8, 8]           1,728\n",
      "            ReLU-238            [-1, 864, 8, 8]               0\n",
      "          Conv2d-239            [-1, 128, 8, 8]         110,592\n",
      "     BatchNorm2d-240            [-1, 128, 8, 8]             256\n",
      "            ReLU-241            [-1, 128, 8, 8]               0\n",
      "          Conv2d-242             [-1, 32, 8, 8]          36,864\n",
      "     BatchNorm2d-243            [-1, 896, 8, 8]           1,792\n",
      "            ReLU-244            [-1, 896, 8, 8]               0\n",
      "          Conv2d-245            [-1, 128, 8, 8]         114,688\n",
      "     BatchNorm2d-246            [-1, 128, 8, 8]             256\n",
      "            ReLU-247            [-1, 128, 8, 8]               0\n",
      "          Conv2d-248             [-1, 32, 8, 8]          36,864\n",
      "     BatchNorm2d-249            [-1, 928, 8, 8]           1,856\n",
      "            ReLU-250            [-1, 928, 8, 8]               0\n",
      "          Conv2d-251            [-1, 128, 8, 8]         118,784\n",
      "     BatchNorm2d-252            [-1, 128, 8, 8]             256\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            ReLU-253            [-1, 128, 8, 8]               0\n",
      "          Conv2d-254             [-1, 32, 8, 8]          36,864\n",
      "     BatchNorm2d-255            [-1, 960, 8, 8]           1,920\n",
      "            ReLU-256            [-1, 960, 8, 8]               0\n",
      "          Conv2d-257            [-1, 128, 8, 8]         122,880\n",
      "     BatchNorm2d-258            [-1, 128, 8, 8]             256\n",
      "            ReLU-259            [-1, 128, 8, 8]               0\n",
      "          Conv2d-260             [-1, 32, 8, 8]          36,864\n",
      "     BatchNorm2d-261            [-1, 992, 8, 8]           1,984\n",
      "            ReLU-262            [-1, 992, 8, 8]               0\n",
      "          Conv2d-263            [-1, 128, 8, 8]         126,976\n",
      "     BatchNorm2d-264            [-1, 128, 8, 8]             256\n",
      "            ReLU-265            [-1, 128, 8, 8]               0\n",
      "          Conv2d-266             [-1, 32, 8, 8]          36,864\n",
      "     _DenseBlock-267           [-1, 1024, 8, 8]               0\n",
      "     BatchNorm2d-268           [-1, 1024, 8, 8]           2,048\n",
      "            ReLU-269           [-1, 1024, 8, 8]               0\n",
      "          Conv2d-270            [-1, 512, 8, 8]         524,288\n",
      "       AvgPool2d-271            [-1, 512, 4, 4]               0\n",
      "     BatchNorm2d-272            [-1, 512, 4, 4]           1,024\n",
      "            ReLU-273            [-1, 512, 4, 4]               0\n",
      "          Conv2d-274            [-1, 128, 4, 4]          65,536\n",
      "     BatchNorm2d-275            [-1, 128, 4, 4]             256\n",
      "            ReLU-276            [-1, 128, 4, 4]               0\n",
      "          Conv2d-277             [-1, 32, 4, 4]          36,864\n",
      "     BatchNorm2d-278            [-1, 544, 4, 4]           1,088\n",
      "            ReLU-279            [-1, 544, 4, 4]               0\n",
      "          Conv2d-280            [-1, 128, 4, 4]          69,632\n",
      "     BatchNorm2d-281            [-1, 128, 4, 4]             256\n",
      "            ReLU-282            [-1, 128, 4, 4]               0\n",
      "          Conv2d-283             [-1, 32, 4, 4]          36,864\n",
      "     BatchNorm2d-284            [-1, 576, 4, 4]           1,152\n",
      "            ReLU-285            [-1, 576, 4, 4]               0\n",
      "          Conv2d-286            [-1, 128, 4, 4]          73,728\n",
      "     BatchNorm2d-287            [-1, 128, 4, 4]             256\n",
      "            ReLU-288            [-1, 128, 4, 4]               0\n",
      "          Conv2d-289             [-1, 32, 4, 4]          36,864\n",
      "     BatchNorm2d-290            [-1, 608, 4, 4]           1,216\n",
      "            ReLU-291            [-1, 608, 4, 4]               0\n",
      "          Conv2d-292            [-1, 128, 4, 4]          77,824\n",
      "     BatchNorm2d-293            [-1, 128, 4, 4]             256\n",
      "            ReLU-294            [-1, 128, 4, 4]               0\n",
      "          Conv2d-295             [-1, 32, 4, 4]          36,864\n",
      "     BatchNorm2d-296            [-1, 640, 4, 4]           1,280\n",
      "            ReLU-297            [-1, 640, 4, 4]               0\n",
      "          Conv2d-298            [-1, 128, 4, 4]          81,920\n",
      "     BatchNorm2d-299            [-1, 128, 4, 4]             256\n",
      "            ReLU-300            [-1, 128, 4, 4]               0\n",
      "          Conv2d-301             [-1, 32, 4, 4]          36,864\n",
      "     BatchNorm2d-302            [-1, 672, 4, 4]           1,344\n",
      "            ReLU-303            [-1, 672, 4, 4]               0\n",
      "          Conv2d-304            [-1, 128, 4, 4]          86,016\n",
      "     BatchNorm2d-305            [-1, 128, 4, 4]             256\n",
      "            ReLU-306            [-1, 128, 4, 4]               0\n",
      "          Conv2d-307             [-1, 32, 4, 4]          36,864\n",
      "     BatchNorm2d-308            [-1, 704, 4, 4]           1,408\n",
      "            ReLU-309            [-1, 704, 4, 4]               0\n",
      "          Conv2d-310            [-1, 128, 4, 4]          90,112\n",
      "     BatchNorm2d-311            [-1, 128, 4, 4]             256\n",
      "            ReLU-312            [-1, 128, 4, 4]               0\n",
      "          Conv2d-313             [-1, 32, 4, 4]          36,864\n",
      "     BatchNorm2d-314            [-1, 736, 4, 4]           1,472\n",
      "            ReLU-315            [-1, 736, 4, 4]               0\n",
      "          Conv2d-316            [-1, 128, 4, 4]          94,208\n",
      "     BatchNorm2d-317            [-1, 128, 4, 4]             256\n",
      "            ReLU-318            [-1, 128, 4, 4]               0\n",
      "          Conv2d-319             [-1, 32, 4, 4]          36,864\n",
      "     BatchNorm2d-320            [-1, 768, 4, 4]           1,536\n",
      "            ReLU-321            [-1, 768, 4, 4]               0\n",
      "          Conv2d-322            [-1, 128, 4, 4]          98,304\n",
      "     BatchNorm2d-323            [-1, 128, 4, 4]             256\n",
      "            ReLU-324            [-1, 128, 4, 4]               0\n",
      "          Conv2d-325             [-1, 32, 4, 4]          36,864\n",
      "     BatchNorm2d-326            [-1, 800, 4, 4]           1,600\n",
      "            ReLU-327            [-1, 800, 4, 4]               0\n",
      "          Conv2d-328            [-1, 128, 4, 4]         102,400\n",
      "     BatchNorm2d-329            [-1, 128, 4, 4]             256\n",
      "            ReLU-330            [-1, 128, 4, 4]               0\n",
      "          Conv2d-331             [-1, 32, 4, 4]          36,864\n",
      "     BatchNorm2d-332            [-1, 832, 4, 4]           1,664\n",
      "            ReLU-333            [-1, 832, 4, 4]               0\n",
      "          Conv2d-334            [-1, 128, 4, 4]         106,496\n",
      "     BatchNorm2d-335            [-1, 128, 4, 4]             256\n",
      "            ReLU-336            [-1, 128, 4, 4]               0\n",
      "          Conv2d-337             [-1, 32, 4, 4]          36,864\n",
      "     BatchNorm2d-338            [-1, 864, 4, 4]           1,728\n",
      "            ReLU-339            [-1, 864, 4, 4]               0\n",
      "          Conv2d-340            [-1, 128, 4, 4]         110,592\n",
      "     BatchNorm2d-341            [-1, 128, 4, 4]             256\n",
      "            ReLU-342            [-1, 128, 4, 4]               0\n",
      "          Conv2d-343             [-1, 32, 4, 4]          36,864\n",
      "     BatchNorm2d-344            [-1, 896, 4, 4]           1,792\n",
      "            ReLU-345            [-1, 896, 4, 4]               0\n",
      "          Conv2d-346            [-1, 128, 4, 4]         114,688\n",
      "     BatchNorm2d-347            [-1, 128, 4, 4]             256\n",
      "            ReLU-348            [-1, 128, 4, 4]               0\n",
      "          Conv2d-349             [-1, 32, 4, 4]          36,864\n",
      "     BatchNorm2d-350            [-1, 928, 4, 4]           1,856\n",
      "            ReLU-351            [-1, 928, 4, 4]               0\n",
      "          Conv2d-352            [-1, 128, 4, 4]         118,784\n",
      "     BatchNorm2d-353            [-1, 128, 4, 4]             256\n",
      "            ReLU-354            [-1, 128, 4, 4]               0\n",
      "          Conv2d-355             [-1, 32, 4, 4]          36,864\n",
      "     BatchNorm2d-356            [-1, 960, 4, 4]           1,920\n",
      "            ReLU-357            [-1, 960, 4, 4]               0\n",
      "          Conv2d-358            [-1, 128, 4, 4]         122,880\n",
      "     BatchNorm2d-359            [-1, 128, 4, 4]             256\n",
      "            ReLU-360            [-1, 128, 4, 4]               0\n",
      "          Conv2d-361             [-1, 32, 4, 4]          36,864\n",
      "     BatchNorm2d-362            [-1, 992, 4, 4]           1,984\n",
      "            ReLU-363            [-1, 992, 4, 4]               0\n",
      "          Conv2d-364            [-1, 128, 4, 4]         126,976\n",
      "     BatchNorm2d-365            [-1, 128, 4, 4]             256\n",
      "            ReLU-366            [-1, 128, 4, 4]               0\n",
      "          Conv2d-367             [-1, 32, 4, 4]          36,864\n",
      "     _DenseBlock-368           [-1, 1024, 4, 4]               0\n",
      "     BatchNorm2d-369           [-1, 1024, 4, 4]           2,048\n",
      "          Linear-370                  [-1, 128]         131,200\n",
      "        DenseNet-371                  [-1, 128]               0\n",
      "================================================================\n",
      "Total params: 7,085,056\n",
      "Trainable params: 7,085,056\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.19\n",
      "Forward/backward pass size (MB): 99.69\n",
      "Params size (MB): 27.03\n",
      "Estimated Total Size (MB): 126.90\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(SpectrogramEncoderNet(), input_size=(3, IMG_HEIGHT, IMG_HEIGHT), device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 64, 64]           9,408\n",
      "       BatchNorm2d-2           [-1, 64, 64, 64]             128\n",
      "              ReLU-3           [-1, 64, 64, 64]               0\n",
      "         MaxPool2d-4           [-1, 64, 32, 32]               0\n",
      "       BatchNorm2d-5           [-1, 64, 32, 32]             128\n",
      "              ReLU-6           [-1, 64, 32, 32]               0\n",
      "            Conv2d-7          [-1, 128, 32, 32]           8,192\n",
      "       BatchNorm2d-8          [-1, 128, 32, 32]             256\n",
      "              ReLU-9          [-1, 128, 32, 32]               0\n",
      "           Conv2d-10           [-1, 32, 32, 32]          36,864\n",
      "      BatchNorm2d-11           [-1, 96, 32, 32]             192\n",
      "             ReLU-12           [-1, 96, 32, 32]               0\n",
      "           Conv2d-13          [-1, 128, 32, 32]          12,288\n",
      "      BatchNorm2d-14          [-1, 128, 32, 32]             256\n",
      "             ReLU-15          [-1, 128, 32, 32]               0\n",
      "           Conv2d-16           [-1, 32, 32, 32]          36,864\n",
      "      BatchNorm2d-17          [-1, 128, 32, 32]             256\n",
      "             ReLU-18          [-1, 128, 32, 32]               0\n",
      "           Conv2d-19          [-1, 128, 32, 32]          16,384\n",
      "      BatchNorm2d-20          [-1, 128, 32, 32]             256\n",
      "             ReLU-21          [-1, 128, 32, 32]               0\n",
      "           Conv2d-22           [-1, 32, 32, 32]          36,864\n",
      "      BatchNorm2d-23          [-1, 160, 32, 32]             320\n",
      "             ReLU-24          [-1, 160, 32, 32]               0\n",
      "           Conv2d-25          [-1, 128, 32, 32]          20,480\n",
      "      BatchNorm2d-26          [-1, 128, 32, 32]             256\n",
      "             ReLU-27          [-1, 128, 32, 32]               0\n",
      "           Conv2d-28           [-1, 32, 32, 32]          36,864\n",
      "      BatchNorm2d-29          [-1, 192, 32, 32]             384\n",
      "             ReLU-30          [-1, 192, 32, 32]               0\n",
      "           Conv2d-31          [-1, 128, 32, 32]          24,576\n",
      "      BatchNorm2d-32          [-1, 128, 32, 32]             256\n",
      "             ReLU-33          [-1, 128, 32, 32]               0\n",
      "           Conv2d-34           [-1, 32, 32, 32]          36,864\n",
      "      BatchNorm2d-35          [-1, 224, 32, 32]             448\n",
      "             ReLU-36          [-1, 224, 32, 32]               0\n",
      "           Conv2d-37          [-1, 128, 32, 32]          28,672\n",
      "      BatchNorm2d-38          [-1, 128, 32, 32]             256\n",
      "             ReLU-39          [-1, 128, 32, 32]               0\n",
      "           Conv2d-40           [-1, 32, 32, 32]          36,864\n",
      "      _DenseBlock-41          [-1, 256, 32, 32]               0\n",
      "      BatchNorm2d-42          [-1, 256, 32, 32]             512\n",
      "             ReLU-43          [-1, 256, 32, 32]               0\n",
      "           Conv2d-44          [-1, 128, 32, 32]          32,768\n",
      "        AvgPool2d-45          [-1, 128, 16, 16]               0\n",
      "      BatchNorm2d-46          [-1, 128, 16, 16]             256\n",
      "             ReLU-47          [-1, 128, 16, 16]               0\n",
      "           Conv2d-48          [-1, 128, 16, 16]          16,384\n",
      "      BatchNorm2d-49          [-1, 128, 16, 16]             256\n",
      "             ReLU-50          [-1, 128, 16, 16]               0\n",
      "           Conv2d-51           [-1, 32, 16, 16]          36,864\n",
      "      BatchNorm2d-52          [-1, 160, 16, 16]             320\n",
      "             ReLU-53          [-1, 160, 16, 16]               0\n",
      "           Conv2d-54          [-1, 128, 16, 16]          20,480\n",
      "      BatchNorm2d-55          [-1, 128, 16, 16]             256\n",
      "             ReLU-56          [-1, 128, 16, 16]               0\n",
      "           Conv2d-57           [-1, 32, 16, 16]          36,864\n",
      "      BatchNorm2d-58          [-1, 192, 16, 16]             384\n",
      "             ReLU-59          [-1, 192, 16, 16]               0\n",
      "           Conv2d-60          [-1, 128, 16, 16]          24,576\n",
      "      BatchNorm2d-61          [-1, 128, 16, 16]             256\n",
      "             ReLU-62          [-1, 128, 16, 16]               0\n",
      "           Conv2d-63           [-1, 32, 16, 16]          36,864\n",
      "      BatchNorm2d-64          [-1, 224, 16, 16]             448\n",
      "             ReLU-65          [-1, 224, 16, 16]               0\n",
      "           Conv2d-66          [-1, 128, 16, 16]          28,672\n",
      "      BatchNorm2d-67          [-1, 128, 16, 16]             256\n",
      "             ReLU-68          [-1, 128, 16, 16]               0\n",
      "           Conv2d-69           [-1, 32, 16, 16]          36,864\n",
      "      BatchNorm2d-70          [-1, 256, 16, 16]             512\n",
      "             ReLU-71          [-1, 256, 16, 16]               0\n",
      "           Conv2d-72          [-1, 128, 16, 16]          32,768\n",
      "      BatchNorm2d-73          [-1, 128, 16, 16]             256\n",
      "             ReLU-74          [-1, 128, 16, 16]               0\n",
      "           Conv2d-75           [-1, 32, 16, 16]          36,864\n",
      "      BatchNorm2d-76          [-1, 288, 16, 16]             576\n",
      "             ReLU-77          [-1, 288, 16, 16]               0\n",
      "           Conv2d-78          [-1, 128, 16, 16]          36,864\n",
      "      BatchNorm2d-79          [-1, 128, 16, 16]             256\n",
      "             ReLU-80          [-1, 128, 16, 16]               0\n",
      "           Conv2d-81           [-1, 32, 16, 16]          36,864\n",
      "      BatchNorm2d-82          [-1, 320, 16, 16]             640\n",
      "             ReLU-83          [-1, 320, 16, 16]               0\n",
      "           Conv2d-84          [-1, 128, 16, 16]          40,960\n",
      "      BatchNorm2d-85          [-1, 128, 16, 16]             256\n",
      "             ReLU-86          [-1, 128, 16, 16]               0\n",
      "           Conv2d-87           [-1, 32, 16, 16]          36,864\n",
      "      BatchNorm2d-88          [-1, 352, 16, 16]             704\n",
      "             ReLU-89          [-1, 352, 16, 16]               0\n",
      "           Conv2d-90          [-1, 128, 16, 16]          45,056\n",
      "      BatchNorm2d-91          [-1, 128, 16, 16]             256\n",
      "             ReLU-92          [-1, 128, 16, 16]               0\n",
      "           Conv2d-93           [-1, 32, 16, 16]          36,864\n",
      "      BatchNorm2d-94          [-1, 384, 16, 16]             768\n",
      "             ReLU-95          [-1, 384, 16, 16]               0\n",
      "           Conv2d-96          [-1, 128, 16, 16]          49,152\n",
      "      BatchNorm2d-97          [-1, 128, 16, 16]             256\n",
      "             ReLU-98          [-1, 128, 16, 16]               0\n",
      "           Conv2d-99           [-1, 32, 16, 16]          36,864\n",
      "     BatchNorm2d-100          [-1, 416, 16, 16]             832\n",
      "            ReLU-101          [-1, 416, 16, 16]               0\n",
      "          Conv2d-102          [-1, 128, 16, 16]          53,248\n",
      "     BatchNorm2d-103          [-1, 128, 16, 16]             256\n",
      "            ReLU-104          [-1, 128, 16, 16]               0\n",
      "          Conv2d-105           [-1, 32, 16, 16]          36,864\n",
      "     BatchNorm2d-106          [-1, 448, 16, 16]             896\n",
      "            ReLU-107          [-1, 448, 16, 16]               0\n",
      "          Conv2d-108          [-1, 128, 16, 16]          57,344\n",
      "     BatchNorm2d-109          [-1, 128, 16, 16]             256\n",
      "            ReLU-110          [-1, 128, 16, 16]               0\n",
      "          Conv2d-111           [-1, 32, 16, 16]          36,864\n",
      "     BatchNorm2d-112          [-1, 480, 16, 16]             960\n",
      "            ReLU-113          [-1, 480, 16, 16]               0\n",
      "          Conv2d-114          [-1, 128, 16, 16]          61,440\n",
      "     BatchNorm2d-115          [-1, 128, 16, 16]             256\n",
      "            ReLU-116          [-1, 128, 16, 16]               0\n",
      "          Conv2d-117           [-1, 32, 16, 16]          36,864\n",
      "     _DenseBlock-118          [-1, 512, 16, 16]               0\n",
      "     BatchNorm2d-119          [-1, 512, 16, 16]           1,024\n",
      "            ReLU-120          [-1, 512, 16, 16]               0\n",
      "          Conv2d-121          [-1, 256, 16, 16]         131,072\n",
      "       AvgPool2d-122            [-1, 256, 8, 8]               0\n",
      "     BatchNorm2d-123            [-1, 256, 8, 8]             512\n",
      "            ReLU-124            [-1, 256, 8, 8]               0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Conv2d-125            [-1, 128, 8, 8]          32,768\n",
      "     BatchNorm2d-126            [-1, 128, 8, 8]             256\n",
      "            ReLU-127            [-1, 128, 8, 8]               0\n",
      "          Conv2d-128             [-1, 32, 8, 8]          36,864\n",
      "     BatchNorm2d-129            [-1, 288, 8, 8]             576\n",
      "            ReLU-130            [-1, 288, 8, 8]               0\n",
      "          Conv2d-131            [-1, 128, 8, 8]          36,864\n",
      "     BatchNorm2d-132            [-1, 128, 8, 8]             256\n",
      "            ReLU-133            [-1, 128, 8, 8]               0\n",
      "          Conv2d-134             [-1, 32, 8, 8]          36,864\n",
      "     BatchNorm2d-135            [-1, 320, 8, 8]             640\n",
      "            ReLU-136            [-1, 320, 8, 8]               0\n",
      "          Conv2d-137            [-1, 128, 8, 8]          40,960\n",
      "     BatchNorm2d-138            [-1, 128, 8, 8]             256\n",
      "            ReLU-139            [-1, 128, 8, 8]               0\n",
      "          Conv2d-140             [-1, 32, 8, 8]          36,864\n",
      "     BatchNorm2d-141            [-1, 352, 8, 8]             704\n",
      "            ReLU-142            [-1, 352, 8, 8]               0\n",
      "          Conv2d-143            [-1, 128, 8, 8]          45,056\n",
      "     BatchNorm2d-144            [-1, 128, 8, 8]             256\n",
      "            ReLU-145            [-1, 128, 8, 8]               0\n",
      "          Conv2d-146             [-1, 32, 8, 8]          36,864\n",
      "     BatchNorm2d-147            [-1, 384, 8, 8]             768\n",
      "            ReLU-148            [-1, 384, 8, 8]               0\n",
      "          Conv2d-149            [-1, 128, 8, 8]          49,152\n",
      "     BatchNorm2d-150            [-1, 128, 8, 8]             256\n",
      "            ReLU-151            [-1, 128, 8, 8]               0\n",
      "          Conv2d-152             [-1, 32, 8, 8]          36,864\n",
      "     BatchNorm2d-153            [-1, 416, 8, 8]             832\n",
      "            ReLU-154            [-1, 416, 8, 8]               0\n",
      "          Conv2d-155            [-1, 128, 8, 8]          53,248\n",
      "     BatchNorm2d-156            [-1, 128, 8, 8]             256\n",
      "            ReLU-157            [-1, 128, 8, 8]               0\n",
      "          Conv2d-158             [-1, 32, 8, 8]          36,864\n",
      "     BatchNorm2d-159            [-1, 448, 8, 8]             896\n",
      "            ReLU-160            [-1, 448, 8, 8]               0\n",
      "          Conv2d-161            [-1, 128, 8, 8]          57,344\n",
      "     BatchNorm2d-162            [-1, 128, 8, 8]             256\n",
      "            ReLU-163            [-1, 128, 8, 8]               0\n",
      "          Conv2d-164             [-1, 32, 8, 8]          36,864\n",
      "     BatchNorm2d-165            [-1, 480, 8, 8]             960\n",
      "            ReLU-166            [-1, 480, 8, 8]               0\n",
      "          Conv2d-167            [-1, 128, 8, 8]          61,440\n",
      "     BatchNorm2d-168            [-1, 128, 8, 8]             256\n",
      "            ReLU-169            [-1, 128, 8, 8]               0\n",
      "          Conv2d-170             [-1, 32, 8, 8]          36,864\n",
      "     BatchNorm2d-171            [-1, 512, 8, 8]           1,024\n",
      "            ReLU-172            [-1, 512, 8, 8]               0\n",
      "          Conv2d-173            [-1, 128, 8, 8]          65,536\n",
      "     BatchNorm2d-174            [-1, 128, 8, 8]             256\n",
      "            ReLU-175            [-1, 128, 8, 8]               0\n",
      "          Conv2d-176             [-1, 32, 8, 8]          36,864\n",
      "     BatchNorm2d-177            [-1, 544, 8, 8]           1,088\n",
      "            ReLU-178            [-1, 544, 8, 8]               0\n",
      "          Conv2d-179            [-1, 128, 8, 8]          69,632\n",
      "     BatchNorm2d-180            [-1, 128, 8, 8]             256\n",
      "            ReLU-181            [-1, 128, 8, 8]               0\n",
      "          Conv2d-182             [-1, 32, 8, 8]          36,864\n",
      "     BatchNorm2d-183            [-1, 576, 8, 8]           1,152\n",
      "            ReLU-184            [-1, 576, 8, 8]               0\n",
      "          Conv2d-185            [-1, 128, 8, 8]          73,728\n",
      "     BatchNorm2d-186            [-1, 128, 8, 8]             256\n",
      "            ReLU-187            [-1, 128, 8, 8]               0\n",
      "          Conv2d-188             [-1, 32, 8, 8]          36,864\n",
      "     BatchNorm2d-189            [-1, 608, 8, 8]           1,216\n",
      "            ReLU-190            [-1, 608, 8, 8]               0\n",
      "          Conv2d-191            [-1, 128, 8, 8]          77,824\n",
      "     BatchNorm2d-192            [-1, 128, 8, 8]             256\n",
      "            ReLU-193            [-1, 128, 8, 8]               0\n",
      "          Conv2d-194             [-1, 32, 8, 8]          36,864\n",
      "     BatchNorm2d-195            [-1, 640, 8, 8]           1,280\n",
      "            ReLU-196            [-1, 640, 8, 8]               0\n",
      "          Conv2d-197            [-1, 128, 8, 8]          81,920\n",
      "     BatchNorm2d-198            [-1, 128, 8, 8]             256\n",
      "            ReLU-199            [-1, 128, 8, 8]               0\n",
      "          Conv2d-200             [-1, 32, 8, 8]          36,864\n",
      "     BatchNorm2d-201            [-1, 672, 8, 8]           1,344\n",
      "            ReLU-202            [-1, 672, 8, 8]               0\n",
      "          Conv2d-203            [-1, 128, 8, 8]          86,016\n",
      "     BatchNorm2d-204            [-1, 128, 8, 8]             256\n",
      "            ReLU-205            [-1, 128, 8, 8]               0\n",
      "          Conv2d-206             [-1, 32, 8, 8]          36,864\n",
      "     BatchNorm2d-207            [-1, 704, 8, 8]           1,408\n",
      "            ReLU-208            [-1, 704, 8, 8]               0\n",
      "          Conv2d-209            [-1, 128, 8, 8]          90,112\n",
      "     BatchNorm2d-210            [-1, 128, 8, 8]             256\n",
      "            ReLU-211            [-1, 128, 8, 8]               0\n",
      "          Conv2d-212             [-1, 32, 8, 8]          36,864\n",
      "     BatchNorm2d-213            [-1, 736, 8, 8]           1,472\n",
      "            ReLU-214            [-1, 736, 8, 8]               0\n",
      "          Conv2d-215            [-1, 128, 8, 8]          94,208\n",
      "     BatchNorm2d-216            [-1, 128, 8, 8]             256\n",
      "            ReLU-217            [-1, 128, 8, 8]               0\n",
      "          Conv2d-218             [-1, 32, 8, 8]          36,864\n",
      "     BatchNorm2d-219            [-1, 768, 8, 8]           1,536\n",
      "            ReLU-220            [-1, 768, 8, 8]               0\n",
      "          Conv2d-221            [-1, 128, 8, 8]          98,304\n",
      "     BatchNorm2d-222            [-1, 128, 8, 8]             256\n",
      "            ReLU-223            [-1, 128, 8, 8]               0\n",
      "          Conv2d-224             [-1, 32, 8, 8]          36,864\n",
      "     BatchNorm2d-225            [-1, 800, 8, 8]           1,600\n",
      "            ReLU-226            [-1, 800, 8, 8]               0\n",
      "          Conv2d-227            [-1, 128, 8, 8]         102,400\n",
      "     BatchNorm2d-228            [-1, 128, 8, 8]             256\n",
      "            ReLU-229            [-1, 128, 8, 8]               0\n",
      "          Conv2d-230             [-1, 32, 8, 8]          36,864\n",
      "     BatchNorm2d-231            [-1, 832, 8, 8]           1,664\n",
      "            ReLU-232            [-1, 832, 8, 8]               0\n",
      "          Conv2d-233            [-1, 128, 8, 8]         106,496\n",
      "     BatchNorm2d-234            [-1, 128, 8, 8]             256\n",
      "            ReLU-235            [-1, 128, 8, 8]               0\n",
      "          Conv2d-236             [-1, 32, 8, 8]          36,864\n",
      "     BatchNorm2d-237            [-1, 864, 8, 8]           1,728\n",
      "            ReLU-238            [-1, 864, 8, 8]               0\n",
      "          Conv2d-239            [-1, 128, 8, 8]         110,592\n",
      "     BatchNorm2d-240            [-1, 128, 8, 8]             256\n",
      "            ReLU-241            [-1, 128, 8, 8]               0\n",
      "          Conv2d-242             [-1, 32, 8, 8]          36,864\n",
      "     BatchNorm2d-243            [-1, 896, 8, 8]           1,792\n",
      "            ReLU-244            [-1, 896, 8, 8]               0\n",
      "          Conv2d-245            [-1, 128, 8, 8]         114,688\n",
      "     BatchNorm2d-246            [-1, 128, 8, 8]             256\n",
      "            ReLU-247            [-1, 128, 8, 8]               0\n",
      "          Conv2d-248             [-1, 32, 8, 8]          36,864\n",
      "     BatchNorm2d-249            [-1, 928, 8, 8]           1,856\n",
      "            ReLU-250            [-1, 928, 8, 8]               0\n",
      "          Conv2d-251            [-1, 128, 8, 8]         118,784\n",
      "     BatchNorm2d-252            [-1, 128, 8, 8]             256\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            ReLU-253            [-1, 128, 8, 8]               0\n",
      "          Conv2d-254             [-1, 32, 8, 8]          36,864\n",
      "     BatchNorm2d-255            [-1, 960, 8, 8]           1,920\n",
      "            ReLU-256            [-1, 960, 8, 8]               0\n",
      "          Conv2d-257            [-1, 128, 8, 8]         122,880\n",
      "     BatchNorm2d-258            [-1, 128, 8, 8]             256\n",
      "            ReLU-259            [-1, 128, 8, 8]               0\n",
      "          Conv2d-260             [-1, 32, 8, 8]          36,864\n",
      "     BatchNorm2d-261            [-1, 992, 8, 8]           1,984\n",
      "            ReLU-262            [-1, 992, 8, 8]               0\n",
      "          Conv2d-263            [-1, 128, 8, 8]         126,976\n",
      "     BatchNorm2d-264            [-1, 128, 8, 8]             256\n",
      "            ReLU-265            [-1, 128, 8, 8]               0\n",
      "          Conv2d-266             [-1, 32, 8, 8]          36,864\n",
      "     _DenseBlock-267           [-1, 1024, 8, 8]               0\n",
      "     BatchNorm2d-268           [-1, 1024, 8, 8]           2,048\n",
      "            ReLU-269           [-1, 1024, 8, 8]               0\n",
      "          Conv2d-270            [-1, 512, 8, 8]         524,288\n",
      "       AvgPool2d-271            [-1, 512, 4, 4]               0\n",
      "     BatchNorm2d-272            [-1, 512, 4, 4]           1,024\n",
      "            ReLU-273            [-1, 512, 4, 4]               0\n",
      "          Conv2d-274            [-1, 128, 4, 4]          65,536\n",
      "     BatchNorm2d-275            [-1, 128, 4, 4]             256\n",
      "            ReLU-276            [-1, 128, 4, 4]               0\n",
      "          Conv2d-277             [-1, 32, 4, 4]          36,864\n",
      "     BatchNorm2d-278            [-1, 544, 4, 4]           1,088\n",
      "            ReLU-279            [-1, 544, 4, 4]               0\n",
      "          Conv2d-280            [-1, 128, 4, 4]          69,632\n",
      "     BatchNorm2d-281            [-1, 128, 4, 4]             256\n",
      "            ReLU-282            [-1, 128, 4, 4]               0\n",
      "          Conv2d-283             [-1, 32, 4, 4]          36,864\n",
      "     BatchNorm2d-284            [-1, 576, 4, 4]           1,152\n",
      "            ReLU-285            [-1, 576, 4, 4]               0\n",
      "          Conv2d-286            [-1, 128, 4, 4]          73,728\n",
      "     BatchNorm2d-287            [-1, 128, 4, 4]             256\n",
      "            ReLU-288            [-1, 128, 4, 4]               0\n",
      "          Conv2d-289             [-1, 32, 4, 4]          36,864\n",
      "     BatchNorm2d-290            [-1, 608, 4, 4]           1,216\n",
      "            ReLU-291            [-1, 608, 4, 4]               0\n",
      "          Conv2d-292            [-1, 128, 4, 4]          77,824\n",
      "     BatchNorm2d-293            [-1, 128, 4, 4]             256\n",
      "            ReLU-294            [-1, 128, 4, 4]               0\n",
      "          Conv2d-295             [-1, 32, 4, 4]          36,864\n",
      "     BatchNorm2d-296            [-1, 640, 4, 4]           1,280\n",
      "            ReLU-297            [-1, 640, 4, 4]               0\n",
      "          Conv2d-298            [-1, 128, 4, 4]          81,920\n",
      "     BatchNorm2d-299            [-1, 128, 4, 4]             256\n",
      "            ReLU-300            [-1, 128, 4, 4]               0\n",
      "          Conv2d-301             [-1, 32, 4, 4]          36,864\n",
      "     BatchNorm2d-302            [-1, 672, 4, 4]           1,344\n",
      "            ReLU-303            [-1, 672, 4, 4]               0\n",
      "          Conv2d-304            [-1, 128, 4, 4]          86,016\n",
      "     BatchNorm2d-305            [-1, 128, 4, 4]             256\n",
      "            ReLU-306            [-1, 128, 4, 4]               0\n",
      "          Conv2d-307             [-1, 32, 4, 4]          36,864\n",
      "     BatchNorm2d-308            [-1, 704, 4, 4]           1,408\n",
      "            ReLU-309            [-1, 704, 4, 4]               0\n",
      "          Conv2d-310            [-1, 128, 4, 4]          90,112\n",
      "     BatchNorm2d-311            [-1, 128, 4, 4]             256\n",
      "            ReLU-312            [-1, 128, 4, 4]               0\n",
      "          Conv2d-313             [-1, 32, 4, 4]          36,864\n",
      "     BatchNorm2d-314            [-1, 736, 4, 4]           1,472\n",
      "            ReLU-315            [-1, 736, 4, 4]               0\n",
      "          Conv2d-316            [-1, 128, 4, 4]          94,208\n",
      "     BatchNorm2d-317            [-1, 128, 4, 4]             256\n",
      "            ReLU-318            [-1, 128, 4, 4]               0\n",
      "          Conv2d-319             [-1, 32, 4, 4]          36,864\n",
      "     BatchNorm2d-320            [-1, 768, 4, 4]           1,536\n",
      "            ReLU-321            [-1, 768, 4, 4]               0\n",
      "          Conv2d-322            [-1, 128, 4, 4]          98,304\n",
      "     BatchNorm2d-323            [-1, 128, 4, 4]             256\n",
      "            ReLU-324            [-1, 128, 4, 4]               0\n",
      "          Conv2d-325             [-1, 32, 4, 4]          36,864\n",
      "     BatchNorm2d-326            [-1, 800, 4, 4]           1,600\n",
      "            ReLU-327            [-1, 800, 4, 4]               0\n",
      "          Conv2d-328            [-1, 128, 4, 4]         102,400\n",
      "     BatchNorm2d-329            [-1, 128, 4, 4]             256\n",
      "            ReLU-330            [-1, 128, 4, 4]               0\n",
      "          Conv2d-331             [-1, 32, 4, 4]          36,864\n",
      "     BatchNorm2d-332            [-1, 832, 4, 4]           1,664\n",
      "            ReLU-333            [-1, 832, 4, 4]               0\n",
      "          Conv2d-334            [-1, 128, 4, 4]         106,496\n",
      "     BatchNorm2d-335            [-1, 128, 4, 4]             256\n",
      "            ReLU-336            [-1, 128, 4, 4]               0\n",
      "          Conv2d-337             [-1, 32, 4, 4]          36,864\n",
      "     BatchNorm2d-338            [-1, 864, 4, 4]           1,728\n",
      "            ReLU-339            [-1, 864, 4, 4]               0\n",
      "          Conv2d-340            [-1, 128, 4, 4]         110,592\n",
      "     BatchNorm2d-341            [-1, 128, 4, 4]             256\n",
      "            ReLU-342            [-1, 128, 4, 4]               0\n",
      "          Conv2d-343             [-1, 32, 4, 4]          36,864\n",
      "     BatchNorm2d-344            [-1, 896, 4, 4]           1,792\n",
      "            ReLU-345            [-1, 896, 4, 4]               0\n",
      "          Conv2d-346            [-1, 128, 4, 4]         114,688\n",
      "     BatchNorm2d-347            [-1, 128, 4, 4]             256\n",
      "            ReLU-348            [-1, 128, 4, 4]               0\n",
      "          Conv2d-349             [-1, 32, 4, 4]          36,864\n",
      "     BatchNorm2d-350            [-1, 928, 4, 4]           1,856\n",
      "            ReLU-351            [-1, 928, 4, 4]               0\n",
      "          Conv2d-352            [-1, 128, 4, 4]         118,784\n",
      "     BatchNorm2d-353            [-1, 128, 4, 4]             256\n",
      "            ReLU-354            [-1, 128, 4, 4]               0\n",
      "          Conv2d-355             [-1, 32, 4, 4]          36,864\n",
      "     BatchNorm2d-356            [-1, 960, 4, 4]           1,920\n",
      "            ReLU-357            [-1, 960, 4, 4]               0\n",
      "          Conv2d-358            [-1, 128, 4, 4]         122,880\n",
      "     BatchNorm2d-359            [-1, 128, 4, 4]             256\n",
      "            ReLU-360            [-1, 128, 4, 4]               0\n",
      "          Conv2d-361             [-1, 32, 4, 4]          36,864\n",
      "     BatchNorm2d-362            [-1, 992, 4, 4]           1,984\n",
      "            ReLU-363            [-1, 992, 4, 4]               0\n",
      "          Conv2d-364            [-1, 128, 4, 4]         126,976\n",
      "     BatchNorm2d-365            [-1, 128, 4, 4]             256\n",
      "            ReLU-366            [-1, 128, 4, 4]               0\n",
      "          Conv2d-367             [-1, 32, 4, 4]          36,864\n",
      "     _DenseBlock-368           [-1, 1024, 4, 4]               0\n",
      "     BatchNorm2d-369           [-1, 1024, 4, 4]           2,048\n",
      "          Linear-370                  [-1, 128]         131,200\n",
      "        DenseNet-371                  [-1, 128]               0\n",
      "SpectrogramEncoderNet-372                  [-1, 128]               0\n",
      "        Identity-373                  [-1, 128]               0\n",
      "          Conv2d-374           [-1, 64, 64, 64]           9,408\n",
      "     BatchNorm2d-375           [-1, 64, 64, 64]             128\n",
      "            ReLU-376           [-1, 64, 64, 64]               0\n",
      "       MaxPool2d-377           [-1, 64, 32, 32]               0\n",
      "     BatchNorm2d-378           [-1, 64, 32, 32]             128\n",
      "            ReLU-379           [-1, 64, 32, 32]               0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Conv2d-380          [-1, 128, 32, 32]           8,192\n",
      "     BatchNorm2d-381          [-1, 128, 32, 32]             256\n",
      "            ReLU-382          [-1, 128, 32, 32]               0\n",
      "          Conv2d-383           [-1, 32, 32, 32]          36,864\n",
      "     BatchNorm2d-384           [-1, 96, 32, 32]             192\n",
      "            ReLU-385           [-1, 96, 32, 32]               0\n",
      "          Conv2d-386          [-1, 128, 32, 32]          12,288\n",
      "     BatchNorm2d-387          [-1, 128, 32, 32]             256\n",
      "            ReLU-388          [-1, 128, 32, 32]               0\n",
      "          Conv2d-389           [-1, 32, 32, 32]          36,864\n",
      "     BatchNorm2d-390          [-1, 128, 32, 32]             256\n",
      "            ReLU-391          [-1, 128, 32, 32]               0\n",
      "          Conv2d-392          [-1, 128, 32, 32]          16,384\n",
      "     BatchNorm2d-393          [-1, 128, 32, 32]             256\n",
      "            ReLU-394          [-1, 128, 32, 32]               0\n",
      "          Conv2d-395           [-1, 32, 32, 32]          36,864\n",
      "     BatchNorm2d-396          [-1, 160, 32, 32]             320\n",
      "            ReLU-397          [-1, 160, 32, 32]               0\n",
      "          Conv2d-398          [-1, 128, 32, 32]          20,480\n",
      "     BatchNorm2d-399          [-1, 128, 32, 32]             256\n",
      "            ReLU-400          [-1, 128, 32, 32]               0\n",
      "          Conv2d-401           [-1, 32, 32, 32]          36,864\n",
      "     BatchNorm2d-402          [-1, 192, 32, 32]             384\n",
      "            ReLU-403          [-1, 192, 32, 32]               0\n",
      "          Conv2d-404          [-1, 128, 32, 32]          24,576\n",
      "     BatchNorm2d-405          [-1, 128, 32, 32]             256\n",
      "            ReLU-406          [-1, 128, 32, 32]               0\n",
      "          Conv2d-407           [-1, 32, 32, 32]          36,864\n",
      "     BatchNorm2d-408          [-1, 224, 32, 32]             448\n",
      "            ReLU-409          [-1, 224, 32, 32]               0\n",
      "          Conv2d-410          [-1, 128, 32, 32]          28,672\n",
      "     BatchNorm2d-411          [-1, 128, 32, 32]             256\n",
      "            ReLU-412          [-1, 128, 32, 32]               0\n",
      "          Conv2d-413           [-1, 32, 32, 32]          36,864\n",
      "     _DenseBlock-414          [-1, 256, 32, 32]               0\n",
      "     BatchNorm2d-415          [-1, 256, 32, 32]             512\n",
      "            ReLU-416          [-1, 256, 32, 32]               0\n",
      "          Conv2d-417          [-1, 128, 32, 32]          32,768\n",
      "       AvgPool2d-418          [-1, 128, 16, 16]               0\n",
      "     BatchNorm2d-419          [-1, 128, 16, 16]             256\n",
      "            ReLU-420          [-1, 128, 16, 16]               0\n",
      "          Conv2d-421          [-1, 128, 16, 16]          16,384\n",
      "     BatchNorm2d-422          [-1, 128, 16, 16]             256\n",
      "            ReLU-423          [-1, 128, 16, 16]               0\n",
      "          Conv2d-424           [-1, 32, 16, 16]          36,864\n",
      "     BatchNorm2d-425          [-1, 160, 16, 16]             320\n",
      "            ReLU-426          [-1, 160, 16, 16]               0\n",
      "          Conv2d-427          [-1, 128, 16, 16]          20,480\n",
      "     BatchNorm2d-428          [-1, 128, 16, 16]             256\n",
      "            ReLU-429          [-1, 128, 16, 16]               0\n",
      "          Conv2d-430           [-1, 32, 16, 16]          36,864\n",
      "     BatchNorm2d-431          [-1, 192, 16, 16]             384\n",
      "            ReLU-432          [-1, 192, 16, 16]               0\n",
      "          Conv2d-433          [-1, 128, 16, 16]          24,576\n",
      "     BatchNorm2d-434          [-1, 128, 16, 16]             256\n",
      "            ReLU-435          [-1, 128, 16, 16]               0\n",
      "          Conv2d-436           [-1, 32, 16, 16]          36,864\n",
      "     BatchNorm2d-437          [-1, 224, 16, 16]             448\n",
      "            ReLU-438          [-1, 224, 16, 16]               0\n",
      "          Conv2d-439          [-1, 128, 16, 16]          28,672\n",
      "     BatchNorm2d-440          [-1, 128, 16, 16]             256\n",
      "            ReLU-441          [-1, 128, 16, 16]               0\n",
      "          Conv2d-442           [-1, 32, 16, 16]          36,864\n",
      "     BatchNorm2d-443          [-1, 256, 16, 16]             512\n",
      "            ReLU-444          [-1, 256, 16, 16]               0\n",
      "          Conv2d-445          [-1, 128, 16, 16]          32,768\n",
      "     BatchNorm2d-446          [-1, 128, 16, 16]             256\n",
      "            ReLU-447          [-1, 128, 16, 16]               0\n",
      "          Conv2d-448           [-1, 32, 16, 16]          36,864\n",
      "     BatchNorm2d-449          [-1, 288, 16, 16]             576\n",
      "            ReLU-450          [-1, 288, 16, 16]               0\n",
      "          Conv2d-451          [-1, 128, 16, 16]          36,864\n",
      "     BatchNorm2d-452          [-1, 128, 16, 16]             256\n",
      "            ReLU-453          [-1, 128, 16, 16]               0\n",
      "          Conv2d-454           [-1, 32, 16, 16]          36,864\n",
      "     BatchNorm2d-455          [-1, 320, 16, 16]             640\n",
      "            ReLU-456          [-1, 320, 16, 16]               0\n",
      "          Conv2d-457          [-1, 128, 16, 16]          40,960\n",
      "     BatchNorm2d-458          [-1, 128, 16, 16]             256\n",
      "            ReLU-459          [-1, 128, 16, 16]               0\n",
      "          Conv2d-460           [-1, 32, 16, 16]          36,864\n",
      "     BatchNorm2d-461          [-1, 352, 16, 16]             704\n",
      "            ReLU-462          [-1, 352, 16, 16]               0\n",
      "          Conv2d-463          [-1, 128, 16, 16]          45,056\n",
      "     BatchNorm2d-464          [-1, 128, 16, 16]             256\n",
      "            ReLU-465          [-1, 128, 16, 16]               0\n",
      "          Conv2d-466           [-1, 32, 16, 16]          36,864\n",
      "     BatchNorm2d-467          [-1, 384, 16, 16]             768\n",
      "            ReLU-468          [-1, 384, 16, 16]               0\n",
      "          Conv2d-469          [-1, 128, 16, 16]          49,152\n",
      "     BatchNorm2d-470          [-1, 128, 16, 16]             256\n",
      "            ReLU-471          [-1, 128, 16, 16]               0\n",
      "          Conv2d-472           [-1, 32, 16, 16]          36,864\n",
      "     BatchNorm2d-473          [-1, 416, 16, 16]             832\n",
      "            ReLU-474          [-1, 416, 16, 16]               0\n",
      "          Conv2d-475          [-1, 128, 16, 16]          53,248\n",
      "     BatchNorm2d-476          [-1, 128, 16, 16]             256\n",
      "            ReLU-477          [-1, 128, 16, 16]               0\n",
      "          Conv2d-478           [-1, 32, 16, 16]          36,864\n",
      "     BatchNorm2d-479          [-1, 448, 16, 16]             896\n",
      "            ReLU-480          [-1, 448, 16, 16]               0\n",
      "          Conv2d-481          [-1, 128, 16, 16]          57,344\n",
      "     BatchNorm2d-482          [-1, 128, 16, 16]             256\n",
      "            ReLU-483          [-1, 128, 16, 16]               0\n",
      "          Conv2d-484           [-1, 32, 16, 16]          36,864\n",
      "     BatchNorm2d-485          [-1, 480, 16, 16]             960\n",
      "            ReLU-486          [-1, 480, 16, 16]               0\n",
      "          Conv2d-487          [-1, 128, 16, 16]          61,440\n",
      "     BatchNorm2d-488          [-1, 128, 16, 16]             256\n",
      "            ReLU-489          [-1, 128, 16, 16]               0\n",
      "          Conv2d-490           [-1, 32, 16, 16]          36,864\n",
      "     _DenseBlock-491          [-1, 512, 16, 16]               0\n",
      "     BatchNorm2d-492          [-1, 512, 16, 16]           1,024\n",
      "            ReLU-493          [-1, 512, 16, 16]               0\n",
      "          Conv2d-494          [-1, 256, 16, 16]         131,072\n",
      "       AvgPool2d-495            [-1, 256, 8, 8]               0\n",
      "     BatchNorm2d-496            [-1, 256, 8, 8]             512\n",
      "            ReLU-497            [-1, 256, 8, 8]               0\n",
      "          Conv2d-498            [-1, 128, 8, 8]          32,768\n",
      "     BatchNorm2d-499            [-1, 128, 8, 8]             256\n",
      "            ReLU-500            [-1, 128, 8, 8]               0\n",
      "          Conv2d-501             [-1, 32, 8, 8]          36,864\n",
      "     BatchNorm2d-502            [-1, 288, 8, 8]             576\n",
      "            ReLU-503            [-1, 288, 8, 8]               0\n",
      "          Conv2d-504            [-1, 128, 8, 8]          36,864\n",
      "     BatchNorm2d-505            [-1, 128, 8, 8]             256\n",
      "            ReLU-506            [-1, 128, 8, 8]               0\n",
      "          Conv2d-507             [-1, 32, 8, 8]          36,864\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     BatchNorm2d-508            [-1, 320, 8, 8]             640\n",
      "            ReLU-509            [-1, 320, 8, 8]               0\n",
      "          Conv2d-510            [-1, 128, 8, 8]          40,960\n",
      "     BatchNorm2d-511            [-1, 128, 8, 8]             256\n",
      "            ReLU-512            [-1, 128, 8, 8]               0\n",
      "          Conv2d-513             [-1, 32, 8, 8]          36,864\n",
      "     BatchNorm2d-514            [-1, 352, 8, 8]             704\n",
      "            ReLU-515            [-1, 352, 8, 8]               0\n",
      "          Conv2d-516            [-1, 128, 8, 8]          45,056\n",
      "     BatchNorm2d-517            [-1, 128, 8, 8]             256\n",
      "            ReLU-518            [-1, 128, 8, 8]               0\n",
      "          Conv2d-519             [-1, 32, 8, 8]          36,864\n",
      "     BatchNorm2d-520            [-1, 384, 8, 8]             768\n",
      "            ReLU-521            [-1, 384, 8, 8]               0\n",
      "          Conv2d-522            [-1, 128, 8, 8]          49,152\n",
      "     BatchNorm2d-523            [-1, 128, 8, 8]             256\n",
      "            ReLU-524            [-1, 128, 8, 8]               0\n",
      "          Conv2d-525             [-1, 32, 8, 8]          36,864\n",
      "     BatchNorm2d-526            [-1, 416, 8, 8]             832\n",
      "            ReLU-527            [-1, 416, 8, 8]               0\n",
      "          Conv2d-528            [-1, 128, 8, 8]          53,248\n",
      "     BatchNorm2d-529            [-1, 128, 8, 8]             256\n",
      "            ReLU-530            [-1, 128, 8, 8]               0\n",
      "          Conv2d-531             [-1, 32, 8, 8]          36,864\n",
      "     BatchNorm2d-532            [-1, 448, 8, 8]             896\n",
      "            ReLU-533            [-1, 448, 8, 8]               0\n",
      "          Conv2d-534            [-1, 128, 8, 8]          57,344\n",
      "     BatchNorm2d-535            [-1, 128, 8, 8]             256\n",
      "            ReLU-536            [-1, 128, 8, 8]               0\n",
      "          Conv2d-537             [-1, 32, 8, 8]          36,864\n",
      "     BatchNorm2d-538            [-1, 480, 8, 8]             960\n",
      "            ReLU-539            [-1, 480, 8, 8]               0\n",
      "          Conv2d-540            [-1, 128, 8, 8]          61,440\n",
      "     BatchNorm2d-541            [-1, 128, 8, 8]             256\n",
      "            ReLU-542            [-1, 128, 8, 8]               0\n",
      "          Conv2d-543             [-1, 32, 8, 8]          36,864\n",
      "     BatchNorm2d-544            [-1, 512, 8, 8]           1,024\n",
      "            ReLU-545            [-1, 512, 8, 8]               0\n",
      "          Conv2d-546            [-1, 128, 8, 8]          65,536\n",
      "     BatchNorm2d-547            [-1, 128, 8, 8]             256\n",
      "            ReLU-548            [-1, 128, 8, 8]               0\n",
      "          Conv2d-549             [-1, 32, 8, 8]          36,864\n",
      "     BatchNorm2d-550            [-1, 544, 8, 8]           1,088\n",
      "            ReLU-551            [-1, 544, 8, 8]               0\n",
      "          Conv2d-552            [-1, 128, 8, 8]          69,632\n",
      "     BatchNorm2d-553            [-1, 128, 8, 8]             256\n",
      "            ReLU-554            [-1, 128, 8, 8]               0\n",
      "          Conv2d-555             [-1, 32, 8, 8]          36,864\n",
      "     BatchNorm2d-556            [-1, 576, 8, 8]           1,152\n",
      "            ReLU-557            [-1, 576, 8, 8]               0\n",
      "          Conv2d-558            [-1, 128, 8, 8]          73,728\n",
      "     BatchNorm2d-559            [-1, 128, 8, 8]             256\n",
      "            ReLU-560            [-1, 128, 8, 8]               0\n",
      "          Conv2d-561             [-1, 32, 8, 8]          36,864\n",
      "     BatchNorm2d-562            [-1, 608, 8, 8]           1,216\n",
      "            ReLU-563            [-1, 608, 8, 8]               0\n",
      "          Conv2d-564            [-1, 128, 8, 8]          77,824\n",
      "     BatchNorm2d-565            [-1, 128, 8, 8]             256\n",
      "            ReLU-566            [-1, 128, 8, 8]               0\n",
      "          Conv2d-567             [-1, 32, 8, 8]          36,864\n",
      "     BatchNorm2d-568            [-1, 640, 8, 8]           1,280\n",
      "            ReLU-569            [-1, 640, 8, 8]               0\n",
      "          Conv2d-570            [-1, 128, 8, 8]          81,920\n",
      "     BatchNorm2d-571            [-1, 128, 8, 8]             256\n",
      "            ReLU-572            [-1, 128, 8, 8]               0\n",
      "          Conv2d-573             [-1, 32, 8, 8]          36,864\n",
      "     BatchNorm2d-574            [-1, 672, 8, 8]           1,344\n",
      "            ReLU-575            [-1, 672, 8, 8]               0\n",
      "          Conv2d-576            [-1, 128, 8, 8]          86,016\n",
      "     BatchNorm2d-577            [-1, 128, 8, 8]             256\n",
      "            ReLU-578            [-1, 128, 8, 8]               0\n",
      "          Conv2d-579             [-1, 32, 8, 8]          36,864\n",
      "     BatchNorm2d-580            [-1, 704, 8, 8]           1,408\n",
      "            ReLU-581            [-1, 704, 8, 8]               0\n",
      "          Conv2d-582            [-1, 128, 8, 8]          90,112\n",
      "     BatchNorm2d-583            [-1, 128, 8, 8]             256\n",
      "            ReLU-584            [-1, 128, 8, 8]               0\n",
      "          Conv2d-585             [-1, 32, 8, 8]          36,864\n",
      "     BatchNorm2d-586            [-1, 736, 8, 8]           1,472\n",
      "            ReLU-587            [-1, 736, 8, 8]               0\n",
      "          Conv2d-588            [-1, 128, 8, 8]          94,208\n",
      "     BatchNorm2d-589            [-1, 128, 8, 8]             256\n",
      "            ReLU-590            [-1, 128, 8, 8]               0\n",
      "          Conv2d-591             [-1, 32, 8, 8]          36,864\n",
      "     BatchNorm2d-592            [-1, 768, 8, 8]           1,536\n",
      "            ReLU-593            [-1, 768, 8, 8]               0\n",
      "          Conv2d-594            [-1, 128, 8, 8]          98,304\n",
      "     BatchNorm2d-595            [-1, 128, 8, 8]             256\n",
      "            ReLU-596            [-1, 128, 8, 8]               0\n",
      "          Conv2d-597             [-1, 32, 8, 8]          36,864\n",
      "     BatchNorm2d-598            [-1, 800, 8, 8]           1,600\n",
      "            ReLU-599            [-1, 800, 8, 8]               0\n",
      "          Conv2d-600            [-1, 128, 8, 8]         102,400\n",
      "     BatchNorm2d-601            [-1, 128, 8, 8]             256\n",
      "            ReLU-602            [-1, 128, 8, 8]               0\n",
      "          Conv2d-603             [-1, 32, 8, 8]          36,864\n",
      "     BatchNorm2d-604            [-1, 832, 8, 8]           1,664\n",
      "            ReLU-605            [-1, 832, 8, 8]               0\n",
      "          Conv2d-606            [-1, 128, 8, 8]         106,496\n",
      "     BatchNorm2d-607            [-1, 128, 8, 8]             256\n",
      "            ReLU-608            [-1, 128, 8, 8]               0\n",
      "          Conv2d-609             [-1, 32, 8, 8]          36,864\n",
      "     BatchNorm2d-610            [-1, 864, 8, 8]           1,728\n",
      "            ReLU-611            [-1, 864, 8, 8]               0\n",
      "          Conv2d-612            [-1, 128, 8, 8]         110,592\n",
      "     BatchNorm2d-613            [-1, 128, 8, 8]             256\n",
      "            ReLU-614            [-1, 128, 8, 8]               0\n",
      "          Conv2d-615             [-1, 32, 8, 8]          36,864\n",
      "     BatchNorm2d-616            [-1, 896, 8, 8]           1,792\n",
      "            ReLU-617            [-1, 896, 8, 8]               0\n",
      "          Conv2d-618            [-1, 128, 8, 8]         114,688\n",
      "     BatchNorm2d-619            [-1, 128, 8, 8]             256\n",
      "            ReLU-620            [-1, 128, 8, 8]               0\n",
      "          Conv2d-621             [-1, 32, 8, 8]          36,864\n",
      "     BatchNorm2d-622            [-1, 928, 8, 8]           1,856\n",
      "            ReLU-623            [-1, 928, 8, 8]               0\n",
      "          Conv2d-624            [-1, 128, 8, 8]         118,784\n",
      "     BatchNorm2d-625            [-1, 128, 8, 8]             256\n",
      "            ReLU-626            [-1, 128, 8, 8]               0\n",
      "          Conv2d-627             [-1, 32, 8, 8]          36,864\n",
      "     BatchNorm2d-628            [-1, 960, 8, 8]           1,920\n",
      "            ReLU-629            [-1, 960, 8, 8]               0\n",
      "          Conv2d-630            [-1, 128, 8, 8]         122,880\n",
      "     BatchNorm2d-631            [-1, 128, 8, 8]             256\n",
      "            ReLU-632            [-1, 128, 8, 8]               0\n",
      "          Conv2d-633             [-1, 32, 8, 8]          36,864\n",
      "     BatchNorm2d-634            [-1, 992, 8, 8]           1,984\n",
      "            ReLU-635            [-1, 992, 8, 8]               0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Conv2d-636            [-1, 128, 8, 8]         126,976\n",
      "     BatchNorm2d-637            [-1, 128, 8, 8]             256\n",
      "            ReLU-638            [-1, 128, 8, 8]               0\n",
      "          Conv2d-639             [-1, 32, 8, 8]          36,864\n",
      "     _DenseBlock-640           [-1, 1024, 8, 8]               0\n",
      "     BatchNorm2d-641           [-1, 1024, 8, 8]           2,048\n",
      "            ReLU-642           [-1, 1024, 8, 8]               0\n",
      "          Conv2d-643            [-1, 512, 8, 8]         524,288\n",
      "       AvgPool2d-644            [-1, 512, 4, 4]               0\n",
      "     BatchNorm2d-645            [-1, 512, 4, 4]           1,024\n",
      "            ReLU-646            [-1, 512, 4, 4]               0\n",
      "          Conv2d-647            [-1, 128, 4, 4]          65,536\n",
      "     BatchNorm2d-648            [-1, 128, 4, 4]             256\n",
      "            ReLU-649            [-1, 128, 4, 4]               0\n",
      "          Conv2d-650             [-1, 32, 4, 4]          36,864\n",
      "     BatchNorm2d-651            [-1, 544, 4, 4]           1,088\n",
      "            ReLU-652            [-1, 544, 4, 4]               0\n",
      "          Conv2d-653            [-1, 128, 4, 4]          69,632\n",
      "     BatchNorm2d-654            [-1, 128, 4, 4]             256\n",
      "            ReLU-655            [-1, 128, 4, 4]               0\n",
      "          Conv2d-656             [-1, 32, 4, 4]          36,864\n",
      "     BatchNorm2d-657            [-1, 576, 4, 4]           1,152\n",
      "            ReLU-658            [-1, 576, 4, 4]               0\n",
      "          Conv2d-659            [-1, 128, 4, 4]          73,728\n",
      "     BatchNorm2d-660            [-1, 128, 4, 4]             256\n",
      "            ReLU-661            [-1, 128, 4, 4]               0\n",
      "          Conv2d-662             [-1, 32, 4, 4]          36,864\n",
      "     BatchNorm2d-663            [-1, 608, 4, 4]           1,216\n",
      "            ReLU-664            [-1, 608, 4, 4]               0\n",
      "          Conv2d-665            [-1, 128, 4, 4]          77,824\n",
      "     BatchNorm2d-666            [-1, 128, 4, 4]             256\n",
      "            ReLU-667            [-1, 128, 4, 4]               0\n",
      "          Conv2d-668             [-1, 32, 4, 4]          36,864\n",
      "     BatchNorm2d-669            [-1, 640, 4, 4]           1,280\n",
      "            ReLU-670            [-1, 640, 4, 4]               0\n",
      "          Conv2d-671            [-1, 128, 4, 4]          81,920\n",
      "     BatchNorm2d-672            [-1, 128, 4, 4]             256\n",
      "            ReLU-673            [-1, 128, 4, 4]               0\n",
      "          Conv2d-674             [-1, 32, 4, 4]          36,864\n",
      "     BatchNorm2d-675            [-1, 672, 4, 4]           1,344\n",
      "            ReLU-676            [-1, 672, 4, 4]               0\n",
      "          Conv2d-677            [-1, 128, 4, 4]          86,016\n",
      "     BatchNorm2d-678            [-1, 128, 4, 4]             256\n",
      "            ReLU-679            [-1, 128, 4, 4]               0\n",
      "          Conv2d-680             [-1, 32, 4, 4]          36,864\n",
      "     BatchNorm2d-681            [-1, 704, 4, 4]           1,408\n",
      "            ReLU-682            [-1, 704, 4, 4]               0\n",
      "          Conv2d-683            [-1, 128, 4, 4]          90,112\n",
      "     BatchNorm2d-684            [-1, 128, 4, 4]             256\n",
      "            ReLU-685            [-1, 128, 4, 4]               0\n",
      "          Conv2d-686             [-1, 32, 4, 4]          36,864\n",
      "     BatchNorm2d-687            [-1, 736, 4, 4]           1,472\n",
      "            ReLU-688            [-1, 736, 4, 4]               0\n",
      "          Conv2d-689            [-1, 128, 4, 4]          94,208\n",
      "     BatchNorm2d-690            [-1, 128, 4, 4]             256\n",
      "            ReLU-691            [-1, 128, 4, 4]               0\n",
      "          Conv2d-692             [-1, 32, 4, 4]          36,864\n",
      "     BatchNorm2d-693            [-1, 768, 4, 4]           1,536\n",
      "            ReLU-694            [-1, 768, 4, 4]               0\n",
      "          Conv2d-695            [-1, 128, 4, 4]          98,304\n",
      "     BatchNorm2d-696            [-1, 128, 4, 4]             256\n",
      "            ReLU-697            [-1, 128, 4, 4]               0\n",
      "          Conv2d-698             [-1, 32, 4, 4]          36,864\n",
      "     BatchNorm2d-699            [-1, 800, 4, 4]           1,600\n",
      "            ReLU-700            [-1, 800, 4, 4]               0\n",
      "          Conv2d-701            [-1, 128, 4, 4]         102,400\n",
      "     BatchNorm2d-702            [-1, 128, 4, 4]             256\n",
      "            ReLU-703            [-1, 128, 4, 4]               0\n",
      "          Conv2d-704             [-1, 32, 4, 4]          36,864\n",
      "     BatchNorm2d-705            [-1, 832, 4, 4]           1,664\n",
      "            ReLU-706            [-1, 832, 4, 4]               0\n",
      "          Conv2d-707            [-1, 128, 4, 4]         106,496\n",
      "     BatchNorm2d-708            [-1, 128, 4, 4]             256\n",
      "            ReLU-709            [-1, 128, 4, 4]               0\n",
      "          Conv2d-710             [-1, 32, 4, 4]          36,864\n",
      "     BatchNorm2d-711            [-1, 864, 4, 4]           1,728\n",
      "            ReLU-712            [-1, 864, 4, 4]               0\n",
      "          Conv2d-713            [-1, 128, 4, 4]         110,592\n",
      "     BatchNorm2d-714            [-1, 128, 4, 4]             256\n",
      "            ReLU-715            [-1, 128, 4, 4]               0\n",
      "          Conv2d-716             [-1, 32, 4, 4]          36,864\n",
      "     BatchNorm2d-717            [-1, 896, 4, 4]           1,792\n",
      "            ReLU-718            [-1, 896, 4, 4]               0\n",
      "          Conv2d-719            [-1, 128, 4, 4]         114,688\n",
      "     BatchNorm2d-720            [-1, 128, 4, 4]             256\n",
      "            ReLU-721            [-1, 128, 4, 4]               0\n",
      "          Conv2d-722             [-1, 32, 4, 4]          36,864\n",
      "     BatchNorm2d-723            [-1, 928, 4, 4]           1,856\n",
      "            ReLU-724            [-1, 928, 4, 4]               0\n",
      "          Conv2d-725            [-1, 128, 4, 4]         118,784\n",
      "     BatchNorm2d-726            [-1, 128, 4, 4]             256\n",
      "            ReLU-727            [-1, 128, 4, 4]               0\n",
      "          Conv2d-728             [-1, 32, 4, 4]          36,864\n",
      "     BatchNorm2d-729            [-1, 960, 4, 4]           1,920\n",
      "            ReLU-730            [-1, 960, 4, 4]               0\n",
      "          Conv2d-731            [-1, 128, 4, 4]         122,880\n",
      "     BatchNorm2d-732            [-1, 128, 4, 4]             256\n",
      "            ReLU-733            [-1, 128, 4, 4]               0\n",
      "          Conv2d-734             [-1, 32, 4, 4]          36,864\n",
      "     BatchNorm2d-735            [-1, 992, 4, 4]           1,984\n",
      "            ReLU-736            [-1, 992, 4, 4]               0\n",
      "          Conv2d-737            [-1, 128, 4, 4]         126,976\n",
      "     BatchNorm2d-738            [-1, 128, 4, 4]             256\n",
      "            ReLU-739            [-1, 128, 4, 4]               0\n",
      "          Conv2d-740             [-1, 32, 4, 4]          36,864\n",
      "     _DenseBlock-741           [-1, 1024, 4, 4]               0\n",
      "     BatchNorm2d-742           [-1, 1024, 4, 4]           2,048\n",
      "          Linear-743                  [-1, 128]         131,200\n",
      "        DenseNet-744                  [-1, 128]               0\n",
      "SpectrogramEncoderNet-745                  [-1, 128]               0\n",
      "        Identity-746                  [-1, 128]               0\n",
      "================================================================\n",
      "Total params: 14,170,112\n",
      "Trainable params: 14,170,112\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 1.12\n",
      "Forward/backward pass size (MB): 199.38\n",
      "Params size (MB): 54.05\n",
      "Estimated Total Size (MB): 254.56\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(MultiSiameseContrastiveClassifierNet(), input_size=(CANDIDATE_SIZE+1, 3, IMG_HEIGHT, IMG_HEIGHT), device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Training data\n",
    "training_folder = os.path.join(output_data_folder, \"training_dataset_full_spectrogram/vox1_dev_wav\")\n",
    "spectrogram_samples_files = [os.path.join(training_folder, file) for file in os.listdir(training_folder)]\n",
    "candidate_size = CANDIDATE_SIZE\n",
    "# batch_size = 15   # mobilenet_v2\n",
    "batch_size = 6   # densenet121\n",
    "num_batches = 2000 // batch_size\n",
    "num_sub_samples = 200\n",
    "training_data_generator = ContrastiveDataGenerator(spectrogram_samples_files, candidate_size, batch_size, num_batches, num_sub_samples, IMG_HEIGHT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Validation data\n",
    "validation_set_file = os.path.join(output_data_folder, \"validation_sets\", \"contrastive_validation_set.pickle\")\n",
    "with open(validation_set_file, 'rb') as f:\n",
    "    validation_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intra_class_variance_reduction(contrastive_model, contrastive_sub_samples, log_handler):   \n",
    "    num_classes = 2\n",
    "    samples_per_class = 10\n",
    "    \n",
    "    # prep for training\n",
    "    criterion = nn.MSELoss(reduction='mean')\n",
    "#     criterion = nn.L1Loss(reduction='mean')\n",
    "    \n",
    "    # train\n",
    "    total_loss = 0.0\n",
    "    spectrogram_sub_samples = random.sample(contrastive_sub_samples, num_classes)  # sample classes\n",
    "    for spectrogram in spectrogram_sub_samples:   # treat one spectrogram/user as one class\n",
    "        input_imgs = [BaseDataGenerator.get_sliding_img_slice_from_spectrogram(spectrogram) for _ in range(samples_per_class)]\n",
    "        input_imgs = torch.tensor(input_imgs)\n",
    "        inputs = input_imgs.to(device)  \n",
    "        \n",
    "        encoded_outputs = contrastive_model.encoder(inputs)\n",
    "        mean = torch.mean(encoded_outputs, dim=0)   # get mean encoding vector of this class\n",
    "        mean = mean.repeat(samples_per_class, 1)   # mean vector\n",
    "        loss = criterion(encoded_outputs, mean)   # MSE against mean (variance)\n",
    "        total_loss += loss\n",
    "    return total_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, num_epochs, training_data_generator, validation_data, log_handler):\n",
    "    since = time.time()\n",
    "    best_acc = 0.0\n",
    "    \n",
    "    # intra class variance reduction \n",
    "#     run_variance_reduction_on_epoch = [*range(1, num_epochs)]\n",
    "    run_variance_reduction_on_epoch = [*range(0, num_epochs)]   # continue training\n",
    "#     run_variance_reduction_on_epoch = [*range(1, num_epochs, 2)]   # alternate\n",
    "    variance_reduction_frequency = 2   # every n batches\n",
    "    loss_variance_scale = 0.20\n",
    "\n",
    "    t1 = time.time()\n",
    "    for epoch in range(num_epochs):\n",
    "        log_handler.print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        log_handler.print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            \n",
    "            # intra class variance reduction\n",
    "            run_variance_reduction = phase == 'train' and epoch in run_variance_reduction_on_epoch\n",
    "            if run_variance_reduction: log_handler.print(\"-- variance reduction\")\n",
    "            \n",
    "            # Main training (contrastive training)\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "            \n",
    "            # Iterate over data.\n",
    "            batches_used = 0\n",
    "            data_generator = training_data_generator.generate_batches() if phase == 'train' else validation_data\n",
    "            for data in data_generator:\n",
    "                batches_used += 1                \n",
    "                \n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):   # gradient only for train\n",
    "                    \n",
    "                    # intra class variance reduction\n",
    "                    loss_var = None\n",
    "                    if batches_used % variance_reduction_frequency == 0 and run_variance_reduction:\n",
    "                        loss_var = intra_class_variance_reduction(model, training_data_generator.sub_samples, log_handler)\n",
    "                        loss_var *= loss_variance_scale\n",
    "                        loss_var.backward()\n",
    "                    \n",
    "                    # Main training (contrastive training)\n",
    "                    input_imgs, labels = data\n",
    "                    inputs = [img.to(device) for img in input_imgs]\n",
    "                    labels = labels.to(device)\n",
    "                    \n",
    "                    outputs = model(inputs)      \n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "    \n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                if loss_var is not None: loss += loss_var   # Overall loss with variance reduction\n",
    "                running_loss += loss.item() * inputs[0].size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "                \n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "            \n",
    "            epoch_loss = running_loss / (batches_used * inputs[0].size(0))\n",
    "            epoch_acc = running_corrects.double() / (batches_used * inputs[0].size(0))\n",
    "            log_handler.print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc and epoch > 0:\n",
    "                best_acc = epoch_acc\n",
    "                log_handler.save_pytorch_model(model, \"best_model_{}.pt\".format(model.__class__.__name__))\n",
    "                example = [torch.rand(1, 3, IMG_HEIGHT, IMG_HEIGHT), torch.rand(1, 3, IMG_HEIGHT, IMG_HEIGHT)]\n",
    "                log_handler.save_pytorch_model_as_torchscript(model, \"mobile_model.pt\", (example,))\n",
    "\n",
    "        # end of epoch\n",
    "        log_handler.print(\"Time taken is {} seconds\".format(int(time.time()-t1)))\n",
    "        t1 = time.time()\n",
    "        log_handler.print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    log_handler.print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    log_handler.print('Best val Acc: {:4f}'.format(best_acc))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model from contrastive training\n",
    "def load_contrastive_encoder_model():\n",
    "    encoder_model_folder = os.path.join(models_folder, \"contrastive_encoder\", \"good_models\", \"2020-04-08_02-34-53\")\n",
    "    module_file = os.path.join(encoder_model_folder, \"model_definitions.py\")\n",
    "    module_name = \"MultiSiameseContrastiveClassifierNet\"\n",
    "    module = load_module_from_file(module_file, module_name)\n",
    "    # load model\n",
    "    model = module.MultiSiameseContrastiveClassifierNet()\n",
    "    state_dict_file = os.path.join(encoder_model_folder, \"best_model_MultiSiameseContrastiveClassifierNet.pt\")\n",
    "    model.load_state_dict(torch.load(state_dict_file, map_location=\"cpu\"))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Desktop\\projects\\speaker_recognition_voxceleb1\\output_data\\models\\contrastive_encoder\\2020-04-08_16-43-10\n",
      "Description: Candidates: 5, Encoding: 128, Projection: None\n",
      "Base Model: mobileNetV2\n",
      "With Intra Class Variance Reduction\n",
      "Continued from 2020-04-08_02-34-53\n",
      "Epoch 0/69\n",
      "----------\n",
      "-- variance reduction\n",
      "train Loss: 1.4895 Acc: 0.3789\n",
      "val Loss: 1.3208 Acc: 0.5479\n",
      "Time taken is 645 seconds\n",
      "\n",
      "Epoch 1/69\n",
      "----------\n",
      "-- variance reduction\n",
      "train Loss: 1.3866 Acc: 0.4575\n",
      "val Loss: 1.2705 Acc: 0.5799\n",
      "MODEL SAVED\n",
      "MODEL SAVED (MOBILE)\n",
      "Time taken is 648 seconds\n",
      "\n",
      "Epoch 2/69\n",
      "----------\n",
      "-- variance reduction\n",
      "train Loss: 1.3563 Acc: 0.4590\n",
      "val Loss: 1.2262 Acc: 0.5805\n",
      "MODEL SAVED\n",
      "MODEL SAVED (MOBILE)\n",
      "Time taken is 645 seconds\n",
      "\n",
      "Epoch 3/69\n",
      "----------\n",
      "-- variance reduction\n",
      "train Loss: 1.3494 Acc: 0.4595\n",
      "val Loss: 1.2486 Acc: 0.5664\n",
      "Time taken is 642 seconds\n",
      "\n",
      "Epoch 4/69\n",
      "----------\n",
      "-- variance reduction\n",
      "train Loss: 1.3237 Acc: 0.4805\n",
      "val Loss: 1.2274 Acc: 0.5855\n",
      "MODEL SAVED\n",
      "MODEL SAVED (MOBILE)\n",
      "Time taken is 678 seconds\n",
      "\n",
      "Epoch 5/69\n",
      "----------\n",
      "-- variance reduction\n",
      "train Loss: 1.2908 Acc: 0.4965\n",
      "val Loss: 1.2128 Acc: 0.5970\n",
      "MODEL SAVED\n",
      "MODEL SAVED (MOBILE)\n",
      "Time taken is 679 seconds\n",
      "\n",
      "Epoch 6/69\n",
      "----------\n",
      "-- variance reduction\n",
      "train Loss: 1.2736 Acc: 0.4975\n",
      "val Loss: 1.1975 Acc: 0.5890\n",
      "Time taken is 672 seconds\n",
      "\n",
      "Epoch 7/69\n",
      "----------\n",
      "-- variance reduction\n",
      "train Loss: 1.2877 Acc: 0.5045\n",
      "val Loss: 1.2028 Acc: 0.6085\n",
      "MODEL SAVED\n",
      "MODEL SAVED (MOBILE)\n",
      "Time taken is 666 seconds\n",
      "\n",
      "Epoch 8/69\n",
      "----------\n",
      "-- variance reduction\n",
      "train Loss: 1.2672 Acc: 0.4940\n",
      "val Loss: 1.2144 Acc: 0.5910\n",
      "Time taken is 662 seconds\n",
      "\n",
      "Epoch 9/69\n",
      "----------\n",
      "-- variance reduction\n",
      "train Loss: 1.2585 Acc: 0.5105\n",
      "val Loss: 1.1859 Acc: 0.5900\n",
      "Time taken is 661 seconds\n",
      "\n",
      "Epoch 10/69\n",
      "----------\n",
      "-- variance reduction\n",
      "train Loss: 1.2466 Acc: 0.5175\n",
      "val Loss: 1.2249 Acc: 0.5744\n",
      "Time taken is 661 seconds\n",
      "\n",
      "Epoch 11/69\n",
      "----------\n",
      "-- variance reduction\n",
      "train Loss: 1.2825 Acc: 0.5020\n",
      "val Loss: 1.1935 Acc: 0.6180\n",
      "MODEL SAVED\n",
      "MODEL SAVED (MOBILE)\n",
      "Time taken is 674 seconds\n",
      "\n",
      "Epoch 12/69\n",
      "----------\n",
      "-- variance reduction\n",
      "train Loss: 1.2144 Acc: 0.5380\n",
      "val Loss: 1.1879 Acc: 0.6025\n",
      "Time taken is 673 seconds\n",
      "\n",
      "Epoch 13/69\n",
      "----------\n",
      "-- variance reduction\n",
      "train Loss: 1.2280 Acc: 0.5250\n",
      "val Loss: 1.1752 Acc: 0.6155\n",
      "Time taken is 668 seconds\n",
      "\n",
      "Epoch 14/69\n",
      "----------\n",
      "-- variance reduction\n",
      "train Loss: 1.2083 Acc: 0.5315\n",
      "val Loss: 1.2082 Acc: 0.5950\n",
      "Time taken is 673 seconds\n",
      "\n",
      "Epoch 15/69\n",
      "----------\n",
      "-- variance reduction\n",
      "train Loss: 1.1939 Acc: 0.5571\n",
      "val Loss: 1.1713 Acc: 0.6125\n",
      "Time taken is 676 seconds\n",
      "\n",
      "Epoch 16/69\n",
      "----------\n",
      "-- variance reduction\n",
      "train Loss: 1.2002 Acc: 0.5596\n",
      "val Loss: 1.1666 Acc: 0.6376\n",
      "MODEL SAVED\n",
      "MODEL SAVED (MOBILE)\n",
      "Time taken is 679 seconds\n",
      "\n",
      "Epoch 17/69\n",
      "----------\n",
      "-- variance reduction\n",
      "train Loss: 1.1895 Acc: 0.5656\n",
      "val Loss: 1.1634 Acc: 0.6441\n",
      "MODEL SAVED\n",
      "MODEL SAVED (MOBILE)\n",
      "Time taken is 688 seconds\n",
      "\n",
      "Epoch 18/69\n",
      "----------\n",
      "-- variance reduction\n",
      "train Loss: 1.1859 Acc: 0.5696\n",
      "val Loss: 1.1746 Acc: 0.6271\n",
      "Time taken is 677 seconds\n",
      "\n",
      "Epoch 19/69\n",
      "----------\n",
      "-- variance reduction\n",
      "train Loss: 1.1954 Acc: 0.5485\n",
      "val Loss: 1.1765 Acc: 0.6195\n",
      "Time taken is 666 seconds\n",
      "\n",
      "Epoch 20/69\n",
      "----------\n",
      "-- variance reduction\n",
      "train Loss: 1.1785 Acc: 0.5546\n",
      "val Loss: 1.1566 Acc: 0.6351\n",
      "Time taken is 664 seconds\n",
      "\n",
      "Epoch 21/69\n",
      "----------\n",
      "-- variance reduction\n",
      "train Loss: 1.1896 Acc: 0.5651\n",
      "val Loss: 1.1555 Acc: 0.6486\n",
      "MODEL SAVED\n",
      "MODEL SAVED (MOBILE)\n",
      "Time taken is 672 seconds\n",
      "\n",
      "Epoch 22/69\n",
      "----------\n",
      "-- variance reduction\n",
      "train Loss: 1.1723 Acc: 0.5876\n",
      "val Loss: 1.1610 Acc: 0.6617\n",
      "MODEL SAVED\n",
      "MODEL SAVED (MOBILE)\n",
      "Time taken is 686 seconds\n",
      "\n",
      "Epoch 23/69\n",
      "----------\n",
      "-- variance reduction\n",
      "train Loss: 1.1574 Acc: 0.6106\n",
      "val Loss: 1.1481 Acc: 0.6541\n",
      "Time taken is 675 seconds\n",
      "\n",
      "Epoch 24/69\n",
      "----------\n",
      "-- variance reduction\n",
      "train Loss: 1.1731 Acc: 0.5881\n",
      "val Loss: 1.1464 Acc: 0.6506\n",
      "Time taken is 657 seconds\n",
      "\n",
      "Epoch 25/69\n",
      "----------\n",
      "-- variance reduction\n",
      "train Loss: 1.1556 Acc: 0.5791\n",
      "val Loss: 1.1438 Acc: 0.6411\n",
      "Time taken is 643 seconds\n",
      "\n",
      "Epoch 26/69\n",
      "----------\n",
      "-- variance reduction\n",
      "train Loss: 1.1677 Acc: 0.5816\n",
      "val Loss: 1.1480 Acc: 0.6602\n",
      "Time taken is 640 seconds\n",
      "\n",
      "Epoch 27/69\n",
      "----------\n",
      "-- variance reduction\n",
      "train Loss: 1.1741 Acc: 0.5881\n",
      "val Loss: 1.1610 Acc: 0.6481\n",
      "Time taken is 664 seconds\n",
      "\n",
      "Epoch 28/69\n",
      "----------\n",
      "-- variance reduction\n",
      "train Loss: 1.1722 Acc: 0.6051\n",
      "val Loss: 1.1337 Acc: 0.6612\n",
      "Time taken is 675 seconds\n",
      "\n",
      "Epoch 29/69\n",
      "----------\n",
      "-- variance reduction\n",
      "train Loss: 1.1812 Acc: 0.5716\n",
      "val Loss: 1.1519 Acc: 0.6491\n",
      "Time taken is 669 seconds\n",
      "\n",
      "Epoch 30/69\n",
      "----------\n",
      "-- variance reduction\n",
      "train Loss: 1.1545 Acc: 0.6196\n",
      "val Loss: 1.1358 Acc: 0.6607\n",
      "Time taken is 661 seconds\n",
      "\n",
      "Epoch 31/69\n",
      "----------\n",
      "-- variance reduction\n",
      "train Loss: 1.1648 Acc: 0.5986\n",
      "val Loss: 1.1359 Acc: 0.6436\n",
      "Time taken is 669 seconds\n",
      "\n",
      "Epoch 32/69\n",
      "----------\n",
      "-- variance reduction\n",
      "train Loss: 1.1611 Acc: 0.6076\n",
      "val Loss: 1.1538 Acc: 0.6526\n",
      "Time taken is 669 seconds\n",
      "\n",
      "Epoch 33/69\n",
      "----------\n",
      "-- variance reduction\n",
      "train Loss: 1.1502 Acc: 0.6091\n",
      "val Loss: 1.1397 Acc: 0.6591\n",
      "Time taken is 673 seconds\n",
      "\n",
      "Epoch 34/69\n",
      "----------\n",
      "-- variance reduction\n",
      "train Loss: 1.1503 Acc: 0.6051\n",
      "val Loss: 1.1396 Acc: 0.6702\n",
      "MODEL SAVED\n",
      "MODEL SAVED (MOBILE)\n",
      "Time taken is 673 seconds\n",
      "\n",
      "Epoch 35/69\n",
      "----------\n",
      "-- variance reduction\n",
      "train Loss: 1.1772 Acc: 0.5786\n",
      "val Loss: 1.1444 Acc: 0.6461\n",
      "Time taken is 666 seconds\n",
      "\n",
      "Epoch 36/69\n",
      "----------\n",
      "-- variance reduction\n",
      "train Loss: 1.1540 Acc: 0.6196\n",
      "val Loss: 1.1307 Acc: 0.6762\n",
      "MODEL SAVED\n",
      "MODEL SAVED (MOBILE)\n",
      "Time taken is 683 seconds\n",
      "\n",
      "Epoch 37/69\n",
      "----------\n",
      "-- variance reduction\n",
      "train Loss: 1.1641 Acc: 0.6101\n",
      "val Loss: 1.1357 Acc: 0.6561\n",
      "Time taken is 676 seconds\n",
      "\n",
      "Epoch 38/69\n",
      "----------\n",
      "-- variance reduction\n",
      "train Loss: 1.1494 Acc: 0.6061\n",
      "val Loss: 1.1390 Acc: 0.6591\n",
      "Time taken is 641 seconds\n",
      "\n",
      "Epoch 39/69\n",
      "----------\n",
      "-- variance reduction\n",
      "train Loss: 1.1485 Acc: 0.6006\n",
      "val Loss: 1.1429 Acc: 0.6461\n",
      "Time taken is 620 seconds\n",
      "\n",
      "Epoch 40/69\n",
      "----------\n",
      "-- variance reduction\n",
      "train Loss: 1.1562 Acc: 0.6236\n",
      "val Loss: 1.1348 Acc: 0.6737\n",
      "Time taken is 617 seconds\n",
      "\n",
      "Epoch 41/69\n",
      "----------\n",
      "-- variance reduction\n",
      "train Loss: 1.1537 Acc: 0.6131\n",
      "val Loss: 1.1432 Acc: 0.6441\n",
      "Time taken is 630 seconds\n",
      "\n",
      "Epoch 42/69\n",
      "----------\n",
      "-- variance reduction\n",
      "train Loss: 1.1505 Acc: 0.6121\n",
      "val Loss: 1.1475 Acc: 0.6496\n",
      "Time taken is 629 seconds\n",
      "\n",
      "Epoch 43/69\n",
      "----------\n",
      "-- variance reduction\n",
      "train Loss: 1.1721 Acc: 0.6066\n",
      "val Loss: 1.1346 Acc: 0.6506\n",
      "Time taken is 625 seconds\n",
      "\n",
      "Epoch 44/69\n",
      "----------\n",
      "-- variance reduction\n",
      "train Loss: 1.1545 Acc: 0.6236\n",
      "val Loss: 1.1279 Acc: 0.6767\n",
      "MODEL SAVED\n",
      "MODEL SAVED (MOBILE)\n",
      "Time taken is 630 seconds\n",
      "\n",
      "Epoch 45/69\n",
      "----------\n",
      "-- variance reduction\n",
      "train Loss: 1.1627 Acc: 0.6071\n",
      "val Loss: 1.1313 Acc: 0.6632\n",
      "Time taken is 620 seconds\n",
      "\n",
      "Epoch 46/69\n",
      "----------\n",
      "-- variance reduction\n",
      "train Loss: 1.1564 Acc: 0.6136\n",
      "val Loss: 1.1257 Acc: 0.6857\n",
      "MODEL SAVED\n",
      "MODEL SAVED (MOBILE)\n",
      "Time taken is 638 seconds\n",
      "\n",
      "Epoch 47/69\n",
      "----------\n",
      "-- variance reduction\n",
      "train Loss: 1.1342 Acc: 0.6266\n",
      "val Loss: 1.1370 Acc: 0.6757\n",
      "Time taken is 757 seconds\n",
      "\n",
      "Epoch 48/69\n",
      "----------\n",
      "-- variance reduction\n",
      "train Loss: 1.1458 Acc: 0.6281\n",
      "val Loss: 1.1348 Acc: 0.6647\n",
      "Time taken is 632 seconds\n",
      "\n",
      "Epoch 49/69\n",
      "----------\n",
      "-- variance reduction\n",
      "train Loss: 1.1550 Acc: 0.6341\n",
      "val Loss: 1.1405 Acc: 0.6752\n",
      "Time taken is 633 seconds\n",
      "\n",
      "Epoch 50/69\n",
      "----------\n",
      "-- variance reduction\n",
      "train Loss: 1.1515 Acc: 0.6256\n",
      "val Loss: 1.1352 Acc: 0.6652\n",
      "Time taken is 1000 seconds\n",
      "\n",
      "Epoch 51/69\n",
      "----------\n",
      "-- variance reduction\n",
      "train Loss: 1.1510 Acc: 0.6101\n",
      "val Loss: 1.1364 Acc: 0.6827\n",
      "Time taken is 871 seconds\n",
      "\n",
      "Epoch 52/69\n",
      "----------\n",
      "-- variance reduction\n",
      "train Loss: 1.1531 Acc: 0.6306\n",
      "val Loss: 1.1299 Acc: 0.6912\n",
      "MODEL SAVED\n",
      "MODEL SAVED (MOBILE)\n",
      "Time taken is 696 seconds\n",
      "\n",
      "Epoch 53/69\n",
      "----------\n",
      "-- variance reduction\n",
      "train Loss: 1.1437 Acc: 0.6191\n",
      "val Loss: 1.1564 Acc: 0.6667\n",
      "Time taken is 644 seconds\n",
      "\n",
      "Epoch 54/69\n",
      "----------\n",
      "-- variance reduction\n",
      "train Loss: 1.1583 Acc: 0.6281\n",
      "val Loss: 1.1319 Acc: 0.6797\n",
      "Time taken is 623 seconds\n",
      "\n",
      "Epoch 55/69\n",
      "----------\n",
      "-- variance reduction\n",
      "train Loss: 1.1574 Acc: 0.6091\n",
      "val Loss: 1.1346 Acc: 0.6732\n",
      "Time taken is 634 seconds\n",
      "\n",
      "Epoch 56/69\n",
      "----------\n",
      "-- variance reduction\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 1.1537 Acc: 0.6216\n",
      "val Loss: 1.1287 Acc: 0.6737\n",
      "Time taken is 645 seconds\n",
      "\n",
      "Epoch 57/69\n",
      "----------\n",
      "-- variance reduction\n",
      "train Loss: 1.1427 Acc: 0.6416\n",
      "val Loss: 1.1451 Acc: 0.6717\n",
      "Time taken is 632 seconds\n",
      "\n",
      "Epoch 58/69\n",
      "----------\n",
      "-- variance reduction\n",
      "train Loss: 1.1574 Acc: 0.6191\n",
      "val Loss: 1.1265 Acc: 0.6927\n",
      "MODEL SAVED\n",
      "MODEL SAVED (MOBILE)\n",
      "Time taken is 650 seconds\n",
      "\n",
      "Epoch 59/69\n",
      "----------\n",
      "-- variance reduction\n",
      "train Loss: 1.1462 Acc: 0.6351\n",
      "val Loss: 1.1298 Acc: 0.6722\n",
      "Time taken is 636 seconds\n",
      "\n",
      "Epoch 60/69\n",
      "----------\n",
      "-- variance reduction\n",
      "train Loss: 1.1422 Acc: 0.6341\n",
      "val Loss: 1.1330 Acc: 0.6667\n",
      "Time taken is 642 seconds\n",
      "\n",
      "Epoch 61/69\n",
      "----------\n",
      "-- variance reduction\n",
      "train Loss: 1.1576 Acc: 0.6146\n",
      "val Loss: 1.1310 Acc: 0.6807\n",
      "Time taken is 634 seconds\n",
      "\n",
      "Epoch 62/69\n",
      "----------\n",
      "-- variance reduction\n",
      "train Loss: 1.1399 Acc: 0.6517\n",
      "val Loss: 1.1301 Acc: 0.6877\n",
      "Time taken is 631 seconds\n",
      "\n",
      "Epoch 63/69\n",
      "----------\n",
      "-- variance reduction\n",
      "train Loss: 1.1626 Acc: 0.6246\n",
      "val Loss: 1.1516 Acc: 0.6767\n",
      "Time taken is 634 seconds\n",
      "\n",
      "Epoch 64/69\n",
      "----------\n",
      "-- variance reduction\n",
      "train Loss: 1.1557 Acc: 0.6226\n",
      "val Loss: 1.1249 Acc: 0.6882\n",
      "Time taken is 634 seconds\n",
      "\n",
      "Epoch 65/69\n",
      "----------\n",
      "-- variance reduction\n",
      "train Loss: 1.1501 Acc: 0.6236\n",
      "val Loss: 1.1403 Acc: 0.6887\n",
      "Time taken is 627 seconds\n",
      "\n",
      "Epoch 66/69\n",
      "----------\n",
      "-- variance reduction\n",
      "train Loss: 1.1535 Acc: 0.6286\n",
      "val Loss: 1.1407 Acc: 0.6777\n",
      "Time taken is 632 seconds\n",
      "\n",
      "Epoch 67/69\n",
      "----------\n",
      "-- variance reduction\n",
      "train Loss: 1.1536 Acc: 0.6346\n",
      "val Loss: 1.1586 Acc: 0.6817\n",
      "Time taken is 629 seconds\n",
      "\n",
      "Epoch 68/69\n",
      "----------\n",
      "-- variance reduction\n",
      "train Loss: 1.1465 Acc: 0.6306\n",
      "val Loss: 1.1307 Acc: 0.6762\n",
      "Time taken is 635 seconds\n",
      "\n",
      "Epoch 69/69\n",
      "----------\n",
      "-- variance reduction\n",
      "train Loss: 1.1529 Acc: 0.6361\n",
      "val Loss: 1.1332 Acc: 0.6762\n",
      "Time taken is 634 seconds\n",
      "\n",
      "Training complete in 773m 36s\n",
      "Best val Acc: 0.692732\n"
     ]
    }
   ],
   "source": [
    "### Train\n",
    "\n",
    "epochs = 70\n",
    "# epochs = 50\n",
    "# epochs = 30\n",
    "\n",
    "model_ft = MultiSiameseContrastiveClassifierNet().to(device)\n",
    "# model_ft = load_contrastive_encoder_model().to(device)   # continue training\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer_ft = optim.Adam(model_ft.parameters(), lr = 0.0001)\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "# learning_rate_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=10, gamma=0.1)\n",
    "learning_rate_scheduler = lr_scheduler.CyclicLR(optimizer_ft, base_lr=0.0001, max_lr=0.001, cycle_momentum=False)   # 0.001 seems better\n",
    "\n",
    "\n",
    "### Train \n",
    "\n",
    "# Logger\n",
    "model_save_folder = os.path.join(models_folder, \"contrastive_encoder\")\n",
    "log_handler = ModelSaveAndLogHandler(model_save_folder, enable_model_saving=True, enable_logging=True)   # init\n",
    "model_def_src_file_path = os.path.join(r\"D:\\Desktop\\projects\\speaker_recognition_voxceleb1\\scripts\", \"model_definitions.py\")\n",
    "log_handler.save_model_definition_file(model_def_src_file_path)   # copy model def file\n",
    "print(log_handler.folder)\n",
    "\n",
    "# Description\n",
    "log_handler.print(\"Description: Candidates: 5, Encoding: 128, Projection: None\")\n",
    "# log_handler.print(\"Base Model: mobileNetV2\")\n",
    "log_handler.print(\"Base Model: densenet121\")\n",
    "log_handler.print(\"With Intra Class Variance Reduction\")\n",
    "# log_handler.print(\"Continued from 2020-04-08_02-34-53\")\n",
    "\n",
    "# Train\n",
    "# train_model(model_ft, criterion, optimizer_ft, learning_rate_scheduler, epochs, num_batches, training_data_generator, log_handler)\n",
    "train_model(model_ft, criterion, optimizer_ft, learning_rate_scheduler, epochs, training_data_generator, validation_data, log_handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_acc = 1 / CANDIDATE_SIZE\n",
    "random_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: Overall\n",
    "* Contrastive classifier\n",
    "    * separate train and validate methods\n",
    "\n",
    "* (Done) Model saving / checkpointing\n",
    "* **Build binary classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
