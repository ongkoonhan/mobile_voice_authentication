{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import copy\n",
    "import random\n",
    "import pickle\n",
    "import numpy as np\n",
    "from skimage.color import gray2rgb\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torchvision import models\n",
    "from torchsummary import summary\n",
    "\n",
    "from config import models_folder, output_data_folder\n",
    "from config import n_mels\n",
    "\n",
    "from model_definitions import VerificationBinaryClassifierNet\n",
    "from data_generators import VerificationDataGenerator\n",
    "from project_utils import ModelSaveAndLogHandler, load_module_from_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_HEIGHT = n_mels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_model_folder = os.path.join(models_folder, \"contrastive_encoder\", \"good_models\", \"2020-04-06_19-42-12\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model from contrastive training\n",
    "def load_pretrained_encoder_model():\n",
    "    module_file = os.path.join(encoder_model_folder, \"model_definitions.py\")\n",
    "    module_name = \"MultiSiameseContrastiveClassifierNet\"\n",
    "    module = load_module_from_file(module_file, module_name)\n",
    "    # load model\n",
    "    model = module.MultiSiameseContrastiveClassifierNet()\n",
    "    state_dict_file = os.path.join(encoder_model_folder, \"best_model_MultiSiameseContrastiveClassifierNet.pt\")\n",
    "    model.load_state_dict(torch.load(state_dict_file, map_location=\"cpu\"))\n",
    "    return model.encoder   # return pretrained encoder only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 64, 64]           9,408\n",
      "       BatchNorm2d-2           [-1, 64, 64, 64]             128\n",
      "              ReLU-3           [-1, 64, 64, 64]               0\n",
      "         MaxPool2d-4           [-1, 64, 32, 32]               0\n",
      "       BatchNorm2d-5           [-1, 64, 32, 32]             128\n",
      "              ReLU-6           [-1, 64, 32, 32]               0\n",
      "            Conv2d-7          [-1, 128, 32, 32]           8,192\n",
      "       BatchNorm2d-8          [-1, 128, 32, 32]             256\n",
      "              ReLU-9          [-1, 128, 32, 32]               0\n",
      "           Conv2d-10           [-1, 32, 32, 32]          36,864\n",
      "      BatchNorm2d-11           [-1, 96, 32, 32]             192\n",
      "             ReLU-12           [-1, 96, 32, 32]               0\n",
      "           Conv2d-13          [-1, 128, 32, 32]          12,288\n",
      "      BatchNorm2d-14          [-1, 128, 32, 32]             256\n",
      "             ReLU-15          [-1, 128, 32, 32]               0\n",
      "           Conv2d-16           [-1, 32, 32, 32]          36,864\n",
      "      BatchNorm2d-17          [-1, 128, 32, 32]             256\n",
      "             ReLU-18          [-1, 128, 32, 32]               0\n",
      "           Conv2d-19          [-1, 128, 32, 32]          16,384\n",
      "      BatchNorm2d-20          [-1, 128, 32, 32]             256\n",
      "             ReLU-21          [-1, 128, 32, 32]               0\n",
      "           Conv2d-22           [-1, 32, 32, 32]          36,864\n",
      "      BatchNorm2d-23          [-1, 160, 32, 32]             320\n",
      "             ReLU-24          [-1, 160, 32, 32]               0\n",
      "           Conv2d-25          [-1, 128, 32, 32]          20,480\n",
      "      BatchNorm2d-26          [-1, 128, 32, 32]             256\n",
      "             ReLU-27          [-1, 128, 32, 32]               0\n",
      "           Conv2d-28           [-1, 32, 32, 32]          36,864\n",
      "      BatchNorm2d-29          [-1, 192, 32, 32]             384\n",
      "             ReLU-30          [-1, 192, 32, 32]               0\n",
      "           Conv2d-31          [-1, 128, 32, 32]          24,576\n",
      "      BatchNorm2d-32          [-1, 128, 32, 32]             256\n",
      "             ReLU-33          [-1, 128, 32, 32]               0\n",
      "           Conv2d-34           [-1, 32, 32, 32]          36,864\n",
      "      BatchNorm2d-35          [-1, 224, 32, 32]             448\n",
      "             ReLU-36          [-1, 224, 32, 32]               0\n",
      "           Conv2d-37          [-1, 128, 32, 32]          28,672\n",
      "      BatchNorm2d-38          [-1, 128, 32, 32]             256\n",
      "             ReLU-39          [-1, 128, 32, 32]               0\n",
      "           Conv2d-40           [-1, 32, 32, 32]          36,864\n",
      "      _DenseBlock-41          [-1, 256, 32, 32]               0\n",
      "      BatchNorm2d-42          [-1, 256, 32, 32]             512\n",
      "             ReLU-43          [-1, 256, 32, 32]               0\n",
      "           Conv2d-44          [-1, 128, 32, 32]          32,768\n",
      "        AvgPool2d-45          [-1, 128, 16, 16]               0\n",
      "      BatchNorm2d-46          [-1, 128, 16, 16]             256\n",
      "             ReLU-47          [-1, 128, 16, 16]               0\n",
      "           Conv2d-48          [-1, 128, 16, 16]          16,384\n",
      "      BatchNorm2d-49          [-1, 128, 16, 16]             256\n",
      "             ReLU-50          [-1, 128, 16, 16]               0\n",
      "           Conv2d-51           [-1, 32, 16, 16]          36,864\n",
      "      BatchNorm2d-52          [-1, 160, 16, 16]             320\n",
      "             ReLU-53          [-1, 160, 16, 16]               0\n",
      "           Conv2d-54          [-1, 128, 16, 16]          20,480\n",
      "      BatchNorm2d-55          [-1, 128, 16, 16]             256\n",
      "             ReLU-56          [-1, 128, 16, 16]               0\n",
      "           Conv2d-57           [-1, 32, 16, 16]          36,864\n",
      "      BatchNorm2d-58          [-1, 192, 16, 16]             384\n",
      "             ReLU-59          [-1, 192, 16, 16]               0\n",
      "           Conv2d-60          [-1, 128, 16, 16]          24,576\n",
      "      BatchNorm2d-61          [-1, 128, 16, 16]             256\n",
      "             ReLU-62          [-1, 128, 16, 16]               0\n",
      "           Conv2d-63           [-1, 32, 16, 16]          36,864\n",
      "      BatchNorm2d-64          [-1, 224, 16, 16]             448\n",
      "             ReLU-65          [-1, 224, 16, 16]               0\n",
      "           Conv2d-66          [-1, 128, 16, 16]          28,672\n",
      "      BatchNorm2d-67          [-1, 128, 16, 16]             256\n",
      "             ReLU-68          [-1, 128, 16, 16]               0\n",
      "           Conv2d-69           [-1, 32, 16, 16]          36,864\n",
      "      BatchNorm2d-70          [-1, 256, 16, 16]             512\n",
      "             ReLU-71          [-1, 256, 16, 16]               0\n",
      "           Conv2d-72          [-1, 128, 16, 16]          32,768\n",
      "      BatchNorm2d-73          [-1, 128, 16, 16]             256\n",
      "             ReLU-74          [-1, 128, 16, 16]               0\n",
      "           Conv2d-75           [-1, 32, 16, 16]          36,864\n",
      "      BatchNorm2d-76          [-1, 288, 16, 16]             576\n",
      "             ReLU-77          [-1, 288, 16, 16]               0\n",
      "           Conv2d-78          [-1, 128, 16, 16]          36,864\n",
      "      BatchNorm2d-79          [-1, 128, 16, 16]             256\n",
      "             ReLU-80          [-1, 128, 16, 16]               0\n",
      "           Conv2d-81           [-1, 32, 16, 16]          36,864\n",
      "      BatchNorm2d-82          [-1, 320, 16, 16]             640\n",
      "             ReLU-83          [-1, 320, 16, 16]               0\n",
      "           Conv2d-84          [-1, 128, 16, 16]          40,960\n",
      "      BatchNorm2d-85          [-1, 128, 16, 16]             256\n",
      "             ReLU-86          [-1, 128, 16, 16]               0\n",
      "           Conv2d-87           [-1, 32, 16, 16]          36,864\n",
      "      BatchNorm2d-88          [-1, 352, 16, 16]             704\n",
      "             ReLU-89          [-1, 352, 16, 16]               0\n",
      "           Conv2d-90          [-1, 128, 16, 16]          45,056\n",
      "      BatchNorm2d-91          [-1, 128, 16, 16]             256\n",
      "             ReLU-92          [-1, 128, 16, 16]               0\n",
      "           Conv2d-93           [-1, 32, 16, 16]          36,864\n",
      "      BatchNorm2d-94          [-1, 384, 16, 16]             768\n",
      "             ReLU-95          [-1, 384, 16, 16]               0\n",
      "           Conv2d-96          [-1, 128, 16, 16]          49,152\n",
      "      BatchNorm2d-97          [-1, 128, 16, 16]             256\n",
      "             ReLU-98          [-1, 128, 16, 16]               0\n",
      "           Conv2d-99           [-1, 32, 16, 16]          36,864\n",
      "     BatchNorm2d-100          [-1, 416, 16, 16]             832\n",
      "            ReLU-101          [-1, 416, 16, 16]               0\n",
      "          Conv2d-102          [-1, 128, 16, 16]          53,248\n",
      "     BatchNorm2d-103          [-1, 128, 16, 16]             256\n",
      "            ReLU-104          [-1, 128, 16, 16]               0\n",
      "          Conv2d-105           [-1, 32, 16, 16]          36,864\n",
      "     BatchNorm2d-106          [-1, 448, 16, 16]             896\n",
      "            ReLU-107          [-1, 448, 16, 16]               0\n",
      "          Conv2d-108          [-1, 128, 16, 16]          57,344\n",
      "     BatchNorm2d-109          [-1, 128, 16, 16]             256\n",
      "            ReLU-110          [-1, 128, 16, 16]               0\n",
      "          Conv2d-111           [-1, 32, 16, 16]          36,864\n",
      "     BatchNorm2d-112          [-1, 480, 16, 16]             960\n",
      "            ReLU-113          [-1, 480, 16, 16]               0\n",
      "          Conv2d-114          [-1, 128, 16, 16]          61,440\n",
      "     BatchNorm2d-115          [-1, 128, 16, 16]             256\n",
      "            ReLU-116          [-1, 128, 16, 16]               0\n",
      "          Conv2d-117           [-1, 32, 16, 16]          36,864\n",
      "     _DenseBlock-118          [-1, 512, 16, 16]               0\n",
      "     BatchNorm2d-119          [-1, 512, 16, 16]           1,024\n",
      "            ReLU-120          [-1, 512, 16, 16]               0\n",
      "          Conv2d-121          [-1, 256, 16, 16]         131,072\n",
      "       AvgPool2d-122            [-1, 256, 8, 8]               0\n",
      "     BatchNorm2d-123            [-1, 256, 8, 8]             512\n",
      "            ReLU-124            [-1, 256, 8, 8]               0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Conv2d-125            [-1, 128, 8, 8]          32,768\n",
      "     BatchNorm2d-126            [-1, 128, 8, 8]             256\n",
      "            ReLU-127            [-1, 128, 8, 8]               0\n",
      "          Conv2d-128             [-1, 32, 8, 8]          36,864\n",
      "     BatchNorm2d-129            [-1, 288, 8, 8]             576\n",
      "            ReLU-130            [-1, 288, 8, 8]               0\n",
      "          Conv2d-131            [-1, 128, 8, 8]          36,864\n",
      "     BatchNorm2d-132            [-1, 128, 8, 8]             256\n",
      "            ReLU-133            [-1, 128, 8, 8]               0\n",
      "          Conv2d-134             [-1, 32, 8, 8]          36,864\n",
      "     BatchNorm2d-135            [-1, 320, 8, 8]             640\n",
      "            ReLU-136            [-1, 320, 8, 8]               0\n",
      "          Conv2d-137            [-1, 128, 8, 8]          40,960\n",
      "     BatchNorm2d-138            [-1, 128, 8, 8]             256\n",
      "            ReLU-139            [-1, 128, 8, 8]               0\n",
      "          Conv2d-140             [-1, 32, 8, 8]          36,864\n",
      "     BatchNorm2d-141            [-1, 352, 8, 8]             704\n",
      "            ReLU-142            [-1, 352, 8, 8]               0\n",
      "          Conv2d-143            [-1, 128, 8, 8]          45,056\n",
      "     BatchNorm2d-144            [-1, 128, 8, 8]             256\n",
      "            ReLU-145            [-1, 128, 8, 8]               0\n",
      "          Conv2d-146             [-1, 32, 8, 8]          36,864\n",
      "     BatchNorm2d-147            [-1, 384, 8, 8]             768\n",
      "            ReLU-148            [-1, 384, 8, 8]               0\n",
      "          Conv2d-149            [-1, 128, 8, 8]          49,152\n",
      "     BatchNorm2d-150            [-1, 128, 8, 8]             256\n",
      "            ReLU-151            [-1, 128, 8, 8]               0\n",
      "          Conv2d-152             [-1, 32, 8, 8]          36,864\n",
      "     BatchNorm2d-153            [-1, 416, 8, 8]             832\n",
      "            ReLU-154            [-1, 416, 8, 8]               0\n",
      "          Conv2d-155            [-1, 128, 8, 8]          53,248\n",
      "     BatchNorm2d-156            [-1, 128, 8, 8]             256\n",
      "            ReLU-157            [-1, 128, 8, 8]               0\n",
      "          Conv2d-158             [-1, 32, 8, 8]          36,864\n",
      "     BatchNorm2d-159            [-1, 448, 8, 8]             896\n",
      "            ReLU-160            [-1, 448, 8, 8]               0\n",
      "          Conv2d-161            [-1, 128, 8, 8]          57,344\n",
      "     BatchNorm2d-162            [-1, 128, 8, 8]             256\n",
      "            ReLU-163            [-1, 128, 8, 8]               0\n",
      "          Conv2d-164             [-1, 32, 8, 8]          36,864\n",
      "     BatchNorm2d-165            [-1, 480, 8, 8]             960\n",
      "            ReLU-166            [-1, 480, 8, 8]               0\n",
      "          Conv2d-167            [-1, 128, 8, 8]          61,440\n",
      "     BatchNorm2d-168            [-1, 128, 8, 8]             256\n",
      "            ReLU-169            [-1, 128, 8, 8]               0\n",
      "          Conv2d-170             [-1, 32, 8, 8]          36,864\n",
      "     BatchNorm2d-171            [-1, 512, 8, 8]           1,024\n",
      "            ReLU-172            [-1, 512, 8, 8]               0\n",
      "          Conv2d-173            [-1, 128, 8, 8]          65,536\n",
      "     BatchNorm2d-174            [-1, 128, 8, 8]             256\n",
      "            ReLU-175            [-1, 128, 8, 8]               0\n",
      "          Conv2d-176             [-1, 32, 8, 8]          36,864\n",
      "     BatchNorm2d-177            [-1, 544, 8, 8]           1,088\n",
      "            ReLU-178            [-1, 544, 8, 8]               0\n",
      "          Conv2d-179            [-1, 128, 8, 8]          69,632\n",
      "     BatchNorm2d-180            [-1, 128, 8, 8]             256\n",
      "            ReLU-181            [-1, 128, 8, 8]               0\n",
      "          Conv2d-182             [-1, 32, 8, 8]          36,864\n",
      "     BatchNorm2d-183            [-1, 576, 8, 8]           1,152\n",
      "            ReLU-184            [-1, 576, 8, 8]               0\n",
      "          Conv2d-185            [-1, 128, 8, 8]          73,728\n",
      "     BatchNorm2d-186            [-1, 128, 8, 8]             256\n",
      "            ReLU-187            [-1, 128, 8, 8]               0\n",
      "          Conv2d-188             [-1, 32, 8, 8]          36,864\n",
      "     BatchNorm2d-189            [-1, 608, 8, 8]           1,216\n",
      "            ReLU-190            [-1, 608, 8, 8]               0\n",
      "          Conv2d-191            [-1, 128, 8, 8]          77,824\n",
      "     BatchNorm2d-192            [-1, 128, 8, 8]             256\n",
      "            ReLU-193            [-1, 128, 8, 8]               0\n",
      "          Conv2d-194             [-1, 32, 8, 8]          36,864\n",
      "     BatchNorm2d-195            [-1, 640, 8, 8]           1,280\n",
      "            ReLU-196            [-1, 640, 8, 8]               0\n",
      "          Conv2d-197            [-1, 128, 8, 8]          81,920\n",
      "     BatchNorm2d-198            [-1, 128, 8, 8]             256\n",
      "            ReLU-199            [-1, 128, 8, 8]               0\n",
      "          Conv2d-200             [-1, 32, 8, 8]          36,864\n",
      "     BatchNorm2d-201            [-1, 672, 8, 8]           1,344\n",
      "            ReLU-202            [-1, 672, 8, 8]               0\n",
      "          Conv2d-203            [-1, 128, 8, 8]          86,016\n",
      "     BatchNorm2d-204            [-1, 128, 8, 8]             256\n",
      "            ReLU-205            [-1, 128, 8, 8]               0\n",
      "          Conv2d-206             [-1, 32, 8, 8]          36,864\n",
      "     BatchNorm2d-207            [-1, 704, 8, 8]           1,408\n",
      "            ReLU-208            [-1, 704, 8, 8]               0\n",
      "          Conv2d-209            [-1, 128, 8, 8]          90,112\n",
      "     BatchNorm2d-210            [-1, 128, 8, 8]             256\n",
      "            ReLU-211            [-1, 128, 8, 8]               0\n",
      "          Conv2d-212             [-1, 32, 8, 8]          36,864\n",
      "     BatchNorm2d-213            [-1, 736, 8, 8]           1,472\n",
      "            ReLU-214            [-1, 736, 8, 8]               0\n",
      "          Conv2d-215            [-1, 128, 8, 8]          94,208\n",
      "     BatchNorm2d-216            [-1, 128, 8, 8]             256\n",
      "            ReLU-217            [-1, 128, 8, 8]               0\n",
      "          Conv2d-218             [-1, 32, 8, 8]          36,864\n",
      "     BatchNorm2d-219            [-1, 768, 8, 8]           1,536\n",
      "            ReLU-220            [-1, 768, 8, 8]               0\n",
      "          Conv2d-221            [-1, 128, 8, 8]          98,304\n",
      "     BatchNorm2d-222            [-1, 128, 8, 8]             256\n",
      "            ReLU-223            [-1, 128, 8, 8]               0\n",
      "          Conv2d-224             [-1, 32, 8, 8]          36,864\n",
      "     BatchNorm2d-225            [-1, 800, 8, 8]           1,600\n",
      "            ReLU-226            [-1, 800, 8, 8]               0\n",
      "          Conv2d-227            [-1, 128, 8, 8]         102,400\n",
      "     BatchNorm2d-228            [-1, 128, 8, 8]             256\n",
      "            ReLU-229            [-1, 128, 8, 8]               0\n",
      "          Conv2d-230             [-1, 32, 8, 8]          36,864\n",
      "     BatchNorm2d-231            [-1, 832, 8, 8]           1,664\n",
      "            ReLU-232            [-1, 832, 8, 8]               0\n",
      "          Conv2d-233            [-1, 128, 8, 8]         106,496\n",
      "     BatchNorm2d-234            [-1, 128, 8, 8]             256\n",
      "            ReLU-235            [-1, 128, 8, 8]               0\n",
      "          Conv2d-236             [-1, 32, 8, 8]          36,864\n",
      "     BatchNorm2d-237            [-1, 864, 8, 8]           1,728\n",
      "            ReLU-238            [-1, 864, 8, 8]               0\n",
      "          Conv2d-239            [-1, 128, 8, 8]         110,592\n",
      "     BatchNorm2d-240            [-1, 128, 8, 8]             256\n",
      "            ReLU-241            [-1, 128, 8, 8]               0\n",
      "          Conv2d-242             [-1, 32, 8, 8]          36,864\n",
      "     BatchNorm2d-243            [-1, 896, 8, 8]           1,792\n",
      "            ReLU-244            [-1, 896, 8, 8]               0\n",
      "          Conv2d-245            [-1, 128, 8, 8]         114,688\n",
      "     BatchNorm2d-246            [-1, 128, 8, 8]             256\n",
      "            ReLU-247            [-1, 128, 8, 8]               0\n",
      "          Conv2d-248             [-1, 32, 8, 8]          36,864\n",
      "     BatchNorm2d-249            [-1, 928, 8, 8]           1,856\n",
      "            ReLU-250            [-1, 928, 8, 8]               0\n",
      "          Conv2d-251            [-1, 128, 8, 8]         118,784\n",
      "     BatchNorm2d-252            [-1, 128, 8, 8]             256\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            ReLU-253            [-1, 128, 8, 8]               0\n",
      "          Conv2d-254             [-1, 32, 8, 8]          36,864\n",
      "     BatchNorm2d-255            [-1, 960, 8, 8]           1,920\n",
      "            ReLU-256            [-1, 960, 8, 8]               0\n",
      "          Conv2d-257            [-1, 128, 8, 8]         122,880\n",
      "     BatchNorm2d-258            [-1, 128, 8, 8]             256\n",
      "            ReLU-259            [-1, 128, 8, 8]               0\n",
      "          Conv2d-260             [-1, 32, 8, 8]          36,864\n",
      "     BatchNorm2d-261            [-1, 992, 8, 8]           1,984\n",
      "            ReLU-262            [-1, 992, 8, 8]               0\n",
      "          Conv2d-263            [-1, 128, 8, 8]         126,976\n",
      "     BatchNorm2d-264            [-1, 128, 8, 8]             256\n",
      "            ReLU-265            [-1, 128, 8, 8]               0\n",
      "          Conv2d-266             [-1, 32, 8, 8]          36,864\n",
      "     _DenseBlock-267           [-1, 1024, 8, 8]               0\n",
      "     BatchNorm2d-268           [-1, 1024, 8, 8]           2,048\n",
      "            ReLU-269           [-1, 1024, 8, 8]               0\n",
      "          Conv2d-270            [-1, 512, 8, 8]         524,288\n",
      "       AvgPool2d-271            [-1, 512, 4, 4]               0\n",
      "     BatchNorm2d-272            [-1, 512, 4, 4]           1,024\n",
      "            ReLU-273            [-1, 512, 4, 4]               0\n",
      "          Conv2d-274            [-1, 128, 4, 4]          65,536\n",
      "     BatchNorm2d-275            [-1, 128, 4, 4]             256\n",
      "            ReLU-276            [-1, 128, 4, 4]               0\n",
      "          Conv2d-277             [-1, 32, 4, 4]          36,864\n",
      "     BatchNorm2d-278            [-1, 544, 4, 4]           1,088\n",
      "            ReLU-279            [-1, 544, 4, 4]               0\n",
      "          Conv2d-280            [-1, 128, 4, 4]          69,632\n",
      "     BatchNorm2d-281            [-1, 128, 4, 4]             256\n",
      "            ReLU-282            [-1, 128, 4, 4]               0\n",
      "          Conv2d-283             [-1, 32, 4, 4]          36,864\n",
      "     BatchNorm2d-284            [-1, 576, 4, 4]           1,152\n",
      "            ReLU-285            [-1, 576, 4, 4]               0\n",
      "          Conv2d-286            [-1, 128, 4, 4]          73,728\n",
      "     BatchNorm2d-287            [-1, 128, 4, 4]             256\n",
      "            ReLU-288            [-1, 128, 4, 4]               0\n",
      "          Conv2d-289             [-1, 32, 4, 4]          36,864\n",
      "     BatchNorm2d-290            [-1, 608, 4, 4]           1,216\n",
      "            ReLU-291            [-1, 608, 4, 4]               0\n",
      "          Conv2d-292            [-1, 128, 4, 4]          77,824\n",
      "     BatchNorm2d-293            [-1, 128, 4, 4]             256\n",
      "            ReLU-294            [-1, 128, 4, 4]               0\n",
      "          Conv2d-295             [-1, 32, 4, 4]          36,864\n",
      "     BatchNorm2d-296            [-1, 640, 4, 4]           1,280\n",
      "            ReLU-297            [-1, 640, 4, 4]               0\n",
      "          Conv2d-298            [-1, 128, 4, 4]          81,920\n",
      "     BatchNorm2d-299            [-1, 128, 4, 4]             256\n",
      "            ReLU-300            [-1, 128, 4, 4]               0\n",
      "          Conv2d-301             [-1, 32, 4, 4]          36,864\n",
      "     BatchNorm2d-302            [-1, 672, 4, 4]           1,344\n",
      "            ReLU-303            [-1, 672, 4, 4]               0\n",
      "          Conv2d-304            [-1, 128, 4, 4]          86,016\n",
      "     BatchNorm2d-305            [-1, 128, 4, 4]             256\n",
      "            ReLU-306            [-1, 128, 4, 4]               0\n",
      "          Conv2d-307             [-1, 32, 4, 4]          36,864\n",
      "     BatchNorm2d-308            [-1, 704, 4, 4]           1,408\n",
      "            ReLU-309            [-1, 704, 4, 4]               0\n",
      "          Conv2d-310            [-1, 128, 4, 4]          90,112\n",
      "     BatchNorm2d-311            [-1, 128, 4, 4]             256\n",
      "            ReLU-312            [-1, 128, 4, 4]               0\n",
      "          Conv2d-313             [-1, 32, 4, 4]          36,864\n",
      "     BatchNorm2d-314            [-1, 736, 4, 4]           1,472\n",
      "            ReLU-315            [-1, 736, 4, 4]               0\n",
      "          Conv2d-316            [-1, 128, 4, 4]          94,208\n",
      "     BatchNorm2d-317            [-1, 128, 4, 4]             256\n",
      "            ReLU-318            [-1, 128, 4, 4]               0\n",
      "          Conv2d-319             [-1, 32, 4, 4]          36,864\n",
      "     BatchNorm2d-320            [-1, 768, 4, 4]           1,536\n",
      "            ReLU-321            [-1, 768, 4, 4]               0\n",
      "          Conv2d-322            [-1, 128, 4, 4]          98,304\n",
      "     BatchNorm2d-323            [-1, 128, 4, 4]             256\n",
      "            ReLU-324            [-1, 128, 4, 4]               0\n",
      "          Conv2d-325             [-1, 32, 4, 4]          36,864\n",
      "     BatchNorm2d-326            [-1, 800, 4, 4]           1,600\n",
      "            ReLU-327            [-1, 800, 4, 4]               0\n",
      "          Conv2d-328            [-1, 128, 4, 4]         102,400\n",
      "     BatchNorm2d-329            [-1, 128, 4, 4]             256\n",
      "            ReLU-330            [-1, 128, 4, 4]               0\n",
      "          Conv2d-331             [-1, 32, 4, 4]          36,864\n",
      "     BatchNorm2d-332            [-1, 832, 4, 4]           1,664\n",
      "            ReLU-333            [-1, 832, 4, 4]               0\n",
      "          Conv2d-334            [-1, 128, 4, 4]         106,496\n",
      "     BatchNorm2d-335            [-1, 128, 4, 4]             256\n",
      "            ReLU-336            [-1, 128, 4, 4]               0\n",
      "          Conv2d-337             [-1, 32, 4, 4]          36,864\n",
      "     BatchNorm2d-338            [-1, 864, 4, 4]           1,728\n",
      "            ReLU-339            [-1, 864, 4, 4]               0\n",
      "          Conv2d-340            [-1, 128, 4, 4]         110,592\n",
      "     BatchNorm2d-341            [-1, 128, 4, 4]             256\n",
      "            ReLU-342            [-1, 128, 4, 4]               0\n",
      "          Conv2d-343             [-1, 32, 4, 4]          36,864\n",
      "     BatchNorm2d-344            [-1, 896, 4, 4]           1,792\n",
      "            ReLU-345            [-1, 896, 4, 4]               0\n",
      "          Conv2d-346            [-1, 128, 4, 4]         114,688\n",
      "     BatchNorm2d-347            [-1, 128, 4, 4]             256\n",
      "            ReLU-348            [-1, 128, 4, 4]               0\n",
      "          Conv2d-349             [-1, 32, 4, 4]          36,864\n",
      "     BatchNorm2d-350            [-1, 928, 4, 4]           1,856\n",
      "            ReLU-351            [-1, 928, 4, 4]               0\n",
      "          Conv2d-352            [-1, 128, 4, 4]         118,784\n",
      "     BatchNorm2d-353            [-1, 128, 4, 4]             256\n",
      "            ReLU-354            [-1, 128, 4, 4]               0\n",
      "          Conv2d-355             [-1, 32, 4, 4]          36,864\n",
      "     BatchNorm2d-356            [-1, 960, 4, 4]           1,920\n",
      "            ReLU-357            [-1, 960, 4, 4]               0\n",
      "          Conv2d-358            [-1, 128, 4, 4]         122,880\n",
      "     BatchNorm2d-359            [-1, 128, 4, 4]             256\n",
      "            ReLU-360            [-1, 128, 4, 4]               0\n",
      "          Conv2d-361             [-1, 32, 4, 4]          36,864\n",
      "     BatchNorm2d-362            [-1, 992, 4, 4]           1,984\n",
      "            ReLU-363            [-1, 992, 4, 4]               0\n",
      "          Conv2d-364            [-1, 128, 4, 4]         126,976\n",
      "     BatchNorm2d-365            [-1, 128, 4, 4]             256\n",
      "            ReLU-366            [-1, 128, 4, 4]               0\n",
      "          Conv2d-367             [-1, 32, 4, 4]          36,864\n",
      "     _DenseBlock-368           [-1, 1024, 4, 4]               0\n",
      "     BatchNorm2d-369           [-1, 1024, 4, 4]           2,048\n",
      "          Linear-370                  [-1, 128]         131,200\n",
      "        DenseNet-371                  [-1, 128]               0\n",
      "================================================================\n",
      "Total params: 7,085,056\n",
      "Trainable params: 0\n",
      "Non-trainable params: 7,085,056\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.19\n",
      "Forward/backward pass size (MB): 99.69\n",
      "Params size (MB): 27.03\n",
      "Estimated Total Size (MB): 126.90\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Encoder model\n",
    "encoder_model = load_pretrained_encoder_model()\n",
    "for param in encoder_model.parameters(): param.requires_grad = False   # freeze encoder layers\n",
    "summary(encoder_model, input_size=(3, IMG_HEIGHT, IMG_HEIGHT), device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 64, 64]           9,408\n",
      "       BatchNorm2d-2           [-1, 64, 64, 64]             128\n",
      "              ReLU-3           [-1, 64, 64, 64]               0\n",
      "         MaxPool2d-4           [-1, 64, 32, 32]               0\n",
      "       BatchNorm2d-5           [-1, 64, 32, 32]             128\n",
      "              ReLU-6           [-1, 64, 32, 32]               0\n",
      "            Conv2d-7          [-1, 128, 32, 32]           8,192\n",
      "       BatchNorm2d-8          [-1, 128, 32, 32]             256\n",
      "              ReLU-9          [-1, 128, 32, 32]               0\n",
      "           Conv2d-10           [-1, 32, 32, 32]          36,864\n",
      "      BatchNorm2d-11           [-1, 96, 32, 32]             192\n",
      "             ReLU-12           [-1, 96, 32, 32]               0\n",
      "           Conv2d-13          [-1, 128, 32, 32]          12,288\n",
      "      BatchNorm2d-14          [-1, 128, 32, 32]             256\n",
      "             ReLU-15          [-1, 128, 32, 32]               0\n",
      "           Conv2d-16           [-1, 32, 32, 32]          36,864\n",
      "      BatchNorm2d-17          [-1, 128, 32, 32]             256\n",
      "             ReLU-18          [-1, 128, 32, 32]               0\n",
      "           Conv2d-19          [-1, 128, 32, 32]          16,384\n",
      "      BatchNorm2d-20          [-1, 128, 32, 32]             256\n",
      "             ReLU-21          [-1, 128, 32, 32]               0\n",
      "           Conv2d-22           [-1, 32, 32, 32]          36,864\n",
      "      BatchNorm2d-23          [-1, 160, 32, 32]             320\n",
      "             ReLU-24          [-1, 160, 32, 32]               0\n",
      "           Conv2d-25          [-1, 128, 32, 32]          20,480\n",
      "      BatchNorm2d-26          [-1, 128, 32, 32]             256\n",
      "             ReLU-27          [-1, 128, 32, 32]               0\n",
      "           Conv2d-28           [-1, 32, 32, 32]          36,864\n",
      "      BatchNorm2d-29          [-1, 192, 32, 32]             384\n",
      "             ReLU-30          [-1, 192, 32, 32]               0\n",
      "           Conv2d-31          [-1, 128, 32, 32]          24,576\n",
      "      BatchNorm2d-32          [-1, 128, 32, 32]             256\n",
      "             ReLU-33          [-1, 128, 32, 32]               0\n",
      "           Conv2d-34           [-1, 32, 32, 32]          36,864\n",
      "      BatchNorm2d-35          [-1, 224, 32, 32]             448\n",
      "             ReLU-36          [-1, 224, 32, 32]               0\n",
      "           Conv2d-37          [-1, 128, 32, 32]          28,672\n",
      "      BatchNorm2d-38          [-1, 128, 32, 32]             256\n",
      "             ReLU-39          [-1, 128, 32, 32]               0\n",
      "           Conv2d-40           [-1, 32, 32, 32]          36,864\n",
      "      _DenseBlock-41          [-1, 256, 32, 32]               0\n",
      "      BatchNorm2d-42          [-1, 256, 32, 32]             512\n",
      "             ReLU-43          [-1, 256, 32, 32]               0\n",
      "           Conv2d-44          [-1, 128, 32, 32]          32,768\n",
      "        AvgPool2d-45          [-1, 128, 16, 16]               0\n",
      "      BatchNorm2d-46          [-1, 128, 16, 16]             256\n",
      "             ReLU-47          [-1, 128, 16, 16]               0\n",
      "           Conv2d-48          [-1, 128, 16, 16]          16,384\n",
      "      BatchNorm2d-49          [-1, 128, 16, 16]             256\n",
      "             ReLU-50          [-1, 128, 16, 16]               0\n",
      "           Conv2d-51           [-1, 32, 16, 16]          36,864\n",
      "      BatchNorm2d-52          [-1, 160, 16, 16]             320\n",
      "             ReLU-53          [-1, 160, 16, 16]               0\n",
      "           Conv2d-54          [-1, 128, 16, 16]          20,480\n",
      "      BatchNorm2d-55          [-1, 128, 16, 16]             256\n",
      "             ReLU-56          [-1, 128, 16, 16]               0\n",
      "           Conv2d-57           [-1, 32, 16, 16]          36,864\n",
      "      BatchNorm2d-58          [-1, 192, 16, 16]             384\n",
      "             ReLU-59          [-1, 192, 16, 16]               0\n",
      "           Conv2d-60          [-1, 128, 16, 16]          24,576\n",
      "      BatchNorm2d-61          [-1, 128, 16, 16]             256\n",
      "             ReLU-62          [-1, 128, 16, 16]               0\n",
      "           Conv2d-63           [-1, 32, 16, 16]          36,864\n",
      "      BatchNorm2d-64          [-1, 224, 16, 16]             448\n",
      "             ReLU-65          [-1, 224, 16, 16]               0\n",
      "           Conv2d-66          [-1, 128, 16, 16]          28,672\n",
      "      BatchNorm2d-67          [-1, 128, 16, 16]             256\n",
      "             ReLU-68          [-1, 128, 16, 16]               0\n",
      "           Conv2d-69           [-1, 32, 16, 16]          36,864\n",
      "      BatchNorm2d-70          [-1, 256, 16, 16]             512\n",
      "             ReLU-71          [-1, 256, 16, 16]               0\n",
      "           Conv2d-72          [-1, 128, 16, 16]          32,768\n",
      "      BatchNorm2d-73          [-1, 128, 16, 16]             256\n",
      "             ReLU-74          [-1, 128, 16, 16]               0\n",
      "           Conv2d-75           [-1, 32, 16, 16]          36,864\n",
      "      BatchNorm2d-76          [-1, 288, 16, 16]             576\n",
      "             ReLU-77          [-1, 288, 16, 16]               0\n",
      "           Conv2d-78          [-1, 128, 16, 16]          36,864\n",
      "      BatchNorm2d-79          [-1, 128, 16, 16]             256\n",
      "             ReLU-80          [-1, 128, 16, 16]               0\n",
      "           Conv2d-81           [-1, 32, 16, 16]          36,864\n",
      "      BatchNorm2d-82          [-1, 320, 16, 16]             640\n",
      "             ReLU-83          [-1, 320, 16, 16]               0\n",
      "           Conv2d-84          [-1, 128, 16, 16]          40,960\n",
      "      BatchNorm2d-85          [-1, 128, 16, 16]             256\n",
      "             ReLU-86          [-1, 128, 16, 16]               0\n",
      "           Conv2d-87           [-1, 32, 16, 16]          36,864\n",
      "      BatchNorm2d-88          [-1, 352, 16, 16]             704\n",
      "             ReLU-89          [-1, 352, 16, 16]               0\n",
      "           Conv2d-90          [-1, 128, 16, 16]          45,056\n",
      "      BatchNorm2d-91          [-1, 128, 16, 16]             256\n",
      "             ReLU-92          [-1, 128, 16, 16]               0\n",
      "           Conv2d-93           [-1, 32, 16, 16]          36,864\n",
      "      BatchNorm2d-94          [-1, 384, 16, 16]             768\n",
      "             ReLU-95          [-1, 384, 16, 16]               0\n",
      "           Conv2d-96          [-1, 128, 16, 16]          49,152\n",
      "      BatchNorm2d-97          [-1, 128, 16, 16]             256\n",
      "             ReLU-98          [-1, 128, 16, 16]               0\n",
      "           Conv2d-99           [-1, 32, 16, 16]          36,864\n",
      "     BatchNorm2d-100          [-1, 416, 16, 16]             832\n",
      "            ReLU-101          [-1, 416, 16, 16]               0\n",
      "          Conv2d-102          [-1, 128, 16, 16]          53,248\n",
      "     BatchNorm2d-103          [-1, 128, 16, 16]             256\n",
      "            ReLU-104          [-1, 128, 16, 16]               0\n",
      "          Conv2d-105           [-1, 32, 16, 16]          36,864\n",
      "     BatchNorm2d-106          [-1, 448, 16, 16]             896\n",
      "            ReLU-107          [-1, 448, 16, 16]               0\n",
      "          Conv2d-108          [-1, 128, 16, 16]          57,344\n",
      "     BatchNorm2d-109          [-1, 128, 16, 16]             256\n",
      "            ReLU-110          [-1, 128, 16, 16]               0\n",
      "          Conv2d-111           [-1, 32, 16, 16]          36,864\n",
      "     BatchNorm2d-112          [-1, 480, 16, 16]             960\n",
      "            ReLU-113          [-1, 480, 16, 16]               0\n",
      "          Conv2d-114          [-1, 128, 16, 16]          61,440\n",
      "     BatchNorm2d-115          [-1, 128, 16, 16]             256\n",
      "            ReLU-116          [-1, 128, 16, 16]               0\n",
      "          Conv2d-117           [-1, 32, 16, 16]          36,864\n",
      "     _DenseBlock-118          [-1, 512, 16, 16]               0\n",
      "     BatchNorm2d-119          [-1, 512, 16, 16]           1,024\n",
      "            ReLU-120          [-1, 512, 16, 16]               0\n",
      "          Conv2d-121          [-1, 256, 16, 16]         131,072\n",
      "       AvgPool2d-122            [-1, 256, 8, 8]               0\n",
      "     BatchNorm2d-123            [-1, 256, 8, 8]             512\n",
      "            ReLU-124            [-1, 256, 8, 8]               0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Conv2d-125            [-1, 128, 8, 8]          32,768\n",
      "     BatchNorm2d-126            [-1, 128, 8, 8]             256\n",
      "            ReLU-127            [-1, 128, 8, 8]               0\n",
      "          Conv2d-128             [-1, 32, 8, 8]          36,864\n",
      "     BatchNorm2d-129            [-1, 288, 8, 8]             576\n",
      "            ReLU-130            [-1, 288, 8, 8]               0\n",
      "          Conv2d-131            [-1, 128, 8, 8]          36,864\n",
      "     BatchNorm2d-132            [-1, 128, 8, 8]             256\n",
      "            ReLU-133            [-1, 128, 8, 8]               0\n",
      "          Conv2d-134             [-1, 32, 8, 8]          36,864\n",
      "     BatchNorm2d-135            [-1, 320, 8, 8]             640\n",
      "            ReLU-136            [-1, 320, 8, 8]               0\n",
      "          Conv2d-137            [-1, 128, 8, 8]          40,960\n",
      "     BatchNorm2d-138            [-1, 128, 8, 8]             256\n",
      "            ReLU-139            [-1, 128, 8, 8]               0\n",
      "          Conv2d-140             [-1, 32, 8, 8]          36,864\n",
      "     BatchNorm2d-141            [-1, 352, 8, 8]             704\n",
      "            ReLU-142            [-1, 352, 8, 8]               0\n",
      "          Conv2d-143            [-1, 128, 8, 8]          45,056\n",
      "     BatchNorm2d-144            [-1, 128, 8, 8]             256\n",
      "            ReLU-145            [-1, 128, 8, 8]               0\n",
      "          Conv2d-146             [-1, 32, 8, 8]          36,864\n",
      "     BatchNorm2d-147            [-1, 384, 8, 8]             768\n",
      "            ReLU-148            [-1, 384, 8, 8]               0\n",
      "          Conv2d-149            [-1, 128, 8, 8]          49,152\n",
      "     BatchNorm2d-150            [-1, 128, 8, 8]             256\n",
      "            ReLU-151            [-1, 128, 8, 8]               0\n",
      "          Conv2d-152             [-1, 32, 8, 8]          36,864\n",
      "     BatchNorm2d-153            [-1, 416, 8, 8]             832\n",
      "            ReLU-154            [-1, 416, 8, 8]               0\n",
      "          Conv2d-155            [-1, 128, 8, 8]          53,248\n",
      "     BatchNorm2d-156            [-1, 128, 8, 8]             256\n",
      "            ReLU-157            [-1, 128, 8, 8]               0\n",
      "          Conv2d-158             [-1, 32, 8, 8]          36,864\n",
      "     BatchNorm2d-159            [-1, 448, 8, 8]             896\n",
      "            ReLU-160            [-1, 448, 8, 8]               0\n",
      "          Conv2d-161            [-1, 128, 8, 8]          57,344\n",
      "     BatchNorm2d-162            [-1, 128, 8, 8]             256\n",
      "            ReLU-163            [-1, 128, 8, 8]               0\n",
      "          Conv2d-164             [-1, 32, 8, 8]          36,864\n",
      "     BatchNorm2d-165            [-1, 480, 8, 8]             960\n",
      "            ReLU-166            [-1, 480, 8, 8]               0\n",
      "          Conv2d-167            [-1, 128, 8, 8]          61,440\n",
      "     BatchNorm2d-168            [-1, 128, 8, 8]             256\n",
      "            ReLU-169            [-1, 128, 8, 8]               0\n",
      "          Conv2d-170             [-1, 32, 8, 8]          36,864\n",
      "     BatchNorm2d-171            [-1, 512, 8, 8]           1,024\n",
      "            ReLU-172            [-1, 512, 8, 8]               0\n",
      "          Conv2d-173            [-1, 128, 8, 8]          65,536\n",
      "     BatchNorm2d-174            [-1, 128, 8, 8]             256\n",
      "            ReLU-175            [-1, 128, 8, 8]               0\n",
      "          Conv2d-176             [-1, 32, 8, 8]          36,864\n",
      "     BatchNorm2d-177            [-1, 544, 8, 8]           1,088\n",
      "            ReLU-178            [-1, 544, 8, 8]               0\n",
      "          Conv2d-179            [-1, 128, 8, 8]          69,632\n",
      "     BatchNorm2d-180            [-1, 128, 8, 8]             256\n",
      "            ReLU-181            [-1, 128, 8, 8]               0\n",
      "          Conv2d-182             [-1, 32, 8, 8]          36,864\n",
      "     BatchNorm2d-183            [-1, 576, 8, 8]           1,152\n",
      "            ReLU-184            [-1, 576, 8, 8]               0\n",
      "          Conv2d-185            [-1, 128, 8, 8]          73,728\n",
      "     BatchNorm2d-186            [-1, 128, 8, 8]             256\n",
      "            ReLU-187            [-1, 128, 8, 8]               0\n",
      "          Conv2d-188             [-1, 32, 8, 8]          36,864\n",
      "     BatchNorm2d-189            [-1, 608, 8, 8]           1,216\n",
      "            ReLU-190            [-1, 608, 8, 8]               0\n",
      "          Conv2d-191            [-1, 128, 8, 8]          77,824\n",
      "     BatchNorm2d-192            [-1, 128, 8, 8]             256\n",
      "            ReLU-193            [-1, 128, 8, 8]               0\n",
      "          Conv2d-194             [-1, 32, 8, 8]          36,864\n",
      "     BatchNorm2d-195            [-1, 640, 8, 8]           1,280\n",
      "            ReLU-196            [-1, 640, 8, 8]               0\n",
      "          Conv2d-197            [-1, 128, 8, 8]          81,920\n",
      "     BatchNorm2d-198            [-1, 128, 8, 8]             256\n",
      "            ReLU-199            [-1, 128, 8, 8]               0\n",
      "          Conv2d-200             [-1, 32, 8, 8]          36,864\n",
      "     BatchNorm2d-201            [-1, 672, 8, 8]           1,344\n",
      "            ReLU-202            [-1, 672, 8, 8]               0\n",
      "          Conv2d-203            [-1, 128, 8, 8]          86,016\n",
      "     BatchNorm2d-204            [-1, 128, 8, 8]             256\n",
      "            ReLU-205            [-1, 128, 8, 8]               0\n",
      "          Conv2d-206             [-1, 32, 8, 8]          36,864\n",
      "     BatchNorm2d-207            [-1, 704, 8, 8]           1,408\n",
      "            ReLU-208            [-1, 704, 8, 8]               0\n",
      "          Conv2d-209            [-1, 128, 8, 8]          90,112\n",
      "     BatchNorm2d-210            [-1, 128, 8, 8]             256\n",
      "            ReLU-211            [-1, 128, 8, 8]               0\n",
      "          Conv2d-212             [-1, 32, 8, 8]          36,864\n",
      "     BatchNorm2d-213            [-1, 736, 8, 8]           1,472\n",
      "            ReLU-214            [-1, 736, 8, 8]               0\n",
      "          Conv2d-215            [-1, 128, 8, 8]          94,208\n",
      "     BatchNorm2d-216            [-1, 128, 8, 8]             256\n",
      "            ReLU-217            [-1, 128, 8, 8]               0\n",
      "          Conv2d-218             [-1, 32, 8, 8]          36,864\n",
      "     BatchNorm2d-219            [-1, 768, 8, 8]           1,536\n",
      "            ReLU-220            [-1, 768, 8, 8]               0\n",
      "          Conv2d-221            [-1, 128, 8, 8]          98,304\n",
      "     BatchNorm2d-222            [-1, 128, 8, 8]             256\n",
      "            ReLU-223            [-1, 128, 8, 8]               0\n",
      "          Conv2d-224             [-1, 32, 8, 8]          36,864\n",
      "     BatchNorm2d-225            [-1, 800, 8, 8]           1,600\n",
      "            ReLU-226            [-1, 800, 8, 8]               0\n",
      "          Conv2d-227            [-1, 128, 8, 8]         102,400\n",
      "     BatchNorm2d-228            [-1, 128, 8, 8]             256\n",
      "            ReLU-229            [-1, 128, 8, 8]               0\n",
      "          Conv2d-230             [-1, 32, 8, 8]          36,864\n",
      "     BatchNorm2d-231            [-1, 832, 8, 8]           1,664\n",
      "            ReLU-232            [-1, 832, 8, 8]               0\n",
      "          Conv2d-233            [-1, 128, 8, 8]         106,496\n",
      "     BatchNorm2d-234            [-1, 128, 8, 8]             256\n",
      "            ReLU-235            [-1, 128, 8, 8]               0\n",
      "          Conv2d-236             [-1, 32, 8, 8]          36,864\n",
      "     BatchNorm2d-237            [-1, 864, 8, 8]           1,728\n",
      "            ReLU-238            [-1, 864, 8, 8]               0\n",
      "          Conv2d-239            [-1, 128, 8, 8]         110,592\n",
      "     BatchNorm2d-240            [-1, 128, 8, 8]             256\n",
      "            ReLU-241            [-1, 128, 8, 8]               0\n",
      "          Conv2d-242             [-1, 32, 8, 8]          36,864\n",
      "     BatchNorm2d-243            [-1, 896, 8, 8]           1,792\n",
      "            ReLU-244            [-1, 896, 8, 8]               0\n",
      "          Conv2d-245            [-1, 128, 8, 8]         114,688\n",
      "     BatchNorm2d-246            [-1, 128, 8, 8]             256\n",
      "            ReLU-247            [-1, 128, 8, 8]               0\n",
      "          Conv2d-248             [-1, 32, 8, 8]          36,864\n",
      "     BatchNorm2d-249            [-1, 928, 8, 8]           1,856\n",
      "            ReLU-250            [-1, 928, 8, 8]               0\n",
      "          Conv2d-251            [-1, 128, 8, 8]         118,784\n",
      "     BatchNorm2d-252            [-1, 128, 8, 8]             256\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            ReLU-253            [-1, 128, 8, 8]               0\n",
      "          Conv2d-254             [-1, 32, 8, 8]          36,864\n",
      "     BatchNorm2d-255            [-1, 960, 8, 8]           1,920\n",
      "            ReLU-256            [-1, 960, 8, 8]               0\n",
      "          Conv2d-257            [-1, 128, 8, 8]         122,880\n",
      "     BatchNorm2d-258            [-1, 128, 8, 8]             256\n",
      "            ReLU-259            [-1, 128, 8, 8]               0\n",
      "          Conv2d-260             [-1, 32, 8, 8]          36,864\n",
      "     BatchNorm2d-261            [-1, 992, 8, 8]           1,984\n",
      "            ReLU-262            [-1, 992, 8, 8]               0\n",
      "          Conv2d-263            [-1, 128, 8, 8]         126,976\n",
      "     BatchNorm2d-264            [-1, 128, 8, 8]             256\n",
      "            ReLU-265            [-1, 128, 8, 8]               0\n",
      "          Conv2d-266             [-1, 32, 8, 8]          36,864\n",
      "     _DenseBlock-267           [-1, 1024, 8, 8]               0\n",
      "     BatchNorm2d-268           [-1, 1024, 8, 8]           2,048\n",
      "            ReLU-269           [-1, 1024, 8, 8]               0\n",
      "          Conv2d-270            [-1, 512, 8, 8]         524,288\n",
      "       AvgPool2d-271            [-1, 512, 4, 4]               0\n",
      "     BatchNorm2d-272            [-1, 512, 4, 4]           1,024\n",
      "            ReLU-273            [-1, 512, 4, 4]               0\n",
      "          Conv2d-274            [-1, 128, 4, 4]          65,536\n",
      "     BatchNorm2d-275            [-1, 128, 4, 4]             256\n",
      "            ReLU-276            [-1, 128, 4, 4]               0\n",
      "          Conv2d-277             [-1, 32, 4, 4]          36,864\n",
      "     BatchNorm2d-278            [-1, 544, 4, 4]           1,088\n",
      "            ReLU-279            [-1, 544, 4, 4]               0\n",
      "          Conv2d-280            [-1, 128, 4, 4]          69,632\n",
      "     BatchNorm2d-281            [-1, 128, 4, 4]             256\n",
      "            ReLU-282            [-1, 128, 4, 4]               0\n",
      "          Conv2d-283             [-1, 32, 4, 4]          36,864\n",
      "     BatchNorm2d-284            [-1, 576, 4, 4]           1,152\n",
      "            ReLU-285            [-1, 576, 4, 4]               0\n",
      "          Conv2d-286            [-1, 128, 4, 4]          73,728\n",
      "     BatchNorm2d-287            [-1, 128, 4, 4]             256\n",
      "            ReLU-288            [-1, 128, 4, 4]               0\n",
      "          Conv2d-289             [-1, 32, 4, 4]          36,864\n",
      "     BatchNorm2d-290            [-1, 608, 4, 4]           1,216\n",
      "            ReLU-291            [-1, 608, 4, 4]               0\n",
      "          Conv2d-292            [-1, 128, 4, 4]          77,824\n",
      "     BatchNorm2d-293            [-1, 128, 4, 4]             256\n",
      "            ReLU-294            [-1, 128, 4, 4]               0\n",
      "          Conv2d-295             [-1, 32, 4, 4]          36,864\n",
      "     BatchNorm2d-296            [-1, 640, 4, 4]           1,280\n",
      "            ReLU-297            [-1, 640, 4, 4]               0\n",
      "          Conv2d-298            [-1, 128, 4, 4]          81,920\n",
      "     BatchNorm2d-299            [-1, 128, 4, 4]             256\n",
      "            ReLU-300            [-1, 128, 4, 4]               0\n",
      "          Conv2d-301             [-1, 32, 4, 4]          36,864\n",
      "     BatchNorm2d-302            [-1, 672, 4, 4]           1,344\n",
      "            ReLU-303            [-1, 672, 4, 4]               0\n",
      "          Conv2d-304            [-1, 128, 4, 4]          86,016\n",
      "     BatchNorm2d-305            [-1, 128, 4, 4]             256\n",
      "            ReLU-306            [-1, 128, 4, 4]               0\n",
      "          Conv2d-307             [-1, 32, 4, 4]          36,864\n",
      "     BatchNorm2d-308            [-1, 704, 4, 4]           1,408\n",
      "            ReLU-309            [-1, 704, 4, 4]               0\n",
      "          Conv2d-310            [-1, 128, 4, 4]          90,112\n",
      "     BatchNorm2d-311            [-1, 128, 4, 4]             256\n",
      "            ReLU-312            [-1, 128, 4, 4]               0\n",
      "          Conv2d-313             [-1, 32, 4, 4]          36,864\n",
      "     BatchNorm2d-314            [-1, 736, 4, 4]           1,472\n",
      "            ReLU-315            [-1, 736, 4, 4]               0\n",
      "          Conv2d-316            [-1, 128, 4, 4]          94,208\n",
      "     BatchNorm2d-317            [-1, 128, 4, 4]             256\n",
      "            ReLU-318            [-1, 128, 4, 4]               0\n",
      "          Conv2d-319             [-1, 32, 4, 4]          36,864\n",
      "     BatchNorm2d-320            [-1, 768, 4, 4]           1,536\n",
      "            ReLU-321            [-1, 768, 4, 4]               0\n",
      "          Conv2d-322            [-1, 128, 4, 4]          98,304\n",
      "     BatchNorm2d-323            [-1, 128, 4, 4]             256\n",
      "            ReLU-324            [-1, 128, 4, 4]               0\n",
      "          Conv2d-325             [-1, 32, 4, 4]          36,864\n",
      "     BatchNorm2d-326            [-1, 800, 4, 4]           1,600\n",
      "            ReLU-327            [-1, 800, 4, 4]               0\n",
      "          Conv2d-328            [-1, 128, 4, 4]         102,400\n",
      "     BatchNorm2d-329            [-1, 128, 4, 4]             256\n",
      "            ReLU-330            [-1, 128, 4, 4]               0\n",
      "          Conv2d-331             [-1, 32, 4, 4]          36,864\n",
      "     BatchNorm2d-332            [-1, 832, 4, 4]           1,664\n",
      "            ReLU-333            [-1, 832, 4, 4]               0\n",
      "          Conv2d-334            [-1, 128, 4, 4]         106,496\n",
      "     BatchNorm2d-335            [-1, 128, 4, 4]             256\n",
      "            ReLU-336            [-1, 128, 4, 4]               0\n",
      "          Conv2d-337             [-1, 32, 4, 4]          36,864\n",
      "     BatchNorm2d-338            [-1, 864, 4, 4]           1,728\n",
      "            ReLU-339            [-1, 864, 4, 4]               0\n",
      "          Conv2d-340            [-1, 128, 4, 4]         110,592\n",
      "     BatchNorm2d-341            [-1, 128, 4, 4]             256\n",
      "            ReLU-342            [-1, 128, 4, 4]               0\n",
      "          Conv2d-343             [-1, 32, 4, 4]          36,864\n",
      "     BatchNorm2d-344            [-1, 896, 4, 4]           1,792\n",
      "            ReLU-345            [-1, 896, 4, 4]               0\n",
      "          Conv2d-346            [-1, 128, 4, 4]         114,688\n",
      "     BatchNorm2d-347            [-1, 128, 4, 4]             256\n",
      "            ReLU-348            [-1, 128, 4, 4]               0\n",
      "          Conv2d-349             [-1, 32, 4, 4]          36,864\n",
      "     BatchNorm2d-350            [-1, 928, 4, 4]           1,856\n",
      "            ReLU-351            [-1, 928, 4, 4]               0\n",
      "          Conv2d-352            [-1, 128, 4, 4]         118,784\n",
      "     BatchNorm2d-353            [-1, 128, 4, 4]             256\n",
      "            ReLU-354            [-1, 128, 4, 4]               0\n",
      "          Conv2d-355             [-1, 32, 4, 4]          36,864\n",
      "     BatchNorm2d-356            [-1, 960, 4, 4]           1,920\n",
      "            ReLU-357            [-1, 960, 4, 4]               0\n",
      "          Conv2d-358            [-1, 128, 4, 4]         122,880\n",
      "     BatchNorm2d-359            [-1, 128, 4, 4]             256\n",
      "            ReLU-360            [-1, 128, 4, 4]               0\n",
      "          Conv2d-361             [-1, 32, 4, 4]          36,864\n",
      "     BatchNorm2d-362            [-1, 992, 4, 4]           1,984\n",
      "            ReLU-363            [-1, 992, 4, 4]               0\n",
      "          Conv2d-364            [-1, 128, 4, 4]         126,976\n",
      "     BatchNorm2d-365            [-1, 128, 4, 4]             256\n",
      "            ReLU-366            [-1, 128, 4, 4]               0\n",
      "          Conv2d-367             [-1, 32, 4, 4]          36,864\n",
      "     _DenseBlock-368           [-1, 1024, 4, 4]               0\n",
      "     BatchNorm2d-369           [-1, 1024, 4, 4]           2,048\n",
      "          Linear-370                  [-1, 128]         131,200\n",
      "        DenseNet-371                  [-1, 128]               0\n",
      "SpectrogramEncoderNet-372                  [-1, 128]               0\n",
      "          Conv2d-373           [-1, 64, 64, 64]           9,408\n",
      "     BatchNorm2d-374           [-1, 64, 64, 64]             128\n",
      "            ReLU-375           [-1, 64, 64, 64]               0\n",
      "       MaxPool2d-376           [-1, 64, 32, 32]               0\n",
      "     BatchNorm2d-377           [-1, 64, 32, 32]             128\n",
      "            ReLU-378           [-1, 64, 32, 32]               0\n",
      "          Conv2d-379          [-1, 128, 32, 32]           8,192\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     BatchNorm2d-380          [-1, 128, 32, 32]             256\n",
      "            ReLU-381          [-1, 128, 32, 32]               0\n",
      "          Conv2d-382           [-1, 32, 32, 32]          36,864\n",
      "     BatchNorm2d-383           [-1, 96, 32, 32]             192\n",
      "            ReLU-384           [-1, 96, 32, 32]               0\n",
      "          Conv2d-385          [-1, 128, 32, 32]          12,288\n",
      "     BatchNorm2d-386          [-1, 128, 32, 32]             256\n",
      "            ReLU-387          [-1, 128, 32, 32]               0\n",
      "          Conv2d-388           [-1, 32, 32, 32]          36,864\n",
      "     BatchNorm2d-389          [-1, 128, 32, 32]             256\n",
      "            ReLU-390          [-1, 128, 32, 32]               0\n",
      "          Conv2d-391          [-1, 128, 32, 32]          16,384\n",
      "     BatchNorm2d-392          [-1, 128, 32, 32]             256\n",
      "            ReLU-393          [-1, 128, 32, 32]               0\n",
      "          Conv2d-394           [-1, 32, 32, 32]          36,864\n",
      "     BatchNorm2d-395          [-1, 160, 32, 32]             320\n",
      "            ReLU-396          [-1, 160, 32, 32]               0\n",
      "          Conv2d-397          [-1, 128, 32, 32]          20,480\n",
      "     BatchNorm2d-398          [-1, 128, 32, 32]             256\n",
      "            ReLU-399          [-1, 128, 32, 32]               0\n",
      "          Conv2d-400           [-1, 32, 32, 32]          36,864\n",
      "     BatchNorm2d-401          [-1, 192, 32, 32]             384\n",
      "            ReLU-402          [-1, 192, 32, 32]               0\n",
      "          Conv2d-403          [-1, 128, 32, 32]          24,576\n",
      "     BatchNorm2d-404          [-1, 128, 32, 32]             256\n",
      "            ReLU-405          [-1, 128, 32, 32]               0\n",
      "          Conv2d-406           [-1, 32, 32, 32]          36,864\n",
      "     BatchNorm2d-407          [-1, 224, 32, 32]             448\n",
      "            ReLU-408          [-1, 224, 32, 32]               0\n",
      "          Conv2d-409          [-1, 128, 32, 32]          28,672\n",
      "     BatchNorm2d-410          [-1, 128, 32, 32]             256\n",
      "            ReLU-411          [-1, 128, 32, 32]               0\n",
      "          Conv2d-412           [-1, 32, 32, 32]          36,864\n",
      "     _DenseBlock-413          [-1, 256, 32, 32]               0\n",
      "     BatchNorm2d-414          [-1, 256, 32, 32]             512\n",
      "            ReLU-415          [-1, 256, 32, 32]               0\n",
      "          Conv2d-416          [-1, 128, 32, 32]          32,768\n",
      "       AvgPool2d-417          [-1, 128, 16, 16]               0\n",
      "     BatchNorm2d-418          [-1, 128, 16, 16]             256\n",
      "            ReLU-419          [-1, 128, 16, 16]               0\n",
      "          Conv2d-420          [-1, 128, 16, 16]          16,384\n",
      "     BatchNorm2d-421          [-1, 128, 16, 16]             256\n",
      "            ReLU-422          [-1, 128, 16, 16]               0\n",
      "          Conv2d-423           [-1, 32, 16, 16]          36,864\n",
      "     BatchNorm2d-424          [-1, 160, 16, 16]             320\n",
      "            ReLU-425          [-1, 160, 16, 16]               0\n",
      "          Conv2d-426          [-1, 128, 16, 16]          20,480\n",
      "     BatchNorm2d-427          [-1, 128, 16, 16]             256\n",
      "            ReLU-428          [-1, 128, 16, 16]               0\n",
      "          Conv2d-429           [-1, 32, 16, 16]          36,864\n",
      "     BatchNorm2d-430          [-1, 192, 16, 16]             384\n",
      "            ReLU-431          [-1, 192, 16, 16]               0\n",
      "          Conv2d-432          [-1, 128, 16, 16]          24,576\n",
      "     BatchNorm2d-433          [-1, 128, 16, 16]             256\n",
      "            ReLU-434          [-1, 128, 16, 16]               0\n",
      "          Conv2d-435           [-1, 32, 16, 16]          36,864\n",
      "     BatchNorm2d-436          [-1, 224, 16, 16]             448\n",
      "            ReLU-437          [-1, 224, 16, 16]               0\n",
      "          Conv2d-438          [-1, 128, 16, 16]          28,672\n",
      "     BatchNorm2d-439          [-1, 128, 16, 16]             256\n",
      "            ReLU-440          [-1, 128, 16, 16]               0\n",
      "          Conv2d-441           [-1, 32, 16, 16]          36,864\n",
      "     BatchNorm2d-442          [-1, 256, 16, 16]             512\n",
      "            ReLU-443          [-1, 256, 16, 16]               0\n",
      "          Conv2d-444          [-1, 128, 16, 16]          32,768\n",
      "     BatchNorm2d-445          [-1, 128, 16, 16]             256\n",
      "            ReLU-446          [-1, 128, 16, 16]               0\n",
      "          Conv2d-447           [-1, 32, 16, 16]          36,864\n",
      "     BatchNorm2d-448          [-1, 288, 16, 16]             576\n",
      "            ReLU-449          [-1, 288, 16, 16]               0\n",
      "          Conv2d-450          [-1, 128, 16, 16]          36,864\n",
      "     BatchNorm2d-451          [-1, 128, 16, 16]             256\n",
      "            ReLU-452          [-1, 128, 16, 16]               0\n",
      "          Conv2d-453           [-1, 32, 16, 16]          36,864\n",
      "     BatchNorm2d-454          [-1, 320, 16, 16]             640\n",
      "            ReLU-455          [-1, 320, 16, 16]               0\n",
      "          Conv2d-456          [-1, 128, 16, 16]          40,960\n",
      "     BatchNorm2d-457          [-1, 128, 16, 16]             256\n",
      "            ReLU-458          [-1, 128, 16, 16]               0\n",
      "          Conv2d-459           [-1, 32, 16, 16]          36,864\n",
      "     BatchNorm2d-460          [-1, 352, 16, 16]             704\n",
      "            ReLU-461          [-1, 352, 16, 16]               0\n",
      "          Conv2d-462          [-1, 128, 16, 16]          45,056\n",
      "     BatchNorm2d-463          [-1, 128, 16, 16]             256\n",
      "            ReLU-464          [-1, 128, 16, 16]               0\n",
      "          Conv2d-465           [-1, 32, 16, 16]          36,864\n",
      "     BatchNorm2d-466          [-1, 384, 16, 16]             768\n",
      "            ReLU-467          [-1, 384, 16, 16]               0\n",
      "          Conv2d-468          [-1, 128, 16, 16]          49,152\n",
      "     BatchNorm2d-469          [-1, 128, 16, 16]             256\n",
      "            ReLU-470          [-1, 128, 16, 16]               0\n",
      "          Conv2d-471           [-1, 32, 16, 16]          36,864\n",
      "     BatchNorm2d-472          [-1, 416, 16, 16]             832\n",
      "            ReLU-473          [-1, 416, 16, 16]               0\n",
      "          Conv2d-474          [-1, 128, 16, 16]          53,248\n",
      "     BatchNorm2d-475          [-1, 128, 16, 16]             256\n",
      "            ReLU-476          [-1, 128, 16, 16]               0\n",
      "          Conv2d-477           [-1, 32, 16, 16]          36,864\n",
      "     BatchNorm2d-478          [-1, 448, 16, 16]             896\n",
      "            ReLU-479          [-1, 448, 16, 16]               0\n",
      "          Conv2d-480          [-1, 128, 16, 16]          57,344\n",
      "     BatchNorm2d-481          [-1, 128, 16, 16]             256\n",
      "            ReLU-482          [-1, 128, 16, 16]               0\n",
      "          Conv2d-483           [-1, 32, 16, 16]          36,864\n",
      "     BatchNorm2d-484          [-1, 480, 16, 16]             960\n",
      "            ReLU-485          [-1, 480, 16, 16]               0\n",
      "          Conv2d-486          [-1, 128, 16, 16]          61,440\n",
      "     BatchNorm2d-487          [-1, 128, 16, 16]             256\n",
      "            ReLU-488          [-1, 128, 16, 16]               0\n",
      "          Conv2d-489           [-1, 32, 16, 16]          36,864\n",
      "     _DenseBlock-490          [-1, 512, 16, 16]               0\n",
      "     BatchNorm2d-491          [-1, 512, 16, 16]           1,024\n",
      "            ReLU-492          [-1, 512, 16, 16]               0\n",
      "          Conv2d-493          [-1, 256, 16, 16]         131,072\n",
      "       AvgPool2d-494            [-1, 256, 8, 8]               0\n",
      "     BatchNorm2d-495            [-1, 256, 8, 8]             512\n",
      "            ReLU-496            [-1, 256, 8, 8]               0\n",
      "          Conv2d-497            [-1, 128, 8, 8]          32,768\n",
      "     BatchNorm2d-498            [-1, 128, 8, 8]             256\n",
      "            ReLU-499            [-1, 128, 8, 8]               0\n",
      "          Conv2d-500             [-1, 32, 8, 8]          36,864\n",
      "     BatchNorm2d-501            [-1, 288, 8, 8]             576\n",
      "            ReLU-502            [-1, 288, 8, 8]               0\n",
      "          Conv2d-503            [-1, 128, 8, 8]          36,864\n",
      "     BatchNorm2d-504            [-1, 128, 8, 8]             256\n",
      "            ReLU-505            [-1, 128, 8, 8]               0\n",
      "          Conv2d-506             [-1, 32, 8, 8]          36,864\n",
      "     BatchNorm2d-507            [-1, 320, 8, 8]             640\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            ReLU-508            [-1, 320, 8, 8]               0\n",
      "          Conv2d-509            [-1, 128, 8, 8]          40,960\n",
      "     BatchNorm2d-510            [-1, 128, 8, 8]             256\n",
      "            ReLU-511            [-1, 128, 8, 8]               0\n",
      "          Conv2d-512             [-1, 32, 8, 8]          36,864\n",
      "     BatchNorm2d-513            [-1, 352, 8, 8]             704\n",
      "            ReLU-514            [-1, 352, 8, 8]               0\n",
      "          Conv2d-515            [-1, 128, 8, 8]          45,056\n",
      "     BatchNorm2d-516            [-1, 128, 8, 8]             256\n",
      "            ReLU-517            [-1, 128, 8, 8]               0\n",
      "          Conv2d-518             [-1, 32, 8, 8]          36,864\n",
      "     BatchNorm2d-519            [-1, 384, 8, 8]             768\n",
      "            ReLU-520            [-1, 384, 8, 8]               0\n",
      "          Conv2d-521            [-1, 128, 8, 8]          49,152\n",
      "     BatchNorm2d-522            [-1, 128, 8, 8]             256\n",
      "            ReLU-523            [-1, 128, 8, 8]               0\n",
      "          Conv2d-524             [-1, 32, 8, 8]          36,864\n",
      "     BatchNorm2d-525            [-1, 416, 8, 8]             832\n",
      "            ReLU-526            [-1, 416, 8, 8]               0\n",
      "          Conv2d-527            [-1, 128, 8, 8]          53,248\n",
      "     BatchNorm2d-528            [-1, 128, 8, 8]             256\n",
      "            ReLU-529            [-1, 128, 8, 8]               0\n",
      "          Conv2d-530             [-1, 32, 8, 8]          36,864\n",
      "     BatchNorm2d-531            [-1, 448, 8, 8]             896\n",
      "            ReLU-532            [-1, 448, 8, 8]               0\n",
      "          Conv2d-533            [-1, 128, 8, 8]          57,344\n",
      "     BatchNorm2d-534            [-1, 128, 8, 8]             256\n",
      "            ReLU-535            [-1, 128, 8, 8]               0\n",
      "          Conv2d-536             [-1, 32, 8, 8]          36,864\n",
      "     BatchNorm2d-537            [-1, 480, 8, 8]             960\n",
      "            ReLU-538            [-1, 480, 8, 8]               0\n",
      "          Conv2d-539            [-1, 128, 8, 8]          61,440\n",
      "     BatchNorm2d-540            [-1, 128, 8, 8]             256\n",
      "            ReLU-541            [-1, 128, 8, 8]               0\n",
      "          Conv2d-542             [-1, 32, 8, 8]          36,864\n",
      "     BatchNorm2d-543            [-1, 512, 8, 8]           1,024\n",
      "            ReLU-544            [-1, 512, 8, 8]               0\n",
      "          Conv2d-545            [-1, 128, 8, 8]          65,536\n",
      "     BatchNorm2d-546            [-1, 128, 8, 8]             256\n",
      "            ReLU-547            [-1, 128, 8, 8]               0\n",
      "          Conv2d-548             [-1, 32, 8, 8]          36,864\n",
      "     BatchNorm2d-549            [-1, 544, 8, 8]           1,088\n",
      "            ReLU-550            [-1, 544, 8, 8]               0\n",
      "          Conv2d-551            [-1, 128, 8, 8]          69,632\n",
      "     BatchNorm2d-552            [-1, 128, 8, 8]             256\n",
      "            ReLU-553            [-1, 128, 8, 8]               0\n",
      "          Conv2d-554             [-1, 32, 8, 8]          36,864\n",
      "     BatchNorm2d-555            [-1, 576, 8, 8]           1,152\n",
      "            ReLU-556            [-1, 576, 8, 8]               0\n",
      "          Conv2d-557            [-1, 128, 8, 8]          73,728\n",
      "     BatchNorm2d-558            [-1, 128, 8, 8]             256\n",
      "            ReLU-559            [-1, 128, 8, 8]               0\n",
      "          Conv2d-560             [-1, 32, 8, 8]          36,864\n",
      "     BatchNorm2d-561            [-1, 608, 8, 8]           1,216\n",
      "            ReLU-562            [-1, 608, 8, 8]               0\n",
      "          Conv2d-563            [-1, 128, 8, 8]          77,824\n",
      "     BatchNorm2d-564            [-1, 128, 8, 8]             256\n",
      "            ReLU-565            [-1, 128, 8, 8]               0\n",
      "          Conv2d-566             [-1, 32, 8, 8]          36,864\n",
      "     BatchNorm2d-567            [-1, 640, 8, 8]           1,280\n",
      "            ReLU-568            [-1, 640, 8, 8]               0\n",
      "          Conv2d-569            [-1, 128, 8, 8]          81,920\n",
      "     BatchNorm2d-570            [-1, 128, 8, 8]             256\n",
      "            ReLU-571            [-1, 128, 8, 8]               0\n",
      "          Conv2d-572             [-1, 32, 8, 8]          36,864\n",
      "     BatchNorm2d-573            [-1, 672, 8, 8]           1,344\n",
      "            ReLU-574            [-1, 672, 8, 8]               0\n",
      "          Conv2d-575            [-1, 128, 8, 8]          86,016\n",
      "     BatchNorm2d-576            [-1, 128, 8, 8]             256\n",
      "            ReLU-577            [-1, 128, 8, 8]               0\n",
      "          Conv2d-578             [-1, 32, 8, 8]          36,864\n",
      "     BatchNorm2d-579            [-1, 704, 8, 8]           1,408\n",
      "            ReLU-580            [-1, 704, 8, 8]               0\n",
      "          Conv2d-581            [-1, 128, 8, 8]          90,112\n",
      "     BatchNorm2d-582            [-1, 128, 8, 8]             256\n",
      "            ReLU-583            [-1, 128, 8, 8]               0\n",
      "          Conv2d-584             [-1, 32, 8, 8]          36,864\n",
      "     BatchNorm2d-585            [-1, 736, 8, 8]           1,472\n",
      "            ReLU-586            [-1, 736, 8, 8]               0\n",
      "          Conv2d-587            [-1, 128, 8, 8]          94,208\n",
      "     BatchNorm2d-588            [-1, 128, 8, 8]             256\n",
      "            ReLU-589            [-1, 128, 8, 8]               0\n",
      "          Conv2d-590             [-1, 32, 8, 8]          36,864\n",
      "     BatchNorm2d-591            [-1, 768, 8, 8]           1,536\n",
      "            ReLU-592            [-1, 768, 8, 8]               0\n",
      "          Conv2d-593            [-1, 128, 8, 8]          98,304\n",
      "     BatchNorm2d-594            [-1, 128, 8, 8]             256\n",
      "            ReLU-595            [-1, 128, 8, 8]               0\n",
      "          Conv2d-596             [-1, 32, 8, 8]          36,864\n",
      "     BatchNorm2d-597            [-1, 800, 8, 8]           1,600\n",
      "            ReLU-598            [-1, 800, 8, 8]               0\n",
      "          Conv2d-599            [-1, 128, 8, 8]         102,400\n",
      "     BatchNorm2d-600            [-1, 128, 8, 8]             256\n",
      "            ReLU-601            [-1, 128, 8, 8]               0\n",
      "          Conv2d-602             [-1, 32, 8, 8]          36,864\n",
      "     BatchNorm2d-603            [-1, 832, 8, 8]           1,664\n",
      "            ReLU-604            [-1, 832, 8, 8]               0\n",
      "          Conv2d-605            [-1, 128, 8, 8]         106,496\n",
      "     BatchNorm2d-606            [-1, 128, 8, 8]             256\n",
      "            ReLU-607            [-1, 128, 8, 8]               0\n",
      "          Conv2d-608             [-1, 32, 8, 8]          36,864\n",
      "     BatchNorm2d-609            [-1, 864, 8, 8]           1,728\n",
      "            ReLU-610            [-1, 864, 8, 8]               0\n",
      "          Conv2d-611            [-1, 128, 8, 8]         110,592\n",
      "     BatchNorm2d-612            [-1, 128, 8, 8]             256\n",
      "            ReLU-613            [-1, 128, 8, 8]               0\n",
      "          Conv2d-614             [-1, 32, 8, 8]          36,864\n",
      "     BatchNorm2d-615            [-1, 896, 8, 8]           1,792\n",
      "            ReLU-616            [-1, 896, 8, 8]               0\n",
      "          Conv2d-617            [-1, 128, 8, 8]         114,688\n",
      "     BatchNorm2d-618            [-1, 128, 8, 8]             256\n",
      "            ReLU-619            [-1, 128, 8, 8]               0\n",
      "          Conv2d-620             [-1, 32, 8, 8]          36,864\n",
      "     BatchNorm2d-621            [-1, 928, 8, 8]           1,856\n",
      "            ReLU-622            [-1, 928, 8, 8]               0\n",
      "          Conv2d-623            [-1, 128, 8, 8]         118,784\n",
      "     BatchNorm2d-624            [-1, 128, 8, 8]             256\n",
      "            ReLU-625            [-1, 128, 8, 8]               0\n",
      "          Conv2d-626             [-1, 32, 8, 8]          36,864\n",
      "     BatchNorm2d-627            [-1, 960, 8, 8]           1,920\n",
      "            ReLU-628            [-1, 960, 8, 8]               0\n",
      "          Conv2d-629            [-1, 128, 8, 8]         122,880\n",
      "     BatchNorm2d-630            [-1, 128, 8, 8]             256\n",
      "            ReLU-631            [-1, 128, 8, 8]               0\n",
      "          Conv2d-632             [-1, 32, 8, 8]          36,864\n",
      "     BatchNorm2d-633            [-1, 992, 8, 8]           1,984\n",
      "            ReLU-634            [-1, 992, 8, 8]               0\n",
      "          Conv2d-635            [-1, 128, 8, 8]         126,976\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     BatchNorm2d-636            [-1, 128, 8, 8]             256\n",
      "            ReLU-637            [-1, 128, 8, 8]               0\n",
      "          Conv2d-638             [-1, 32, 8, 8]          36,864\n",
      "     _DenseBlock-639           [-1, 1024, 8, 8]               0\n",
      "     BatchNorm2d-640           [-1, 1024, 8, 8]           2,048\n",
      "            ReLU-641           [-1, 1024, 8, 8]               0\n",
      "          Conv2d-642            [-1, 512, 8, 8]         524,288\n",
      "       AvgPool2d-643            [-1, 512, 4, 4]               0\n",
      "     BatchNorm2d-644            [-1, 512, 4, 4]           1,024\n",
      "            ReLU-645            [-1, 512, 4, 4]               0\n",
      "          Conv2d-646            [-1, 128, 4, 4]          65,536\n",
      "     BatchNorm2d-647            [-1, 128, 4, 4]             256\n",
      "            ReLU-648            [-1, 128, 4, 4]               0\n",
      "          Conv2d-649             [-1, 32, 4, 4]          36,864\n",
      "     BatchNorm2d-650            [-1, 544, 4, 4]           1,088\n",
      "            ReLU-651            [-1, 544, 4, 4]               0\n",
      "          Conv2d-652            [-1, 128, 4, 4]          69,632\n",
      "     BatchNorm2d-653            [-1, 128, 4, 4]             256\n",
      "            ReLU-654            [-1, 128, 4, 4]               0\n",
      "          Conv2d-655             [-1, 32, 4, 4]          36,864\n",
      "     BatchNorm2d-656            [-1, 576, 4, 4]           1,152\n",
      "            ReLU-657            [-1, 576, 4, 4]               0\n",
      "          Conv2d-658            [-1, 128, 4, 4]          73,728\n",
      "     BatchNorm2d-659            [-1, 128, 4, 4]             256\n",
      "            ReLU-660            [-1, 128, 4, 4]               0\n",
      "          Conv2d-661             [-1, 32, 4, 4]          36,864\n",
      "     BatchNorm2d-662            [-1, 608, 4, 4]           1,216\n",
      "            ReLU-663            [-1, 608, 4, 4]               0\n",
      "          Conv2d-664            [-1, 128, 4, 4]          77,824\n",
      "     BatchNorm2d-665            [-1, 128, 4, 4]             256\n",
      "            ReLU-666            [-1, 128, 4, 4]               0\n",
      "          Conv2d-667             [-1, 32, 4, 4]          36,864\n",
      "     BatchNorm2d-668            [-1, 640, 4, 4]           1,280\n",
      "            ReLU-669            [-1, 640, 4, 4]               0\n",
      "          Conv2d-670            [-1, 128, 4, 4]          81,920\n",
      "     BatchNorm2d-671            [-1, 128, 4, 4]             256\n",
      "            ReLU-672            [-1, 128, 4, 4]               0\n",
      "          Conv2d-673             [-1, 32, 4, 4]          36,864\n",
      "     BatchNorm2d-674            [-1, 672, 4, 4]           1,344\n",
      "            ReLU-675            [-1, 672, 4, 4]               0\n",
      "          Conv2d-676            [-1, 128, 4, 4]          86,016\n",
      "     BatchNorm2d-677            [-1, 128, 4, 4]             256\n",
      "            ReLU-678            [-1, 128, 4, 4]               0\n",
      "          Conv2d-679             [-1, 32, 4, 4]          36,864\n",
      "     BatchNorm2d-680            [-1, 704, 4, 4]           1,408\n",
      "            ReLU-681            [-1, 704, 4, 4]               0\n",
      "          Conv2d-682            [-1, 128, 4, 4]          90,112\n",
      "     BatchNorm2d-683            [-1, 128, 4, 4]             256\n",
      "            ReLU-684            [-1, 128, 4, 4]               0\n",
      "          Conv2d-685             [-1, 32, 4, 4]          36,864\n",
      "     BatchNorm2d-686            [-1, 736, 4, 4]           1,472\n",
      "            ReLU-687            [-1, 736, 4, 4]               0\n",
      "          Conv2d-688            [-1, 128, 4, 4]          94,208\n",
      "     BatchNorm2d-689            [-1, 128, 4, 4]             256\n",
      "            ReLU-690            [-1, 128, 4, 4]               0\n",
      "          Conv2d-691             [-1, 32, 4, 4]          36,864\n",
      "     BatchNorm2d-692            [-1, 768, 4, 4]           1,536\n",
      "            ReLU-693            [-1, 768, 4, 4]               0\n",
      "          Conv2d-694            [-1, 128, 4, 4]          98,304\n",
      "     BatchNorm2d-695            [-1, 128, 4, 4]             256\n",
      "            ReLU-696            [-1, 128, 4, 4]               0\n",
      "          Conv2d-697             [-1, 32, 4, 4]          36,864\n",
      "     BatchNorm2d-698            [-1, 800, 4, 4]           1,600\n",
      "            ReLU-699            [-1, 800, 4, 4]               0\n",
      "          Conv2d-700            [-1, 128, 4, 4]         102,400\n",
      "     BatchNorm2d-701            [-1, 128, 4, 4]             256\n",
      "            ReLU-702            [-1, 128, 4, 4]               0\n",
      "          Conv2d-703             [-1, 32, 4, 4]          36,864\n",
      "     BatchNorm2d-704            [-1, 832, 4, 4]           1,664\n",
      "            ReLU-705            [-1, 832, 4, 4]               0\n",
      "          Conv2d-706            [-1, 128, 4, 4]         106,496\n",
      "     BatchNorm2d-707            [-1, 128, 4, 4]             256\n",
      "            ReLU-708            [-1, 128, 4, 4]               0\n",
      "          Conv2d-709             [-1, 32, 4, 4]          36,864\n",
      "     BatchNorm2d-710            [-1, 864, 4, 4]           1,728\n",
      "            ReLU-711            [-1, 864, 4, 4]               0\n",
      "          Conv2d-712            [-1, 128, 4, 4]         110,592\n",
      "     BatchNorm2d-713            [-1, 128, 4, 4]             256\n",
      "            ReLU-714            [-1, 128, 4, 4]               0\n",
      "          Conv2d-715             [-1, 32, 4, 4]          36,864\n",
      "     BatchNorm2d-716            [-1, 896, 4, 4]           1,792\n",
      "            ReLU-717            [-1, 896, 4, 4]               0\n",
      "          Conv2d-718            [-1, 128, 4, 4]         114,688\n",
      "     BatchNorm2d-719            [-1, 128, 4, 4]             256\n",
      "            ReLU-720            [-1, 128, 4, 4]               0\n",
      "          Conv2d-721             [-1, 32, 4, 4]          36,864\n",
      "     BatchNorm2d-722            [-1, 928, 4, 4]           1,856\n",
      "            ReLU-723            [-1, 928, 4, 4]               0\n",
      "          Conv2d-724            [-1, 128, 4, 4]         118,784\n",
      "     BatchNorm2d-725            [-1, 128, 4, 4]             256\n",
      "            ReLU-726            [-1, 128, 4, 4]               0\n",
      "          Conv2d-727             [-1, 32, 4, 4]          36,864\n",
      "     BatchNorm2d-728            [-1, 960, 4, 4]           1,920\n",
      "            ReLU-729            [-1, 960, 4, 4]               0\n",
      "          Conv2d-730            [-1, 128, 4, 4]         122,880\n",
      "     BatchNorm2d-731            [-1, 128, 4, 4]             256\n",
      "            ReLU-732            [-1, 128, 4, 4]               0\n",
      "          Conv2d-733             [-1, 32, 4, 4]          36,864\n",
      "     BatchNorm2d-734            [-1, 992, 4, 4]           1,984\n",
      "            ReLU-735            [-1, 992, 4, 4]               0\n",
      "          Conv2d-736            [-1, 128, 4, 4]         126,976\n",
      "     BatchNorm2d-737            [-1, 128, 4, 4]             256\n",
      "            ReLU-738            [-1, 128, 4, 4]               0\n",
      "          Conv2d-739             [-1, 32, 4, 4]          36,864\n",
      "     _DenseBlock-740           [-1, 1024, 4, 4]               0\n",
      "     BatchNorm2d-741           [-1, 1024, 4, 4]           2,048\n",
      "          Linear-742                  [-1, 128]         131,200\n",
      "        DenseNet-743                  [-1, 128]               0\n",
      "SpectrogramEncoderNet-744                  [-1, 128]               0\n",
      "            Tanh-745                  [-1, 128]               0\n",
      "          Linear-746                  [-1, 128]          16,512\n",
      "            ReLU-747                  [-1, 128]               0\n",
      "          Linear-748                  [-1, 128]          16,512\n",
      "            ReLU-749                  [-1, 128]               0\n",
      "          Linear-750                  [-1, 128]          16,512\n",
      "            ReLU-751                  [-1, 128]               0\n",
      "          Linear-752                  [-1, 128]          16,512\n",
      "            ReLU-753                  [-1, 128]               0\n",
      "          Linear-754                    [-1, 2]             258\n",
      "================================================================\n",
      "Total params: 14,236,418\n",
      "Trainable params: 66,306\n",
      "Non-trainable params: 14,170,112\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.38\n",
      "Forward/backward pass size (MB): 199.39\n",
      "Params size (MB): 54.31\n",
      "Estimated Total Size (MB): 254.07\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Verification Binary classifier\n",
    "summary(VerificationBinaryClassifierNet(encoder_model), input_size=(2, 3, IMG_HEIGHT, IMG_HEIGHT), device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Training data\n",
    "training_folder = os.path.join(output_data_folder, \"training_dataset_full_spectrogram/vox1_dev_wav\")\n",
    "spectrogram_samples_files = [os.path.join(training_folder, file) for file in os.listdir(training_folder)]\n",
    "batch_size = 160\n",
    "num_batches = 2000 // batch_size\n",
    "num_sub_samples = 200\n",
    "training_data_generator = VerificationDataGenerator(spectrogram_samples_files, batch_size, num_batches, num_sub_samples, IMG_HEIGHT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Validation data\n",
    "validation_set_file = os.path.join(output_data_folder, \"validation_sets\", \"verification_validation_set.pickle\")\n",
    "with open(validation_set_file, 'rb') as f:\n",
    "    validation_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, num_epochs, training_data_generator, validation_data, log_handler):\n",
    "    since = time.time()\n",
    "    best_acc = 0.0\n",
    "    count_more_than_70 = 0\n",
    "    count_more_than_80 = 0\n",
    "\n",
    "    t1 = time.time()\n",
    "    for epoch in range(num_epochs):\n",
    "        log_handler.print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        log_handler.print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            batches_used = 0\n",
    "            data_generator = training_data_generator.generate_batches() if phase == 'train' else validation_data\n",
    "            for data in data_generator:\n",
    "                batches_used += 1\n",
    "                input_imgs, labels = data\n",
    "                inputs = [img.to(device) for img in input_imgs]\n",
    "                labels = labels.to(device)\n",
    "                \n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):   # gradient only for train\n",
    "                    outputs = model(inputs)   \n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs[0].size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "                \n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "            \n",
    "            epoch_loss = running_loss / (batches_used * inputs[0].size(0))\n",
    "            epoch_acc = running_corrects.double() / (batches_used * inputs[0].size(0))\n",
    "            log_handler.print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                log_handler.save_pytorch_model(model, \"best_model_{}.pt\".format(model.__class__.__name__))\n",
    "                example = [torch.rand(1, 3, IMG_HEIGHT, IMG_HEIGHT), torch.rand(1, 3, IMG_HEIGHT, IMG_HEIGHT)]\n",
    "                log_handler.save_pytorch_model_as_torchscript(model, \"mobile_model.pt\", (example,))\n",
    "            # Track val acc >= 70%\n",
    "            if phase == 'val' and epoch_acc >= 0.70: count_more_than_70 += 1\n",
    "            if phase == 'val' and epoch_acc >= 0.80: count_more_than_80 += 1\n",
    "\n",
    "        # end of epoch\n",
    "        log_handler.print(\"Time taken is {} seconds\".format(int(time.time()-t1)))\n",
    "        t1 = time.time()\n",
    "        log_handler.print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    log_handler.print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    log_handler.print('Best val Acc: {:4f}'.format(best_acc))\n",
    "    log_handler.print('Val Acc >= 0.70: {}'.format(count_more_than_70))\n",
    "    log_handler.print('Val Acc >= 0.80: {}'.format(count_more_than_80))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Desktop\\projects\\speaker_recognition_voxceleb1\\output_data\\models\\verification_classifier\\2020-04-07_10-18-50\n",
      "Encoder: D:\\Desktop\\projects\\speaker_recognition_voxceleb1\\output_data\\models\\contrastive_encoder\\good_models\\2020-04-06_19-42-12\n",
      "Epoch 0/69\n",
      "----------\n",
      "train Loss: 0.6908 Acc: 0.5000\n",
      "val Loss: 0.6872 Acc: 0.5000\n",
      "MODEL SAVED\n",
      "MODEL SAVED (MOBILE)\n",
      "Time taken is 118 seconds\n",
      "\n",
      "Epoch 1/69\n",
      "----------\n",
      "train Loss: 0.6835 Acc: 0.5000\n",
      "val Loss: 0.6789 Acc: 0.5000\n",
      "Time taken is 106 seconds\n",
      "\n",
      "Epoch 2/69\n",
      "----------\n",
      "train Loss: 0.6734 Acc: 0.5000\n",
      "val Loss: 0.6656 Acc: 0.5000\n",
      "Time taken is 114 seconds\n",
      "\n",
      "Epoch 3/69\n",
      "----------\n",
      "train Loss: 0.6548 Acc: 0.5047\n",
      "val Loss: 0.6440 Acc: 0.5266\n",
      "MODEL SAVED\n",
      "MODEL SAVED (MOBILE)\n",
      "Time taken is 115 seconds\n",
      "\n",
      "Epoch 4/69\n",
      "----------\n",
      "train Loss: 0.6304 Acc: 0.5411\n",
      "val Loss: 0.6140 Acc: 0.5935\n",
      "MODEL SAVED\n",
      "MODEL SAVED (MOBILE)\n",
      "Time taken is 110 seconds\n",
      "\n",
      "Epoch 5/69\n",
      "----------\n",
      "train Loss: 0.5991 Acc: 0.6193\n",
      "val Loss: 0.5768 Acc: 0.7000\n",
      "MODEL SAVED\n",
      "MODEL SAVED (MOBILE)\n",
      "Time taken is 109 seconds\n",
      "\n",
      "Epoch 6/69\n",
      "----------\n",
      "train Loss: 0.5637 Acc: 0.7154\n",
      "val Loss: 0.5377 Acc: 0.7682\n",
      "MODEL SAVED\n",
      "MODEL SAVED (MOBILE)\n",
      "Time taken is 107 seconds\n",
      "\n",
      "Epoch 7/69\n",
      "----------\n",
      "train Loss: 0.5230 Acc: 0.7812\n",
      "val Loss: 0.5030 Acc: 0.7852\n",
      "MODEL SAVED\n",
      "MODEL SAVED (MOBILE)\n",
      "Time taken is 107 seconds\n",
      "\n",
      "Epoch 8/69\n",
      "----------\n",
      "train Loss: 0.4921 Acc: 0.7891\n",
      "val Loss: 0.4753 Acc: 0.7922\n",
      "MODEL SAVED\n",
      "MODEL SAVED (MOBILE)\n",
      "Time taken is 112 seconds\n",
      "\n",
      "Epoch 9/69\n",
      "----------\n",
      "train Loss: 0.4748 Acc: 0.7924\n",
      "val Loss: 0.4519 Acc: 0.7977\n",
      "MODEL SAVED\n",
      "MODEL SAVED (MOBILE)\n",
      "Time taken is 143 seconds\n",
      "\n",
      "Epoch 10/69\n",
      "----------\n",
      "train Loss: 0.4406 Acc: 0.8130\n",
      "val Loss: 0.4299 Acc: 0.8005\n",
      "MODEL SAVED\n",
      "MODEL SAVED (MOBILE)\n",
      "Time taken is 117 seconds\n",
      "\n",
      "Epoch 11/69\n",
      "----------\n",
      "train Loss: 0.4110 Acc: 0.8193\n",
      "val Loss: 0.4144 Acc: 0.8057\n",
      "MODEL SAVED\n",
      "MODEL SAVED (MOBILE)\n",
      "Time taken is 115 seconds\n",
      "\n",
      "Epoch 12/69\n",
      "----------\n",
      "train Loss: 0.4118 Acc: 0.8161\n",
      "val Loss: 0.4073 Acc: 0.8120\n",
      "MODEL SAVED\n",
      "MODEL SAVED (MOBILE)\n",
      "Time taken is 104 seconds\n",
      "\n",
      "Epoch 13/69\n",
      "----------\n",
      "train Loss: 0.4051 Acc: 0.8185\n",
      "val Loss: 0.4008 Acc: 0.8117\n",
      "Time taken is 104 seconds\n",
      "\n",
      "Epoch 14/69\n",
      "----------\n",
      "train Loss: 0.4068 Acc: 0.8065\n",
      "val Loss: 0.4004 Acc: 0.8130\n",
      "MODEL SAVED\n",
      "MODEL SAVED (MOBILE)\n",
      "Time taken is 134 seconds\n",
      "\n",
      "Epoch 15/69\n",
      "----------\n",
      "train Loss: 0.3910 Acc: 0.8263\n",
      "val Loss: 0.3975 Acc: 0.8091\n",
      "Time taken is 102 seconds\n",
      "\n",
      "Epoch 16/69\n",
      "----------\n",
      "train Loss: 0.3986 Acc: 0.8107\n",
      "val Loss: 0.3962 Acc: 0.8180\n",
      "MODEL SAVED\n",
      "MODEL SAVED (MOBILE)\n",
      "Time taken is 108 seconds\n",
      "\n",
      "Epoch 17/69\n",
      "----------\n",
      "train Loss: 0.3894 Acc: 0.8234\n",
      "val Loss: 0.3957 Acc: 0.8094\n",
      "Time taken is 126 seconds\n",
      "\n",
      "Epoch 18/69\n",
      "----------\n",
      "train Loss: 0.3914 Acc: 0.8237\n",
      "val Loss: 0.3955 Acc: 0.8159\n",
      "Time taken is 101 seconds\n",
      "\n",
      "Epoch 19/69\n",
      "----------\n",
      "train Loss: 0.3957 Acc: 0.8211\n",
      "val Loss: 0.3916 Acc: 0.8135\n",
      "Time taken is 109 seconds\n",
      "\n",
      "Epoch 20/69\n",
      "----------\n",
      "train Loss: 0.3940 Acc: 0.8289\n",
      "val Loss: 0.3916 Acc: 0.8154\n",
      "Time taken is 112 seconds\n",
      "\n",
      "Epoch 21/69\n",
      "----------\n",
      "train Loss: 0.3975 Acc: 0.8193\n",
      "val Loss: 0.3964 Acc: 0.8115\n",
      "Time taken is 107 seconds\n",
      "\n",
      "Epoch 22/69\n",
      "----------\n",
      "train Loss: 0.3827 Acc: 0.8247\n",
      "val Loss: 0.4027 Acc: 0.8182\n",
      "MODEL SAVED\n",
      "MODEL SAVED (MOBILE)\n",
      "Time taken is 115 seconds\n",
      "\n",
      "Epoch 23/69\n",
      "----------\n",
      "train Loss: 0.3984 Acc: 0.8138\n",
      "val Loss: 0.4065 Acc: 0.8023\n",
      "Time taken is 112 seconds\n",
      "\n",
      "Epoch 24/69\n",
      "----------\n",
      "train Loss: 0.3893 Acc: 0.8193\n",
      "val Loss: 0.4040 Acc: 0.8177\n",
      "Time taken is 104 seconds\n",
      "\n",
      "Epoch 25/69\n",
      "----------\n",
      "train Loss: 0.3755 Acc: 0.8346\n",
      "val Loss: 0.3931 Acc: 0.8130\n",
      "Time taken is 133 seconds\n",
      "\n",
      "Epoch 26/69\n",
      "----------\n",
      "train Loss: 0.4060 Acc: 0.8190\n",
      "val Loss: 0.4006 Acc: 0.8076\n",
      "Time taken is 101 seconds\n",
      "\n",
      "Epoch 27/69\n",
      "----------\n",
      "train Loss: 0.3854 Acc: 0.8240\n",
      "val Loss: 0.3893 Acc: 0.8117\n",
      "Time taken is 114 seconds\n",
      "\n",
      "Epoch 28/69\n",
      "----------\n",
      "train Loss: 0.4063 Acc: 0.8096\n",
      "val Loss: 0.3903 Acc: 0.8172\n",
      "Time taken is 101 seconds\n",
      "\n",
      "Epoch 29/69\n",
      "----------\n",
      "train Loss: 0.3823 Acc: 0.8227\n",
      "val Loss: 0.3900 Acc: 0.8190\n",
      "MODEL SAVED\n",
      "MODEL SAVED (MOBILE)\n",
      "Time taken is 110 seconds\n",
      "\n",
      "Epoch 30/69\n",
      "----------\n",
      "train Loss: 0.3920 Acc: 0.8234\n",
      "val Loss: 0.3962 Acc: 0.8125\n",
      "Time taken is 119 seconds\n",
      "\n",
      "Epoch 31/69\n",
      "----------\n",
      "train Loss: 0.4023 Acc: 0.8102\n",
      "val Loss: 0.4066 Acc: 0.8164\n",
      "Time taken is 105 seconds\n",
      "\n",
      "Epoch 32/69\n",
      "----------\n",
      "train Loss: 0.4005 Acc: 0.8198\n",
      "val Loss: 0.3923 Acc: 0.8198\n",
      "MODEL SAVED\n",
      "MODEL SAVED (MOBILE)\n",
      "Time taken is 125 seconds\n",
      "\n",
      "Epoch 33/69\n",
      "----------\n",
      "train Loss: 0.3890 Acc: 0.8198\n",
      "val Loss: 0.3889 Acc: 0.8125\n",
      "Time taken is 104 seconds\n",
      "\n",
      "Epoch 34/69\n",
      "----------\n",
      "train Loss: 0.3866 Acc: 0.8247\n",
      "val Loss: 0.3939 Acc: 0.8109\n",
      "Time taken is 106 seconds\n",
      "\n",
      "Epoch 35/69\n",
      "----------\n",
      "train Loss: 0.4015 Acc: 0.8174\n",
      "val Loss: 0.3904 Acc: 0.8146\n",
      "Time taken is 114 seconds\n",
      "\n",
      "Epoch 36/69\n",
      "----------\n",
      "train Loss: 0.3992 Acc: 0.8193\n",
      "val Loss: 0.3879 Acc: 0.8161\n",
      "Time taken is 104 seconds\n",
      "\n",
      "Epoch 37/69\n",
      "----------\n",
      "train Loss: 0.3609 Acc: 0.8427\n",
      "val Loss: 0.3894 Acc: 0.8167\n",
      "Time taken is 127 seconds\n",
      "\n",
      "Epoch 38/69\n",
      "----------\n",
      "train Loss: 0.3893 Acc: 0.8190\n",
      "val Loss: 0.3881 Acc: 0.8182\n",
      "Time taken is 101 seconds\n",
      "\n",
      "Epoch 39/69\n",
      "----------\n",
      "train Loss: 0.3831 Acc: 0.8305\n",
      "val Loss: 0.3896 Acc: 0.8182\n",
      "Time taken is 105 seconds\n",
      "\n",
      "Epoch 40/69\n",
      "----------\n",
      "train Loss: 0.3733 Acc: 0.8339\n",
      "val Loss: 0.3931 Acc: 0.8135\n",
      "Time taken is 108 seconds\n",
      "\n",
      "Epoch 41/69\n",
      "----------\n",
      "train Loss: 0.3673 Acc: 0.8312\n",
      "val Loss: 0.3887 Acc: 0.8193\n",
      "Time taken is 109 seconds\n",
      "\n",
      "Epoch 42/69\n",
      "----------\n",
      "train Loss: 0.3761 Acc: 0.8354\n",
      "val Loss: 0.4119 Acc: 0.8159\n",
      "Time taken is 129 seconds\n",
      "\n",
      "Epoch 43/69\n",
      "----------\n",
      "train Loss: 0.3847 Acc: 0.8359\n",
      "val Loss: 0.3883 Acc: 0.8195\n",
      "Time taken is 105 seconds\n",
      "\n",
      "Epoch 44/69\n",
      "----------\n",
      "train Loss: 0.3891 Acc: 0.8211\n",
      "val Loss: 0.3882 Acc: 0.8198\n",
      "Time taken is 102 seconds\n",
      "\n",
      "Epoch 45/69\n",
      "----------\n",
      "train Loss: 0.3827 Acc: 0.8333\n",
      "val Loss: 0.3899 Acc: 0.8177\n",
      "Time taken is 107 seconds\n",
      "\n",
      "Epoch 46/69\n",
      "----------\n",
      "train Loss: 0.3891 Acc: 0.8216\n",
      "val Loss: 0.4051 Acc: 0.8174\n",
      "Time taken is 104 seconds\n",
      "\n",
      "Epoch 47/69\n",
      "----------\n",
      "train Loss: 0.3794 Acc: 0.8253\n",
      "val Loss: 0.3852 Acc: 0.8193\n",
      "Time taken is 131 seconds\n",
      "\n",
      "Epoch 48/69\n",
      "----------\n",
      "train Loss: 0.3755 Acc: 0.8315\n",
      "val Loss: 0.3854 Acc: 0.8206\n",
      "MODEL SAVED\n",
      "MODEL SAVED (MOBILE)\n",
      "Time taken is 114 seconds\n",
      "\n",
      "Epoch 49/69\n",
      "----------\n",
      "train Loss: 0.3870 Acc: 0.8266\n",
      "val Loss: 0.3894 Acc: 0.8182\n",
      "Time taken is 104 seconds\n",
      "\n",
      "Epoch 50/69\n",
      "----------\n",
      "train Loss: 0.3696 Acc: 0.8323\n",
      "val Loss: 0.3884 Acc: 0.8174\n",
      "Time taken is 117 seconds\n",
      "\n",
      "Epoch 51/69\n",
      "----------\n",
      "train Loss: 0.3688 Acc: 0.8315\n",
      "val Loss: 0.3886 Acc: 0.8206\n",
      "Time taken is 104 seconds\n",
      "\n",
      "Epoch 52/69\n",
      "----------\n",
      "train Loss: 0.3749 Acc: 0.8320\n",
      "val Loss: 0.3869 Acc: 0.8214\n",
      "MODEL SAVED\n",
      "MODEL SAVED (MOBILE)\n",
      "Time taken is 133 seconds\n",
      "\n",
      "Epoch 53/69\n",
      "----------\n",
      "train Loss: 0.3817 Acc: 0.8271\n",
      "val Loss: 0.3907 Acc: 0.8146\n",
      "Time taken is 108 seconds\n",
      "\n",
      "Epoch 54/69\n",
      "----------\n",
      "train Loss: 0.4067 Acc: 0.8128\n",
      "val Loss: 0.4020 Acc: 0.8138\n",
      "Time taken is 102 seconds\n",
      "\n",
      "Epoch 55/69\n",
      "----------\n",
      "train Loss: 0.3878 Acc: 0.8229\n",
      "val Loss: 0.3846 Acc: 0.8203\n",
      "Time taken is 114 seconds\n",
      "\n",
      "Epoch 56/69\n",
      "----------\n",
      "train Loss: 0.3973 Acc: 0.8177\n",
      "val Loss: 0.3846 Acc: 0.8201\n",
      "Time taken is 106 seconds\n",
      "\n",
      "Epoch 57/69\n",
      "----------\n",
      "train Loss: 0.3740 Acc: 0.8307\n",
      "val Loss: 0.3880 Acc: 0.8203\n",
      "Time taken is 132 seconds\n",
      "\n",
      "Epoch 58/69\n",
      "----------\n",
      "train Loss: 0.3851 Acc: 0.8237\n",
      "val Loss: 0.3845 Acc: 0.8219\n",
      "MODEL SAVED\n",
      "MODEL SAVED (MOBILE)\n",
      "Time taken is 107 seconds\n",
      "\n",
      "Epoch 59/69\n",
      "----------\n",
      "train Loss: 0.3789 Acc: 0.8260\n",
      "val Loss: 0.3865 Acc: 0.8195\n",
      "Time taken is 103 seconds\n",
      "\n",
      "Epoch 60/69\n",
      "----------\n",
      "train Loss: 0.3939 Acc: 0.8156\n",
      "val Loss: 0.3836 Acc: 0.8211\n",
      "Time taken is 115 seconds\n",
      "\n",
      "Epoch 61/69\n",
      "----------\n",
      "train Loss: 0.3772 Acc: 0.8328\n",
      "val Loss: 0.3899 Acc: 0.8174\n",
      "Time taken is 102 seconds\n",
      "\n",
      "Epoch 62/69\n",
      "----------\n",
      "train Loss: 0.3760 Acc: 0.8315\n",
      "val Loss: 0.3858 Acc: 0.8214\n",
      "Time taken is 130 seconds\n",
      "\n",
      "Epoch 63/69\n",
      "----------\n",
      "train Loss: 0.4046 Acc: 0.8130\n",
      "val Loss: 0.3857 Acc: 0.8195\n",
      "Time taken is 101 seconds\n",
      "\n",
      "Epoch 64/69\n",
      "----------\n",
      "train Loss: 0.3815 Acc: 0.8271\n",
      "val Loss: 0.4124 Acc: 0.8099\n",
      "Time taken is 108 seconds\n",
      "\n",
      "Epoch 65/69\n",
      "----------\n",
      "train Loss: 0.4013 Acc: 0.8216\n",
      "val Loss: 0.3838 Acc: 0.8240\n",
      "MODEL SAVED\n",
      "MODEL SAVED (MOBILE)\n",
      "Time taken is 117 seconds\n",
      "\n",
      "Epoch 66/69\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.3987 Acc: 0.8138\n",
      "val Loss: 0.3836 Acc: 0.8229\n",
      "Time taken is 103 seconds\n",
      "\n",
      "Epoch 67/69\n",
      "----------\n",
      "train Loss: 0.3910 Acc: 0.8219\n",
      "val Loss: 0.3962 Acc: 0.8211\n",
      "Time taken is 102 seconds\n",
      "\n",
      "Epoch 68/69\n",
      "----------\n",
      "train Loss: 0.3729 Acc: 0.8315\n",
      "val Loss: 0.3807 Acc: 0.8234\n",
      "Time taken is 97 seconds\n",
      "\n",
      "Epoch 69/69\n",
      "----------\n",
      "train Loss: 0.3810 Acc: 0.8279\n",
      "val Loss: 0.3957 Acc: 0.8169\n",
      "Time taken is 92 seconds\n",
      "\n",
      "Training complete in 130m 21s\n",
      "Best val Acc: 0.823958\n",
      "Val Acc >= 0.70: 65\n",
      "Val Acc >= 0.80: 60\n"
     ]
    }
   ],
   "source": [
    "### Train\n",
    "\n",
    "epochs = 70\n",
    "# epochs = 50\n",
    "\n",
    "model_ft = VerificationBinaryClassifierNet(encoder_model).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer_ft = optim.Adam(model_ft.parameters(), lr = 0.0001)\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "# learning_rate_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=10, gamma=0.1)\n",
    "learning_rate_scheduler = lr_scheduler.CyclicLR(optimizer_ft, base_lr=0.0001, max_lr=0.01, cycle_momentum=False)   # 0.01 seems better\n",
    "\n",
    "### Train \n",
    "\n",
    "# Logger\n",
    "model_save_folder = os.path.join(models_folder, \"verification_classifier\")\n",
    "log_handler = ModelSaveAndLogHandler(model_save_folder, enable_model_saving=True, enable_logging=True)   # init\n",
    "model_def_src_file_path = os.path.join(r\"D:\\Desktop\\projects\\speaker_recognition_voxceleb1\\scripts\", \"model_definitions.py\")\n",
    "log_handler.save_model_definition_file(model_def_src_file_path)   # copy model def file\n",
    "print(log_handler.folder)\n",
    "\n",
    "# Description\n",
    "log_handler.print(\"Encoder: {}\".format(encoder_model_folder))\n",
    "\n",
    "# Train\n",
    "train_model(model_ft, criterion, optimizer_ft, learning_rate_scheduler, epochs, training_data_generator, validation_data, log_handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# log_handler.print(\"Encoder weights not frozen\")\n",
    "random_acc = 1 / 2\n",
    "random_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: Overall\n",
    "* **Change spectrogram size?**\n",
    "* **Contrastive classifier**\n",
    "    * Implement intraclass variance reduction\n",
    "* **Verification binary classifier**\n",
    "    * Implement EER metric\n",
    "    * Build 2nd validation set? (accent dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
