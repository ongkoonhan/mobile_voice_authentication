{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import copy\n",
    "import random\n",
    "import numpy as np\n",
    "from skimage.color import gray2rgb\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torchvision import models\n",
    "from torchsummary import summary\n",
    "\n",
    "from config import models_folder, output_data_folder\n",
    "from config import n_mels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_HEIGHT = n_mels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 32, 64, 64]             864\n",
      "       BatchNorm2d-2           [-1, 32, 64, 64]              64\n",
      "             ReLU6-3           [-1, 32, 64, 64]               0\n",
      "            Conv2d-4           [-1, 32, 64, 64]             288\n",
      "       BatchNorm2d-5           [-1, 32, 64, 64]              64\n",
      "             ReLU6-6           [-1, 32, 64, 64]               0\n",
      "            Conv2d-7           [-1, 16, 64, 64]             512\n",
      "       BatchNorm2d-8           [-1, 16, 64, 64]              32\n",
      "  InvertedResidual-9           [-1, 16, 64, 64]               0\n",
      "           Conv2d-10           [-1, 96, 64, 64]           1,536\n",
      "      BatchNorm2d-11           [-1, 96, 64, 64]             192\n",
      "            ReLU6-12           [-1, 96, 64, 64]               0\n",
      "           Conv2d-13           [-1, 96, 32, 32]             864\n",
      "      BatchNorm2d-14           [-1, 96, 32, 32]             192\n",
      "            ReLU6-15           [-1, 96, 32, 32]               0\n",
      "           Conv2d-16           [-1, 24, 32, 32]           2,304\n",
      "      BatchNorm2d-17           [-1, 24, 32, 32]              48\n",
      " InvertedResidual-18           [-1, 24, 32, 32]               0\n",
      "           Conv2d-19          [-1, 144, 32, 32]           3,456\n",
      "      BatchNorm2d-20          [-1, 144, 32, 32]             288\n",
      "            ReLU6-21          [-1, 144, 32, 32]               0\n",
      "           Conv2d-22          [-1, 144, 32, 32]           1,296\n",
      "      BatchNorm2d-23          [-1, 144, 32, 32]             288\n",
      "            ReLU6-24          [-1, 144, 32, 32]               0\n",
      "           Conv2d-25           [-1, 24, 32, 32]           3,456\n",
      "      BatchNorm2d-26           [-1, 24, 32, 32]              48\n",
      " InvertedResidual-27           [-1, 24, 32, 32]               0\n",
      "           Conv2d-28          [-1, 144, 32, 32]           3,456\n",
      "      BatchNorm2d-29          [-1, 144, 32, 32]             288\n",
      "            ReLU6-30          [-1, 144, 32, 32]               0\n",
      "           Conv2d-31          [-1, 144, 16, 16]           1,296\n",
      "      BatchNorm2d-32          [-1, 144, 16, 16]             288\n",
      "            ReLU6-33          [-1, 144, 16, 16]               0\n",
      "           Conv2d-34           [-1, 32, 16, 16]           4,608\n",
      "      BatchNorm2d-35           [-1, 32, 16, 16]              64\n",
      " InvertedResidual-36           [-1, 32, 16, 16]               0\n",
      "           Conv2d-37          [-1, 192, 16, 16]           6,144\n",
      "      BatchNorm2d-38          [-1, 192, 16, 16]             384\n",
      "            ReLU6-39          [-1, 192, 16, 16]               0\n",
      "           Conv2d-40          [-1, 192, 16, 16]           1,728\n",
      "      BatchNorm2d-41          [-1, 192, 16, 16]             384\n",
      "            ReLU6-42          [-1, 192, 16, 16]               0\n",
      "           Conv2d-43           [-1, 32, 16, 16]           6,144\n",
      "      BatchNorm2d-44           [-1, 32, 16, 16]              64\n",
      " InvertedResidual-45           [-1, 32, 16, 16]               0\n",
      "           Conv2d-46          [-1, 192, 16, 16]           6,144\n",
      "      BatchNorm2d-47          [-1, 192, 16, 16]             384\n",
      "            ReLU6-48          [-1, 192, 16, 16]               0\n",
      "           Conv2d-49          [-1, 192, 16, 16]           1,728\n",
      "      BatchNorm2d-50          [-1, 192, 16, 16]             384\n",
      "            ReLU6-51          [-1, 192, 16, 16]               0\n",
      "           Conv2d-52           [-1, 32, 16, 16]           6,144\n",
      "      BatchNorm2d-53           [-1, 32, 16, 16]              64\n",
      " InvertedResidual-54           [-1, 32, 16, 16]               0\n",
      "           Conv2d-55          [-1, 192, 16, 16]           6,144\n",
      "      BatchNorm2d-56          [-1, 192, 16, 16]             384\n",
      "            ReLU6-57          [-1, 192, 16, 16]               0\n",
      "           Conv2d-58            [-1, 192, 8, 8]           1,728\n",
      "      BatchNorm2d-59            [-1, 192, 8, 8]             384\n",
      "            ReLU6-60            [-1, 192, 8, 8]               0\n",
      "           Conv2d-61             [-1, 64, 8, 8]          12,288\n",
      "      BatchNorm2d-62             [-1, 64, 8, 8]             128\n",
      " InvertedResidual-63             [-1, 64, 8, 8]               0\n",
      "           Conv2d-64            [-1, 384, 8, 8]          24,576\n",
      "      BatchNorm2d-65            [-1, 384, 8, 8]             768\n",
      "            ReLU6-66            [-1, 384, 8, 8]               0\n",
      "           Conv2d-67            [-1, 384, 8, 8]           3,456\n",
      "      BatchNorm2d-68            [-1, 384, 8, 8]             768\n",
      "            ReLU6-69            [-1, 384, 8, 8]               0\n",
      "           Conv2d-70             [-1, 64, 8, 8]          24,576\n",
      "      BatchNorm2d-71             [-1, 64, 8, 8]             128\n",
      " InvertedResidual-72             [-1, 64, 8, 8]               0\n",
      "           Conv2d-73            [-1, 384, 8, 8]          24,576\n",
      "      BatchNorm2d-74            [-1, 384, 8, 8]             768\n",
      "            ReLU6-75            [-1, 384, 8, 8]               0\n",
      "           Conv2d-76            [-1, 384, 8, 8]           3,456\n",
      "      BatchNorm2d-77            [-1, 384, 8, 8]             768\n",
      "            ReLU6-78            [-1, 384, 8, 8]               0\n",
      "           Conv2d-79             [-1, 64, 8, 8]          24,576\n",
      "      BatchNorm2d-80             [-1, 64, 8, 8]             128\n",
      " InvertedResidual-81             [-1, 64, 8, 8]               0\n",
      "           Conv2d-82            [-1, 384, 8, 8]          24,576\n",
      "      BatchNorm2d-83            [-1, 384, 8, 8]             768\n",
      "            ReLU6-84            [-1, 384, 8, 8]               0\n",
      "           Conv2d-85            [-1, 384, 8, 8]           3,456\n",
      "      BatchNorm2d-86            [-1, 384, 8, 8]             768\n",
      "            ReLU6-87            [-1, 384, 8, 8]               0\n",
      "           Conv2d-88             [-1, 64, 8, 8]          24,576\n",
      "      BatchNorm2d-89             [-1, 64, 8, 8]             128\n",
      " InvertedResidual-90             [-1, 64, 8, 8]               0\n",
      "           Conv2d-91            [-1, 384, 8, 8]          24,576\n",
      "      BatchNorm2d-92            [-1, 384, 8, 8]             768\n",
      "            ReLU6-93            [-1, 384, 8, 8]               0\n",
      "           Conv2d-94            [-1, 384, 8, 8]           3,456\n",
      "      BatchNorm2d-95            [-1, 384, 8, 8]             768\n",
      "            ReLU6-96            [-1, 384, 8, 8]               0\n",
      "           Conv2d-97             [-1, 96, 8, 8]          36,864\n",
      "      BatchNorm2d-98             [-1, 96, 8, 8]             192\n",
      " InvertedResidual-99             [-1, 96, 8, 8]               0\n",
      "          Conv2d-100            [-1, 576, 8, 8]          55,296\n",
      "     BatchNorm2d-101            [-1, 576, 8, 8]           1,152\n",
      "           ReLU6-102            [-1, 576, 8, 8]               0\n",
      "          Conv2d-103            [-1, 576, 8, 8]           5,184\n",
      "     BatchNorm2d-104            [-1, 576, 8, 8]           1,152\n",
      "           ReLU6-105            [-1, 576, 8, 8]               0\n",
      "          Conv2d-106             [-1, 96, 8, 8]          55,296\n",
      "     BatchNorm2d-107             [-1, 96, 8, 8]             192\n",
      "InvertedResidual-108             [-1, 96, 8, 8]               0\n",
      "          Conv2d-109            [-1, 576, 8, 8]          55,296\n",
      "     BatchNorm2d-110            [-1, 576, 8, 8]           1,152\n",
      "           ReLU6-111            [-1, 576, 8, 8]               0\n",
      "          Conv2d-112            [-1, 576, 8, 8]           5,184\n",
      "     BatchNorm2d-113            [-1, 576, 8, 8]           1,152\n",
      "           ReLU6-114            [-1, 576, 8, 8]               0\n",
      "          Conv2d-115             [-1, 96, 8, 8]          55,296\n",
      "     BatchNorm2d-116             [-1, 96, 8, 8]             192\n",
      "InvertedResidual-117             [-1, 96, 8, 8]               0\n",
      "          Conv2d-118            [-1, 576, 8, 8]          55,296\n",
      "     BatchNorm2d-119            [-1, 576, 8, 8]           1,152\n",
      "           ReLU6-120            [-1, 576, 8, 8]               0\n",
      "          Conv2d-121            [-1, 576, 4, 4]           5,184\n",
      "     BatchNorm2d-122            [-1, 576, 4, 4]           1,152\n",
      "           ReLU6-123            [-1, 576, 4, 4]               0\n",
      "          Conv2d-124            [-1, 160, 4, 4]          92,160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     BatchNorm2d-125            [-1, 160, 4, 4]             320\n",
      "InvertedResidual-126            [-1, 160, 4, 4]               0\n",
      "          Conv2d-127            [-1, 960, 4, 4]         153,600\n",
      "     BatchNorm2d-128            [-1, 960, 4, 4]           1,920\n",
      "           ReLU6-129            [-1, 960, 4, 4]               0\n",
      "          Conv2d-130            [-1, 960, 4, 4]           8,640\n",
      "     BatchNorm2d-131            [-1, 960, 4, 4]           1,920\n",
      "           ReLU6-132            [-1, 960, 4, 4]               0\n",
      "          Conv2d-133            [-1, 160, 4, 4]         153,600\n",
      "     BatchNorm2d-134            [-1, 160, 4, 4]             320\n",
      "InvertedResidual-135            [-1, 160, 4, 4]               0\n",
      "          Conv2d-136            [-1, 960, 4, 4]         153,600\n",
      "     BatchNorm2d-137            [-1, 960, 4, 4]           1,920\n",
      "           ReLU6-138            [-1, 960, 4, 4]               0\n",
      "          Conv2d-139            [-1, 960, 4, 4]           8,640\n",
      "     BatchNorm2d-140            [-1, 960, 4, 4]           1,920\n",
      "           ReLU6-141            [-1, 960, 4, 4]               0\n",
      "          Conv2d-142            [-1, 160, 4, 4]         153,600\n",
      "     BatchNorm2d-143            [-1, 160, 4, 4]             320\n",
      "InvertedResidual-144            [-1, 160, 4, 4]               0\n",
      "          Conv2d-145            [-1, 960, 4, 4]         153,600\n",
      "     BatchNorm2d-146            [-1, 960, 4, 4]           1,920\n",
      "           ReLU6-147            [-1, 960, 4, 4]               0\n",
      "          Conv2d-148            [-1, 960, 4, 4]           8,640\n",
      "     BatchNorm2d-149            [-1, 960, 4, 4]           1,920\n",
      "           ReLU6-150            [-1, 960, 4, 4]               0\n",
      "          Conv2d-151            [-1, 320, 4, 4]         307,200\n",
      "     BatchNorm2d-152            [-1, 320, 4, 4]             640\n",
      "InvertedResidual-153            [-1, 320, 4, 4]               0\n",
      "          Conv2d-154           [-1, 1280, 4, 4]         409,600\n",
      "     BatchNorm2d-155           [-1, 1280, 4, 4]           2,560\n",
      "           ReLU6-156           [-1, 1280, 4, 4]               0\n",
      "         Dropout-157                 [-1, 1280]               0\n",
      "          Linear-158                 [-1, 1000]       1,281,000\n",
      "================================================================\n",
      "Total params: 3,504,872\n",
      "Trainable params: 3,504,872\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.19\n",
      "Forward/backward pass size (MB): 49.93\n",
      "Params size (MB): 13.37\n",
      "Estimated Total Size (MB): 63.49\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Mobile net\n",
    "model = models.mobilenet_v2(pretrained=False)\n",
    "summary(model, input_size=(3, IMG_HEIGHT, IMG_HEIGHT), device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Dropout(p=0.2, inplace=False)\n",
       "  (1): Linear(in_features=1280, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mobile net classifier\n",
    "model.classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 32, 64, 64]             864\n",
      "       BatchNorm2d-2           [-1, 32, 64, 64]              64\n",
      "             ReLU6-3           [-1, 32, 64, 64]               0\n",
      "            Conv2d-4           [-1, 32, 64, 64]             288\n",
      "       BatchNorm2d-5           [-1, 32, 64, 64]              64\n",
      "             ReLU6-6           [-1, 32, 64, 64]               0\n",
      "            Conv2d-7           [-1, 16, 64, 64]             512\n",
      "       BatchNorm2d-8           [-1, 16, 64, 64]              32\n",
      "  InvertedResidual-9           [-1, 16, 64, 64]               0\n",
      "           Conv2d-10           [-1, 96, 64, 64]           1,536\n",
      "      BatchNorm2d-11           [-1, 96, 64, 64]             192\n",
      "            ReLU6-12           [-1, 96, 64, 64]               0\n",
      "           Conv2d-13           [-1, 96, 32, 32]             864\n",
      "      BatchNorm2d-14           [-1, 96, 32, 32]             192\n",
      "            ReLU6-15           [-1, 96, 32, 32]               0\n",
      "           Conv2d-16           [-1, 24, 32, 32]           2,304\n",
      "      BatchNorm2d-17           [-1, 24, 32, 32]              48\n",
      " InvertedResidual-18           [-1, 24, 32, 32]               0\n",
      "           Conv2d-19          [-1, 144, 32, 32]           3,456\n",
      "      BatchNorm2d-20          [-1, 144, 32, 32]             288\n",
      "            ReLU6-21          [-1, 144, 32, 32]               0\n",
      "           Conv2d-22          [-1, 144, 32, 32]           1,296\n",
      "      BatchNorm2d-23          [-1, 144, 32, 32]             288\n",
      "            ReLU6-24          [-1, 144, 32, 32]               0\n",
      "           Conv2d-25           [-1, 24, 32, 32]           3,456\n",
      "      BatchNorm2d-26           [-1, 24, 32, 32]              48\n",
      " InvertedResidual-27           [-1, 24, 32, 32]               0\n",
      "           Conv2d-28          [-1, 144, 32, 32]           3,456\n",
      "      BatchNorm2d-29          [-1, 144, 32, 32]             288\n",
      "            ReLU6-30          [-1, 144, 32, 32]               0\n",
      "           Conv2d-31          [-1, 144, 16, 16]           1,296\n",
      "      BatchNorm2d-32          [-1, 144, 16, 16]             288\n",
      "            ReLU6-33          [-1, 144, 16, 16]               0\n",
      "           Conv2d-34           [-1, 32, 16, 16]           4,608\n",
      "      BatchNorm2d-35           [-1, 32, 16, 16]              64\n",
      " InvertedResidual-36           [-1, 32, 16, 16]               0\n",
      "           Conv2d-37          [-1, 192, 16, 16]           6,144\n",
      "      BatchNorm2d-38          [-1, 192, 16, 16]             384\n",
      "            ReLU6-39          [-1, 192, 16, 16]               0\n",
      "           Conv2d-40          [-1, 192, 16, 16]           1,728\n",
      "      BatchNorm2d-41          [-1, 192, 16, 16]             384\n",
      "            ReLU6-42          [-1, 192, 16, 16]               0\n",
      "           Conv2d-43           [-1, 32, 16, 16]           6,144\n",
      "      BatchNorm2d-44           [-1, 32, 16, 16]              64\n",
      " InvertedResidual-45           [-1, 32, 16, 16]               0\n",
      "           Conv2d-46          [-1, 192, 16, 16]           6,144\n",
      "      BatchNorm2d-47          [-1, 192, 16, 16]             384\n",
      "            ReLU6-48          [-1, 192, 16, 16]               0\n",
      "           Conv2d-49          [-1, 192, 16, 16]           1,728\n",
      "      BatchNorm2d-50          [-1, 192, 16, 16]             384\n",
      "            ReLU6-51          [-1, 192, 16, 16]               0\n",
      "           Conv2d-52           [-1, 32, 16, 16]           6,144\n",
      "      BatchNorm2d-53           [-1, 32, 16, 16]              64\n",
      " InvertedResidual-54           [-1, 32, 16, 16]               0\n",
      "           Conv2d-55          [-1, 192, 16, 16]           6,144\n",
      "      BatchNorm2d-56          [-1, 192, 16, 16]             384\n",
      "            ReLU6-57          [-1, 192, 16, 16]               0\n",
      "           Conv2d-58            [-1, 192, 8, 8]           1,728\n",
      "      BatchNorm2d-59            [-1, 192, 8, 8]             384\n",
      "            ReLU6-60            [-1, 192, 8, 8]               0\n",
      "           Conv2d-61             [-1, 64, 8, 8]          12,288\n",
      "      BatchNorm2d-62             [-1, 64, 8, 8]             128\n",
      " InvertedResidual-63             [-1, 64, 8, 8]               0\n",
      "           Conv2d-64            [-1, 384, 8, 8]          24,576\n",
      "      BatchNorm2d-65            [-1, 384, 8, 8]             768\n",
      "            ReLU6-66            [-1, 384, 8, 8]               0\n",
      "           Conv2d-67            [-1, 384, 8, 8]           3,456\n",
      "      BatchNorm2d-68            [-1, 384, 8, 8]             768\n",
      "            ReLU6-69            [-1, 384, 8, 8]               0\n",
      "           Conv2d-70             [-1, 64, 8, 8]          24,576\n",
      "      BatchNorm2d-71             [-1, 64, 8, 8]             128\n",
      " InvertedResidual-72             [-1, 64, 8, 8]               0\n",
      "           Conv2d-73            [-1, 384, 8, 8]          24,576\n",
      "      BatchNorm2d-74            [-1, 384, 8, 8]             768\n",
      "            ReLU6-75            [-1, 384, 8, 8]               0\n",
      "           Conv2d-76            [-1, 384, 8, 8]           3,456\n",
      "      BatchNorm2d-77            [-1, 384, 8, 8]             768\n",
      "            ReLU6-78            [-1, 384, 8, 8]               0\n",
      "           Conv2d-79             [-1, 64, 8, 8]          24,576\n",
      "      BatchNorm2d-80             [-1, 64, 8, 8]             128\n",
      " InvertedResidual-81             [-1, 64, 8, 8]               0\n",
      "           Conv2d-82            [-1, 384, 8, 8]          24,576\n",
      "      BatchNorm2d-83            [-1, 384, 8, 8]             768\n",
      "            ReLU6-84            [-1, 384, 8, 8]               0\n",
      "           Conv2d-85            [-1, 384, 8, 8]           3,456\n",
      "      BatchNorm2d-86            [-1, 384, 8, 8]             768\n",
      "            ReLU6-87            [-1, 384, 8, 8]               0\n",
      "           Conv2d-88             [-1, 64, 8, 8]          24,576\n",
      "      BatchNorm2d-89             [-1, 64, 8, 8]             128\n",
      " InvertedResidual-90             [-1, 64, 8, 8]               0\n",
      "           Conv2d-91            [-1, 384, 8, 8]          24,576\n",
      "      BatchNorm2d-92            [-1, 384, 8, 8]             768\n",
      "            ReLU6-93            [-1, 384, 8, 8]               0\n",
      "           Conv2d-94            [-1, 384, 8, 8]           3,456\n",
      "      BatchNorm2d-95            [-1, 384, 8, 8]             768\n",
      "            ReLU6-96            [-1, 384, 8, 8]               0\n",
      "           Conv2d-97             [-1, 96, 8, 8]          36,864\n",
      "      BatchNorm2d-98             [-1, 96, 8, 8]             192\n",
      " InvertedResidual-99             [-1, 96, 8, 8]               0\n",
      "          Conv2d-100            [-1, 576, 8, 8]          55,296\n",
      "     BatchNorm2d-101            [-1, 576, 8, 8]           1,152\n",
      "           ReLU6-102            [-1, 576, 8, 8]               0\n",
      "          Conv2d-103            [-1, 576, 8, 8]           5,184\n",
      "     BatchNorm2d-104            [-1, 576, 8, 8]           1,152\n",
      "           ReLU6-105            [-1, 576, 8, 8]               0\n",
      "          Conv2d-106             [-1, 96, 8, 8]          55,296\n",
      "     BatchNorm2d-107             [-1, 96, 8, 8]             192\n",
      "InvertedResidual-108             [-1, 96, 8, 8]               0\n",
      "          Conv2d-109            [-1, 576, 8, 8]          55,296\n",
      "     BatchNorm2d-110            [-1, 576, 8, 8]           1,152\n",
      "           ReLU6-111            [-1, 576, 8, 8]               0\n",
      "          Conv2d-112            [-1, 576, 8, 8]           5,184\n",
      "     BatchNorm2d-113            [-1, 576, 8, 8]           1,152\n",
      "           ReLU6-114            [-1, 576, 8, 8]               0\n",
      "          Conv2d-115             [-1, 96, 8, 8]          55,296\n",
      "     BatchNorm2d-116             [-1, 96, 8, 8]             192\n",
      "InvertedResidual-117             [-1, 96, 8, 8]               0\n",
      "          Conv2d-118            [-1, 576, 8, 8]          55,296\n",
      "     BatchNorm2d-119            [-1, 576, 8, 8]           1,152\n",
      "           ReLU6-120            [-1, 576, 8, 8]               0\n",
      "          Conv2d-121            [-1, 576, 4, 4]           5,184\n",
      "     BatchNorm2d-122            [-1, 576, 4, 4]           1,152\n",
      "           ReLU6-123            [-1, 576, 4, 4]               0\n",
      "          Conv2d-124            [-1, 160, 4, 4]          92,160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     BatchNorm2d-125            [-1, 160, 4, 4]             320\n",
      "InvertedResidual-126            [-1, 160, 4, 4]               0\n",
      "          Conv2d-127            [-1, 960, 4, 4]         153,600\n",
      "     BatchNorm2d-128            [-1, 960, 4, 4]           1,920\n",
      "           ReLU6-129            [-1, 960, 4, 4]               0\n",
      "          Conv2d-130            [-1, 960, 4, 4]           8,640\n",
      "     BatchNorm2d-131            [-1, 960, 4, 4]           1,920\n",
      "           ReLU6-132            [-1, 960, 4, 4]               0\n",
      "          Conv2d-133            [-1, 160, 4, 4]         153,600\n",
      "     BatchNorm2d-134            [-1, 160, 4, 4]             320\n",
      "InvertedResidual-135            [-1, 160, 4, 4]               0\n",
      "          Conv2d-136            [-1, 960, 4, 4]         153,600\n",
      "     BatchNorm2d-137            [-1, 960, 4, 4]           1,920\n",
      "           ReLU6-138            [-1, 960, 4, 4]               0\n",
      "          Conv2d-139            [-1, 960, 4, 4]           8,640\n",
      "     BatchNorm2d-140            [-1, 960, 4, 4]           1,920\n",
      "           ReLU6-141            [-1, 960, 4, 4]               0\n",
      "          Conv2d-142            [-1, 160, 4, 4]         153,600\n",
      "     BatchNorm2d-143            [-1, 160, 4, 4]             320\n",
      "InvertedResidual-144            [-1, 160, 4, 4]               0\n",
      "          Conv2d-145            [-1, 960, 4, 4]         153,600\n",
      "     BatchNorm2d-146            [-1, 960, 4, 4]           1,920\n",
      "           ReLU6-147            [-1, 960, 4, 4]               0\n",
      "          Conv2d-148            [-1, 960, 4, 4]           8,640\n",
      "     BatchNorm2d-149            [-1, 960, 4, 4]           1,920\n",
      "           ReLU6-150            [-1, 960, 4, 4]               0\n",
      "          Conv2d-151            [-1, 320, 4, 4]         307,200\n",
      "     BatchNorm2d-152            [-1, 320, 4, 4]             640\n",
      "InvertedResidual-153            [-1, 320, 4, 4]               0\n",
      "          Conv2d-154           [-1, 1280, 4, 4]         409,600\n",
      "     BatchNorm2d-155           [-1, 1280, 4, 4]           2,560\n",
      "           ReLU6-156           [-1, 1280, 4, 4]               0\n",
      "         Flatten-157                 [-1, 1280]               0\n",
      "          Linear-158                  [-1, 512]         655,872\n",
      "          Linear-159                  [-1, 512]         262,656\n",
      "            ReLU-160                  [-1, 512]               0\n",
      "          Linear-161                  [-1, 512]         262,656\n",
      "     MobileNetV2-162                  [-1, 512]               0\n",
      "================================================================\n",
      "Total params: 3,405,056\n",
      "Trainable params: 3,405,056\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.19\n",
      "Forward/backward pass size (MB): 49.94\n",
      "Params size (MB): 12.99\n",
      "Estimated Total Size (MB): 63.12\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "class EncoderNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EncoderNet, self).__init__()\n",
    "        self.encoder = models.mobilenet_v2(pretrained=False)   # base model (transfer learning)\n",
    "        self.encoder.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(1280, 512),   # encoding layer, mobile_netV2 output: 1280 \n",
    "            nn.Linear(512, 512),   # projection layer\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),   # projection layer\n",
    "        )      \n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.encoder(x)  \n",
    "    \n",
    "summary(EncoderNet(), input_size=(3, IMG_HEIGHT, IMG_HEIGHT), device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiSiameseEncoderNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MultiSiameseEncoderNet, self).__init__()\n",
    "        self.encoder = EncoderNet()\n",
    "        \n",
    "    def encode(self, x):\n",
    "        output = self.encoder(x)\n",
    "        return output\n",
    "    \n",
    "    def forward(self, input_imgs):\n",
    "        # cosine sim of query img against each batch img\n",
    "        query_img_encoding = self.encode(input_imgs[0])   # 1st img is the query img\n",
    "        cosine_sims = []\n",
    "        for i in range(1, len(input_imgs)):   # batch imgs\n",
    "            batch_img_encoding = self.encode(input_imgs[i])\n",
    "            cosine_sims.append(F.cosine_similarity(query_img_encoding, batch_img_encoding))\n",
    "        return torch.stack(cosine_sims, dim=1)   # concat cosine sims\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator:\n",
    "    \n",
    "    def __init__(self, spectrogram_samples_files, candidate_size, batch_size, num_batches, num_sub_samples, img_height):\n",
    "        self.spectrogram_samples_files = spectrogram_samples_files   # list of filepaths\n",
    "        self.candidate_size = candidate_size   # 1 positive, n-1 negatives\n",
    "        self.batch_size = batch_size   # batch size\n",
    "        self.num_batches = num_batches   # num batches per epoch\n",
    "        self.num_sub_samples = num_sub_samples   # num sub-samples per epoch\n",
    "        self.img_height = img_height   # height of square img to be generated in the batches\n",
    "        self.sub_samples = []   # list of RGB converted spectrograms\n",
    "    \n",
    "    def generate_batches(self):\n",
    "        while True:\n",
    "            self.create_sub_samples()\n",
    "            # batches per epoch\n",
    "            for _ in range(self.num_batches):\n",
    "                # create batch\n",
    "                labels = []\n",
    "                query_and_candidate_imgs = [[] for _ in range(self.candidate_size + 1)]\n",
    "                for _ in range(self.batch_size):\n",
    "                    sample_spectrograms_indices = random.sample(range(self.num_sub_samples), self.candidate_size)   # sample candidates\n",
    "                    pos_idx = sample_spectrograms_indices[0]   # positive sample\n",
    "                    # Generate query image\n",
    "                    query_img = self.get_sliding_img_slice_from_spectrogram(self.sub_samples[pos_idx])\n",
    "                    # Generate batch images\n",
    "                    random.shuffle(sample_spectrograms_indices)\n",
    "                    candidate_imgs = [self.get_sliding_img_slice_from_spectrogram(self.sub_samples[idx]) for idx in sample_spectrograms_indices]\n",
    "                    # get class label / idx of positive sample\n",
    "                    pos_candidate_idx = sample_spectrograms_indices.index(pos_idx)   \n",
    "                    labels.append(pos_candidate_idx)\n",
    "                    # Normalize input imgs\n",
    "                    for i, img in enumerate([query_img, *candidate_imgs]):\n",
    "                        img = img / np.amax(np.absolute(img))   # normalize to range [-1, 1]\n",
    "                        query_and_candidate_imgs[i].append(img)\n",
    "                # Convert to tensor\n",
    "                labels = torch.tensor(labels)\n",
    "                input_imgs = torch.tensor(query_and_candidate_imgs)\n",
    "                yield (input_imgs, labels)\n",
    "    \n",
    "    def create_sub_samples(self):\n",
    "        self.sub_samples = []   # reset\n",
    "        files = random.sample(self.spectrogram_samples_files, self.num_sub_samples)   # sampling without replacement\n",
    "        for file in files:\n",
    "#             print(file)\n",
    "            spectrogram = np.load(file)\n",
    "            assert spectrogram.shape[0] == self.img_height, \"Input spectrogram height does not match img height\"\n",
    "            self.sub_samples.append(spectrogram)\n",
    "    \n",
    "    @classmethod\n",
    "    def get_sliding_img_slice_from_spectrogram(cls, spectrogram, depth=3, sliding_ratio=2):\n",
    "        ### Combine multiple sliding greyscale img slices into an n-depth image\n",
    "        height = spectrogram.shape[0]\n",
    "        slide_step = height//sliding_ratio\n",
    "        img_slice = np.zeros((depth,height,height))   # initialize empty img (pytorch style)\n",
    "        # Get random start idx\n",
    "        slice_start = random.randint(0, spectrogram.shape[1] - (slide_step*(depth+1)) - 1)\n",
    "        for i in range(depth):\n",
    "            img_slice[i,:,:] = spectrogram[:, slice_start:slice_start+height]   # get slice (pytorch style)\n",
    "#             slice_start += slide_step   # slide\n",
    "        img_slice = img_slice.astype(\"float32\")\n",
    "        return img_slice\n",
    "    \n",
    "    @classmethod\n",
    "    def spectrogram_to_RGB(cls, spectrogram):\n",
    "        assert len(spectrogram.shape) == 2, \"Spectrogram input should be a 2D array\"\n",
    "        spectrogram_rgb = gray2rgb(spectrogram)\n",
    "        return spectrogram_rgb\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_folder = os.path.join(output_data_folder, \"training_dataset_full_spectrogram/vox1_dev_wav\")\n",
    "spectrogram_samples_files = [os.path.join(training_folder, file) for file in os.listdir(training_folder)]\n",
    "candidate_size = 5\n",
    "batch_size = 15\n",
    "num_batches = 1000 // batch_size\n",
    "# num_batches = 200 // batch_size\n",
    "# num_batches = 100\n",
    "num_sub_samples = 100\n",
    "# num_sub_samples = 20\n",
    "training_data_generator = DataGenerator(spectrogram_samples_files, candidate_size, batch_size, num_batches, num_sub_samples, IMG_HEIGHT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, num_epochs, num_batches, training_data_generator):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    t1 = time.time()\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            num_batch_modified = num_batches if phase == 'train' else num_batches // 20\n",
    "            for i, data in zip(range(num_batch_modified), training_data_generator.generate_batches()):               \n",
    "                input_imgs, labels = data\n",
    "                inputs = [img.to(device) for img in input_imgs]\n",
    "                labels = labels.to(device)\n",
    "                \n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):   # gradient only for train\n",
    "                    outputs = model(inputs)   \n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs[0].size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "                \n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "\n",
    "            epoch_loss = running_loss / (num_batch_modified * inputs[0].size(0))\n",
    "            epoch_acc = running_corrects.double() / (num_batch_modified * inputs[0].size(0))\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        # end of epoch\n",
    "        print(\"Time taken is {} seconds\".format(int(time.time()-t1)))\n",
    "        t1 = time.time()\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/49\n",
      "----------\n",
      "train Loss: 1.6006 Acc: 0.2596\n",
      "val Loss: 1.6049 Acc: 0.2444\n",
      "Time taken is 58 seconds\n",
      "\n",
      "Epoch 1/49\n",
      "----------\n",
      "train Loss: 1.5878 Acc: 0.2424\n",
      "val Loss: 1.6089 Acc: 0.2444\n",
      "Time taken is 59 seconds\n",
      "\n",
      "Epoch 2/49\n",
      "----------\n",
      "train Loss: 1.5816 Acc: 0.2747\n",
      "val Loss: 1.5857 Acc: 0.2889\n",
      "Time taken is 59 seconds\n",
      "\n",
      "Epoch 3/49\n",
      "----------\n",
      "train Loss: 1.5255 Acc: 0.3162\n",
      "val Loss: 1.4342 Acc: 0.3778\n",
      "Time taken is 60 seconds\n",
      "\n",
      "Epoch 4/49\n",
      "----------\n",
      "train Loss: 1.4523 Acc: 0.3424\n",
      "val Loss: 1.4670 Acc: 0.3556\n",
      "Time taken is 60 seconds\n",
      "\n",
      "Epoch 5/49\n",
      "----------\n",
      "train Loss: 1.4388 Acc: 0.3808\n",
      "val Loss: 1.2987 Acc: 0.5556\n",
      "Time taken is 59 seconds\n",
      "\n",
      "Epoch 6/49\n",
      "----------\n",
      "train Loss: 1.3957 Acc: 0.3677\n",
      "val Loss: 1.3008 Acc: 0.3111\n",
      "Time taken is 60 seconds\n",
      "\n",
      "Epoch 7/49\n",
      "----------\n",
      "train Loss: 1.4104 Acc: 0.3596\n",
      "val Loss: 1.3340 Acc: 0.5333\n",
      "Time taken is 60 seconds\n",
      "\n",
      "Epoch 8/49\n",
      "----------\n",
      "train Loss: 1.3913 Acc: 0.3414\n",
      "val Loss: 1.3603 Acc: 0.3778\n",
      "Time taken is 59 seconds\n",
      "\n",
      "Epoch 9/49\n",
      "----------\n",
      "train Loss: 1.3786 Acc: 0.3727\n",
      "val Loss: 1.3504 Acc: 0.3111\n",
      "Time taken is 59 seconds\n",
      "\n",
      "Epoch 10/49\n",
      "----------\n",
      "train Loss: 1.3515 Acc: 0.3566\n",
      "val Loss: 1.2426 Acc: 0.3778\n",
      "Time taken is 59 seconds\n",
      "\n",
      "Epoch 11/49\n",
      "----------\n",
      "train Loss: 1.3633 Acc: 0.3697\n",
      "val Loss: 1.3799 Acc: 0.4222\n",
      "Time taken is 59 seconds\n",
      "\n",
      "Epoch 12/49\n",
      "----------\n",
      "train Loss: 1.3081 Acc: 0.3970\n",
      "val Loss: 1.3719 Acc: 0.4000\n",
      "Time taken is 59 seconds\n",
      "\n",
      "Epoch 13/49\n",
      "----------\n",
      "train Loss: 1.3524 Acc: 0.3677\n",
      "val Loss: 1.2952 Acc: 0.4000\n",
      "Time taken is 59 seconds\n",
      "\n",
      "Epoch 14/49\n",
      "----------\n",
      "train Loss: 1.3006 Acc: 0.3606\n",
      "val Loss: 1.2486 Acc: 0.4444\n",
      "Time taken is 60 seconds\n",
      "\n",
      "Epoch 15/49\n",
      "----------\n",
      "train Loss: 1.3492 Acc: 0.3798\n",
      "val Loss: 1.2566 Acc: 0.4444\n",
      "Time taken is 60 seconds\n",
      "\n",
      "Epoch 16/49\n",
      "----------\n",
      "train Loss: 1.3182 Acc: 0.3727\n",
      "val Loss: 1.3464 Acc: 0.4222\n",
      "Time taken is 60 seconds\n",
      "\n",
      "Epoch 17/49\n",
      "----------\n",
      "train Loss: 1.3358 Acc: 0.3879\n",
      "val Loss: 1.2794 Acc: 0.5556\n",
      "Time taken is 59 seconds\n",
      "\n",
      "Epoch 18/49\n",
      "----------\n",
      "train Loss: 1.3235 Acc: 0.4000\n",
      "val Loss: 1.3944 Acc: 0.3556\n",
      "Time taken is 59 seconds\n",
      "\n",
      "Epoch 19/49\n",
      "----------\n",
      "train Loss: 1.3049 Acc: 0.4172\n",
      "val Loss: 1.2566 Acc: 0.4000\n",
      "Time taken is 58 seconds\n",
      "\n",
      "Epoch 20/49\n",
      "----------\n",
      "train Loss: 1.3416 Acc: 0.3838\n",
      "val Loss: 1.3134 Acc: 0.4444\n",
      "Time taken is 58 seconds\n",
      "\n",
      "Epoch 21/49\n",
      "----------\n",
      "train Loss: 1.3913 Acc: 0.3687\n",
      "val Loss: 1.2879 Acc: 0.4222\n",
      "Time taken is 58 seconds\n",
      "\n",
      "Epoch 22/49\n",
      "----------\n",
      "train Loss: 1.3053 Acc: 0.4101\n",
      "val Loss: 1.1898 Acc: 0.4889\n",
      "Time taken is 57 seconds\n",
      "\n",
      "Epoch 23/49\n",
      "----------\n",
      "train Loss: 1.3274 Acc: 0.3828\n",
      "val Loss: 1.3212 Acc: 0.4000\n",
      "Time taken is 57 seconds\n",
      "\n",
      "Epoch 24/49\n",
      "----------\n",
      "train Loss: 1.3598 Acc: 0.4000\n",
      "val Loss: 1.2786 Acc: 0.4000\n",
      "Time taken is 58 seconds\n",
      "\n",
      "Epoch 25/49\n",
      "----------\n",
      "train Loss: 1.3409 Acc: 0.3687\n",
      "val Loss: 1.3752 Acc: 0.3778\n",
      "Time taken is 57 seconds\n",
      "\n",
      "Epoch 26/49\n",
      "----------\n",
      "train Loss: 1.3203 Acc: 0.3848\n",
      "val Loss: 1.4670 Acc: 0.4444\n",
      "Time taken is 57 seconds\n",
      "\n",
      "Epoch 27/49\n",
      "----------\n",
      "train Loss: 1.3221 Acc: 0.4111\n",
      "val Loss: 1.4736 Acc: 0.3333\n",
      "Time taken is 58 seconds\n",
      "\n",
      "Epoch 28/49\n",
      "----------\n",
      "train Loss: 1.3604 Acc: 0.3636\n",
      "val Loss: 1.3030 Acc: 0.3111\n",
      "Time taken is 60 seconds\n",
      "\n",
      "Epoch 29/49\n",
      "----------\n",
      "train Loss: 1.3211 Acc: 0.3929\n",
      "val Loss: 1.3428 Acc: 0.3778\n",
      "Time taken is 59 seconds\n",
      "\n",
      "Epoch 30/49\n",
      "----------\n",
      "train Loss: 1.3707 Acc: 0.3374\n",
      "val Loss: 1.3034 Acc: 0.3111\n",
      "Time taken is 59 seconds\n",
      "\n",
      "Epoch 31/49\n",
      "----------\n",
      "train Loss: 1.3069 Acc: 0.4091\n",
      "val Loss: 1.3555 Acc: 0.3778\n",
      "Time taken is 59 seconds\n",
      "\n",
      "Epoch 32/49\n",
      "----------\n",
      "train Loss: 1.3164 Acc: 0.4182\n",
      "val Loss: 1.3167 Acc: 0.3556\n",
      "Time taken is 59 seconds\n",
      "\n",
      "Epoch 33/49\n",
      "----------\n",
      "train Loss: 1.3409 Acc: 0.3758\n",
      "val Loss: 1.3737 Acc: 0.3333\n",
      "Time taken is 58 seconds\n",
      "\n",
      "Epoch 34/49\n",
      "----------\n",
      "train Loss: 1.3296 Acc: 0.4020\n",
      "val Loss: 1.3218 Acc: 0.4444\n",
      "Time taken is 59 seconds\n",
      "\n",
      "Epoch 35/49\n",
      "----------\n",
      "train Loss: 1.3244 Acc: 0.3828\n",
      "val Loss: 1.2272 Acc: 0.4222\n",
      "Time taken is 57 seconds\n",
      "\n",
      "Epoch 36/49\n",
      "----------\n",
      "train Loss: 1.3209 Acc: 0.4131\n",
      "val Loss: 1.2639 Acc: 0.4222\n",
      "Time taken is 57 seconds\n",
      "\n",
      "Epoch 37/49\n",
      "----------\n",
      "train Loss: 1.3630 Acc: 0.3586\n",
      "val Loss: 1.2997 Acc: 0.4889\n",
      "Time taken is 57 seconds\n",
      "\n",
      "Epoch 38/49\n",
      "----------\n",
      "train Loss: 1.3461 Acc: 0.3717\n",
      "val Loss: 1.4582 Acc: 0.4222\n",
      "Time taken is 57 seconds\n",
      "\n",
      "Epoch 39/49\n",
      "----------\n",
      "train Loss: 1.3653 Acc: 0.3697\n",
      "val Loss: 1.2453 Acc: 0.4889\n",
      "Time taken is 57 seconds\n",
      "\n",
      "Epoch 40/49\n",
      "----------\n",
      "train Loss: 1.3268 Acc: 0.3990\n",
      "val Loss: 1.2923 Acc: 0.5111\n",
      "Time taken is 57 seconds\n",
      "\n",
      "Epoch 41/49\n",
      "----------\n",
      "train Loss: 1.3475 Acc: 0.4091\n",
      "val Loss: 1.1746 Acc: 0.5333\n",
      "Time taken is 57 seconds\n",
      "\n",
      "Epoch 42/49\n",
      "----------\n",
      "train Loss: 1.3399 Acc: 0.3808\n",
      "val Loss: 1.2736 Acc: 0.4222\n",
      "Time taken is 57 seconds\n",
      "\n",
      "Epoch 43/49\n",
      "----------\n",
      "train Loss: 1.3286 Acc: 0.4030\n",
      "val Loss: 1.3281 Acc: 0.5333\n",
      "Time taken is 57 seconds\n",
      "\n",
      "Epoch 44/49\n",
      "----------\n",
      "train Loss: 1.3116 Acc: 0.3869\n",
      "val Loss: 1.5055 Acc: 0.3556\n",
      "Time taken is 57 seconds\n",
      "\n",
      "Epoch 45/49\n",
      "----------\n",
      "train Loss: 1.3185 Acc: 0.3626\n",
      "val Loss: 1.3618 Acc: 0.3556\n",
      "Time taken is 57 seconds\n",
      "\n",
      "Epoch 46/49\n",
      "----------\n",
      "train Loss: 1.3334 Acc: 0.3889\n",
      "val Loss: 1.2198 Acc: 0.4000\n",
      "Time taken is 60 seconds\n",
      "\n",
      "Epoch 47/49\n",
      "----------\n",
      "train Loss: 1.3301 Acc: 0.3949\n",
      "val Loss: 1.2818 Acc: 0.4667\n",
      "Time taken is 59 seconds\n",
      "\n",
      "Epoch 48/49\n",
      "----------\n",
      "train Loss: 1.3342 Acc: 0.3515\n",
      "val Loss: 1.2781 Acc: 0.3333\n",
      "Time taken is 59 seconds\n",
      "\n",
      "Epoch 49/49\n",
      "----------\n",
      "train Loss: 1.3489 Acc: 0.3778\n",
      "val Loss: 1.3659 Acc: 0.4000\n",
      "Time taken is 58 seconds\n",
      "\n",
      "Training complete in 49m 3s\n",
      "Best val Acc: 0.555556\n"
     ]
    }
   ],
   "source": [
    "### Train\n",
    "\n",
    "epochs = 50\n",
    "\n",
    "model_ft = MultiSiameseEncoderNet().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer_ft = optim.Adam(model_ft.parameters(), lr = 0.001)\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\n",
    "\n",
    "### Train \n",
    "\n",
    "model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler, epochs, num_batches, training_data_generator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: Overall\n",
    "* Contrastive classifier: separate train and validate methods\n",
    "* DataGenerator: generate an actual batch instead of just one (batches of candidates)\n",
    "* Model saving / checkpointing\n",
    "* Build binary classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
